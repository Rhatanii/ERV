Log path set to: ./logs/11-07-0209-humanomni_emotion_emer_1format_withpath_withchoice.txt
af_a_tawn
[2025-11-07 02:09:10,819] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-07 02:09:10,832] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-07 02:09:10,874] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-07 02:09:10,940] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 11-07 02:09:12 __init__.py:190] Automatically detected platform cuda.
INFO 11-07 02:09:12 __init__.py:190] Automatically detected platform cuda.
INFO 11-07 02:09:12 __init__.py:190] Automatically detected platform cuda.
INFO 11-07 02:09:12 __init__.py:190] Automatically detected platform cuda.
[2025-11-07 02:09:14,015] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-11-07 02:09:14,026] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-11-07 02:09:14,026] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-11-07 02:09:14,113] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-11-07 02:09:14,136] [INFO] [comm.py:652:init_distributed] cdb=None
has video in dataset
Map:   0%|          | 0/16701 [00:00<?, ? examples/s]has video in dataset
Map:   0%|          | 0/16701 [00:00<?, ? examples/s]has video in dataset
Map:   0%|          | 0/16701 [00:00<?, ? examples/s]has video in dataset
Map:   0%|          | 0/16701 [00:00<?, ? examples/s]Map:  16%|█▋        | 2750/16701 [00:00<00:00, 27372.57 examples/s]Map:  17%|█▋        | 2768/16701 [00:00<00:00, 27551.93 examples/s]Map:  13%|█▎        | 2147/16701 [00:00<00:00, 21366.46 examples/s]Map:  17%|█▋        | 2828/16701 [00:00<00:00, 28152.30 examples/s]Map:  41%|████      | 6863/16701 [00:00<00:00, 26806.79 examples/s]Map:  41%|████      | 6828/16701 [00:00<00:00, 26699.16 examples/s]Map:  32%|███▏      | 5289/16701 [00:00<00:00, 21076.52 examples/s]Map:  41%|████      | 6831/16701 [00:00<00:00, 26922.46 examples/s]Map:  63%|██████▎   | 10578/16701 [00:00<00:00, 13105.21 examples/s]Map:  64%|██████▎   | 10640/16701 [00:00<00:00, 12771.54 examples/s]Map:  48%|████▊     | 8000/16701 [00:00<00:00, 10136.51 examples/s]Map:  64%|██████▍   | 10686/16701 [00:00<00:00, 13421.04 examples/s]Map:  78%|███████▊  | 13012/16701 [00:00<00:00, 15246.36 examples/s]Map:  80%|████████  | 13395/16701 [00:00<00:00, 15269.68 examples/s]Map:  60%|██████    | 10084/16701 [00:00<00:00, 12177.98 examples/s]Map:  80%|████████  | 13422/16701 [00:00<00:00, 15912.66 examples/s]Map:  95%|█████████▍| 15801/16701 [00:00<00:00, 17951.05 examples/s]Map:  96%|█████████▌| 16000/16701 [00:00<00:00, 17396.02 examples/s]Map:  73%|███████▎  | 12244/16701 [00:00<00:00, 14226.78 examples/s]Map: 100%|██████████| 16701/16701 [00:00<00:00, 17395.73 examples/s]
Map:   0%|          | 0/266 [00:00<?, ? examples/s]Map: 100%|██████████| 16701/16701 [00:00<00:00, 17119.85 examples/s]
Map:   0%|          | 0/266 [00:00<?, ? examples/s]Map: 100%|██████████| 266/266 [00:00<00:00, 22135.73 examples/s]
using:  <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Map: 100%|██████████| 266/266 [00:00<00:00, 22933.83 examples/s]
using:  <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Map:  96%|█████████▌| 16000/16701 [00:00<00:00, 17967.83 examples/s][2025-11-07 02:09:15,438] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:15,438] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Map: 100%|██████████| 16701/16701 [00:00<00:00, 17713.65 examples/s]
Map:   0%|          | 0/266 [00:00<?, ? examples/s]Map: 100%|██████████| 266/266 [00:00<00:00, 22704.21 examples/s]
using:  <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Map:  86%|████████▌ | 14373/16701 [00:00<00:00, 15900.31 examples/s][2025-11-07 02:09:15,506] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Map:  99%|█████████▉| 16526/16701 [00:01<00:00, 17323.32 examples/s]Map: 100%|██████████| 16701/16701 [00:01<00:00, 14998.64 examples/s]
Map:   0%|          | 0/266 [00:00<?, ? examples/s]Map: 100%|██████████| 266/266 [00:00<00:00, 18179.94 examples/s]
using:  <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2025-11-07 02:09:15,678] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:16,806] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 624, num_elems = 0.78B
Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.projector.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.classifier.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.conv1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.selSome weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.projector.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_pro.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.wef_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.vision_tower.j.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_ight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encovision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tnorm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_pder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ower.encoder.layers.9.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.classifier.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.lroj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audiayers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.selfo_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'm_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
odel.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2025-11-07 02:09:17,713] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:17,723] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:17,736] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:17,760] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:17,849] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 832, num_elems = 0.87B
[2025-11-07 02:09:18,072] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:18,073] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:18,073] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:18,081] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:18,289] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1323, num_elems = 1.51B
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Frozen Parameters:
  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias
  model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
Frozen Parameters:  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.post_layernorm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.post_layernorm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.head.probe
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.head.layernorm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.head.layernorm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.conv1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
  model.audio_tower.audio_tower.encoder.conv1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
  model.audio_tower.audio_tower.encoder.conv2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight

  model.audio_tower.audio_tower.encoder.conv2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias

  model.audio_tower.audio_tower.encoder.embed_positions.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias

  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  model.audio_tower.audio_tower.encoder.layers.0.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.1.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.1.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.1.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.1.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.fc2.weightFrozen Parameters:
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias

  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight

  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight

  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.3.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.3.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight


  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight


  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias


  model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.5.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.5.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight

  model.audio_tower.audio_tower.encoder.layers.5.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
  model.audio_tower.audio_tower.encoder.layers.5.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.6.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.6.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.6.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight


  model.audio_tower.audio_tower.encoder.layers.6.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.7.fc1.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.fc1.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.fc2.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.fc2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.8.fc1.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.8.fc1.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.8.fc2.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.fc2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
  model.audio_tower.audio_tower.encoder.layers.9.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.fc2.weight

  model.vision_tower.vision_tower.vision_model.post_layernorm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.post_layernorm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.head.probe  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias

  model.vision_tower.vision_tower.vision_model.head.layernorm.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight

  model.vision_tower.vision_tower.vision_model.head.layernorm.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias

  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight

  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.10.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias

  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.10.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.10.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.conv1.weight  model.audio_tower.audio_tower.encoder.layers.10.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.conv1.bias  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.conv2.weight  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.conv2.bias  model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.embed_positions.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.11.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.11.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.11.fc2.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.0.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.11.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.0.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.0.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.12.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight

  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.12.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias

  model.audio_tower.audio_tower.encoder.layers.1.fc1.weight  model.audio_tower.audio_tower.encoder.layers.12.fc2.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.1.fc1.bias  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.layers.1.fc2.weight  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.1.fc2.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.13.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.13.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.13.fc2.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.2.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.13.fc2.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.2.fc1.bias  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.2.fc2.weight  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.2.fc2.bias  model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  model.audio_tower.audio_tower.encoder.layers.3.fc1.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  model.audio_tower.audio_tower.encoder.layers.3.fc1.bias

  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  model.audio_tower.audio_tower.encoder.layers.14.fc1.bias  model.audio_tower.audio_tower.encoder.layers.3.fc2.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.fc2.weight  model.audio_tower.audio_tower.encoder.layers.3.fc2.bias


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.14.fc2.bias  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight


  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight

  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight

  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias

  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.4.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.4.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.15.fc1.weight  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
  model.audio_tower.audio_tower.encoder.layers.4.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.15.fc1.bias  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias

  model.audio_tower.audio_tower.encoder.layers.4.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.15.fc2.weight  model.vision_tower.vision_tower.vision_model.post_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.15.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias  model.vision_tower.vision_tower.vision_model.post_layernorm.bias  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight  model.vision_tower.vision_tower.vision_model.head.probe  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight  model.vision_tower.vision_tower.vision_model.head.layernorm.weight  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias  model.vision_tower.vision_tower.vision_model.head.layernorm.bias  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight  model.audio_tower.audio_tower.encoder.layers.5.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias  model.audio_tower.audio_tower.encoder.layers.5.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.16.fc1.weight  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight  model.audio_tower.audio_tower.encoder.layers.5.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.16.fc1.bias  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.5.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.16.fc2.weight  model.audio_tower.audio_tower.encoder.conv1.weight
  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.16.fc2.bias
  model.audio_tower.audio_tower.encoder.conv1.bias  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.conv2.weight  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.conv2.bias  model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.embed_positions.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.6.fc1.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.6.fc1.bias  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.17.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.6.fc2.weight  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.17.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.6.fc2.bias  model.audio_tower.audio_tower.encoder.layers.0.fc1.weight  model.audio_tower.audio_tower.encoder.layers.17.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.0.fc1.bias  model.audio_tower.audio_tower.encoder.layers.17.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.0.fc2.weight  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.0.fc2.bias  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.18.fc1.weight  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.7.fc1.weight  model.audio_tower.audio_tower.encoder.layers.18.fc1.bias  model.audio_tower.audio_tower.encoder.layers.1.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.7.fc1.bias  model.audio_tower.audio_tower.encoder.layers.18.fc2.weight  model.audio_tower.audio_tower.encoder.layers.1.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.18.fc2.bias  model.audio_tower.audio_tower.encoder.layers.7.fc2.weight  model.audio_tower.audio_tower.encoder.layers.1.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.7.fc2.bias  model.audio_tower.audio_tower.encoder.layers.1.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.19.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.19.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.8.fc1.weight  model.audio_tower.audio_tower.encoder.layers.2.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.19.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.8.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.2.fc1.bias  model.audio_tower.audio_tower.encoder.layers.19.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.8.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.2.fc2.weight  model.audio_tower.audio_tower.encoder.layers.8.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.2.fc2.bias  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.20.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.20.fc1.bias  model.audio_tower.audio_tower.encoder.layers.9.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.3.fc1.weight  model.audio_tower.audio_tower.encoder.layers.20.fc2.weight  model.audio_tower.audio_tower.encoder.layers.9.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.3.fc1.bias  model.audio_tower.audio_tower.encoder.layers.20.fc2.bias  model.audio_tower.audio_tower.encoder.layers.9.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.3.fc2.weight  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.9.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.3.fc2.bias  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.21.fc1.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.21.fc1.bias  model.audio_tower.audio_tower.encoder.layers.10.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.4.fc1.weight  model.audio_tower.audio_tower.encoder.layers.21.fc2.weight  model.audio_tower.audio_tower.encoder.layers.10.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.4.fc1.bias  model.audio_tower.audio_tower.encoder.layers.21.fc2.bias  model.audio_tower.audio_tower.encoder.layers.10.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.4.fc2.weight  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.10.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.4.fc2.bias  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.11.fc1.weight  model.audio_tower.audio_tower.encoder.layers.5.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.11.fc1.bias  model.audio_tower.audio_tower.encoder.layers.5.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.11.fc2.weight  model.audio_tower.audio_tower.encoder.layers.5.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.22.fc1.weight  model.audio_tower.audio_tower.encoder.layers.11.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.5.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.22.fc1.bias  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.22.fc2.weight  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.22.fc2.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.12.fc1.weight  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.12.fc1.bias  model.audio_tower.audio_tower.encoder.layers.23.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.12.fc2.weight  model.audio_tower.audio_tower.encoder.layers.23.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.12.fc2.bias  model.audio_tower.audio_tower.encoder.layers.23.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.23.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.13.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.24.fc1.weight  model.audio_tower.audio_tower.encoder.layers.13.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.24.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.13.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.24.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.13.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.24.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.6.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.6.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.6.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.14.fc1.weight  model.audio_tower.audio_tower.encoder.layers.6.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.14.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.14.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.14.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.7.fc1.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.7.fc1.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.7.fc2.weight  model.audio_tower.audio_tower.encoder.layers.15.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.7.fc2.bias  model.audio_tower.audio_tower.encoder.layers.15.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.15.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.15.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.25.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.25.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.16.fc1.weight  model.audio_tower.audio_tower.encoder.layers.25.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.16.fc1.bias  model.audio_tower.audio_tower.encoder.layers.25.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.16.fc2.weight  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.16.fc2.bias  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.26.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.26.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.26.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.17.fc1.weight  model.audio_tower.audio_tower.encoder.layers.26.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.17.fc1.bias  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.17.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.18.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.8.fc1.weight  model.audio_tower.audio_tower.encoder.layers.18.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.8.fc1.bias  model.audio_tower.audio_tower.encoder.layers.18.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.8.fc2.weight  model.audio_tower.audio_tower.encoder.layers.18.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.8.fc2.bias  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.19.fc1.weight  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.19.fc1.bias  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.19.fc2.weight  model.audio_tower.audio_tower.encoder.layers.9.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.19.fc2.bias  model.audio_tower.audio_tower.encoder.layers.9.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.9.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.9.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.27.fc1.weight  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.27.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.20.fc1.weight  model.audio_tower.audio_tower.encoder.layers.27.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.20.fc1.bias  model.audio_tower.audio_tower.encoder.layers.27.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.20.fc2.weight  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.20.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.28.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.28.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.21.fc1.weight  model.audio_tower.audio_tower.encoder.layers.28.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.28.fc2.bias  model.audio_tower.audio_tower.encoder.layers.21.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.21.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.21.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.29.fc1.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.29.fc2.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.29.fc2.bias  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.10.fc1.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.10.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.10.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.10.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.30.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.30.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.30.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.30.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.11.fc1.weight  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight

  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.11.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.11.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.11.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight

  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.31.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.31.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.31.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.31.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.22.fc1.weight

  model.audio_tower.audio_tower.encoder.layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.22.fc1.bias

  model.audio_tower.audio_tower.encoder.layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.22.fc2.weight

  model.audio_tower.audio_tower.projector.weight  model.audio_tower.audio_tower.encoder.layers.22.fc2.bias

  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight  model.audio_tower.audio_tower.projector.bias

  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias  model.audio_tower.audio_tower.classifier.weight

  model.audio_tower.audio_tower.classifier.bias  model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight


Trainable Parameters:  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias  model.embed_tokens.weight

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight  model.layers.0.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias  model.layers.0.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight  model.layers.0.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias  model.layers.0.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight
  model.layers.0.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias
  model.layers.0.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.23.fc1.weight
  model.layers.0.self_attn.o_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.fc1.bias
  model.layers.0.mlp.gate_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.fc2.weight
  model.layers.0.mlp.up_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.fc2.bias
  model.layers.0.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight  model.layers.0.input_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias  model.layers.0.post_attention_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight  model.layers.1.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight  model.layers.1.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias  model.layers.1.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight  model.layers.1.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias
  model.layers.1.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight

  model.layers.1.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias

  model.layers.1.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight

  model.layers.1.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias

  model.audio_tower.audio_tower.encoder.layers.24.fc1.weight  model.layers.1.mlp.up_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.fc1.bias  model.layers.1.mlp.down_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.fc2.weight  model.layers.1.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.24.fc2.bias  model.layers.1.post_attention_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight  model.layers.2.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias  model.layers.2.self_attn.q_proj.bias

  model.layers.2.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight

  model.layers.2.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight  model.layers.2.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias

  model.layers.2.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight


  model.layers.2.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias

  model.layers.2.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight

  model.layers.2.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight

  model.layers.2.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight


  model.layers.2.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias

  model.layers.2.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.25.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias  model.layers.3.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.fc1.bias

  model.layers.3.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.25.fc2.weight


  model.layers.3.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.fc2.bias

  model.layers.3.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight

  model.layers.3.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias


  model.layers.3.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight

  model.layers.3.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias

  model.layers.3.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias


  model.layers.3.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight

  model.layers.3.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight  model.layers.3.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight


  model.layers.3.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias

  model.layers.4.self_attn.q_proj.weight
  model.layers.4.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias
  model.layers.4.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.26.fc1.bias  model.layers.4.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.26.fc2.weight  model.layers.4.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.26.fc2.bias  model.layers.4.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.12.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight  model.layers.4.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias  model.layers.4.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight  model.layers.4.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight  model.layers.4.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias  model.layers.4.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight  model.layers.4.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias  model.layers.5.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight  model.layers.5.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias  model.layers.5.self_attn.k_proj.weight

  model.layers.5.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight

  model.layers.5.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias

  model.layers.5.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.27.fc1.weight

  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight  model.layers.5.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.27.fc1.bias


  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias  model.layers.5.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.27.fc2.weight


  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight  model.layers.5.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.27.fc2.bias


  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias  model.layers.5.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight  model.layers.5.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias


  model.layers.5.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight


  model.layers.6.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.fc1.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight


  model.layers.6.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.13.fc1.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias


  model.layers.6.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.13.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight
  model.layers.6.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.13.fc2.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias
  model.layers.6.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight  model.layers.6.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias  model.layers.6.self_attn.o_proj.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight  model.layers.6.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight  model.layers.6.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias  model.layers.6.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.fc1.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.fc1.bias  model.layers.6.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.28.fc2.weight  model.layers.6.post_attention_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight  model.audio_tower.audio_tower.encoder.layers.28.fc2.bias  model.layers.7.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight  model.layers.7.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias  model.layers.7.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight
  model.layers.7.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.14.fc1.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight  model.layers.7.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias  model.layers.7.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight  model.layers.7.self_attn.o_proj.weight

  model.layers.7.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias

  model.layers.7.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight

  model.layers.7.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias

  model.audio_tower.audio_tower.encoder.layers.14.fc1.bias  model.layers.7.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight


  model.audio_tower.audio_tower.encoder.layers.14.fc2.weight  model.layers.7.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias


  model.layers.8.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.fc2.bias  model.audio_tower.audio_tower.encoder.layers.29.fc1.weight


  model.layers.8.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.29.fc1.bias


  model.layers.8.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.29.fc2.weight


  model.layers.8.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.fc2.bias


  model.layers.8.self_attn.v_proj.weight
  model.layers.8.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight
  model.layers.8.self_attn.o_proj.weight

  model.layers.8.mlp.gate_proj.weight
  model.layers.8.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias
  model.layers.8.mlp.down_proj.weight

  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias
  model.layers.8.input_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight  model.layers.8.post_attention_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight  model.layers.9.self_attn.q_proj.weight

  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias  model.layers.9.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight  model.layers.9.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight  model.layers.9.self_attn.k_proj.bias

  model.layers.9.self_attn.v_proj.weight
  model.layers.9.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias

  model.layers.9.self_attn.o_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight

  model.layers.9.mlp.gate_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias
  model.layers.9.mlp.up_proj.weight

  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight
  model.layers.9.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias  model.layers.9.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.30.fc1.weight  model.layers.9.post_attention_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.15.fc1.weight  model.audio_tower.audio_tower.encoder.layers.30.fc1.bias  model.layers.10.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.30.fc2.weight
  model.layers.10.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.15.fc1.bias  model.audio_tower.audio_tower.encoder.layers.30.fc2.bias  model.layers.10.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight  model.layers.10.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias  model.audio_tower.audio_tower.encoder.layers.15.fc2.weight  model.layers.10.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight  model.layers.10.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.15.fc2.bias  model.layers.10.self_attn.o_proj.weight

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias
  model.layers.10.mlp.gate_proj.weight

  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight  model.layers.10.mlp.up_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias

  model.layers.10.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight
  model.layers.10.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias

  model.layers.10.post_attention_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight  model.layers.11.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias
  model.layers.11.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.31.fc1.weight  model.layers.11.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.fc1.bias
  model.layers.11.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.31.fc2.weight
  model.layers.11.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.fc2.bias


  model.layers.11.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight

  model.layers.11.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias


  model.layers.11.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layer_norm.weight

  model.layers.11.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layer_norm.bias


  model.layers.11.mlp.down_proj.weight  model.audio_tower.audio_tower.projector.weight

  model.layers.11.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight  model.audio_tower.audio_tower.projector.bias


  model.layers.11.post_attention_layernorm.weight  model.audio_tower.audio_tower.classifier.weight

  model.layers.12.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias  model.audio_tower.audio_tower.classifier.bias


  model.layers.12.self_attn.q_proj.bias

Trainable Parameters:
  model.layers.12.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight
  model.embed_tokens.weight  model.layers.12.self_attn.k_proj.bias


  model.layers.0.self_attn.q_proj.weight  model.layers.12.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias  model.layers.0.self_attn.q_proj.bias

  model.layers.0.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.fc1.weight  model.layers.0.self_attn.k_proj.bias

  model.layers.0.self_attn.v_proj.weight
  model.layers.0.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.16.fc1.bias

  model.layers.0.self_attn.o_proj.weight
  model.layers.12.self_attn.v_proj.bias  model.layers.0.mlp.gate_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.fc2.weight
  model.layers.12.self_attn.o_proj.weight  model.layers.0.mlp.up_proj.weight


  model.layers.12.mlp.gate_proj.weight  model.layers.0.mlp.down_proj.weight

  model.audio_tower.audio_tower.encoder.layers.16.fc2.bias  model.layers.12.mlp.up_proj.weight  model.layers.0.input_layernorm.weight


  model.layers.12.mlp.down_proj.weight  model.layers.0.post_attention_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight  model.layers.12.input_layernorm.weight  model.layers.1.self_attn.q_proj.weight


  model.layers.12.post_attention_layernorm.weight  model.layers.1.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias  model.layers.13.self_attn.q_proj.weight  model.layers.1.self_attn.k_proj.weight


  model.layers.13.self_attn.q_proj.bias  model.layers.1.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight  model.layers.13.self_attn.k_proj.weight  model.layers.1.self_attn.v_proj.weight


  model.layers.13.self_attn.k_proj.bias
  model.layers.1.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight  model.layers.13.self_attn.v_proj.weight  model.layers.1.self_attn.o_proj.weight


  model.layers.13.self_attn.v_proj.bias  model.layers.1.mlp.gate_proj.weight

  model.layers.13.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias  model.layers.1.mlp.up_proj.weight


  model.layers.13.mlp.gate_proj.weight  model.layers.1.mlp.down_proj.weight

  model.layers.13.mlp.up_proj.weight
  model.layers.1.input_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight  model.layers.1.post_attention_layernorm.weight
  model.layers.13.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias

  model.layers.2.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight  model.layers.13.input_layernorm.weight
  model.layers.2.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias  model.layers.13.post_attention_layernorm.weight
  model.layers.2.self_attn.k_proj.weight

  model.layers.14.self_attn.q_proj.weight  model.layers.2.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight

  model.layers.14.self_attn.q_proj.bias  model.layers.2.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias

  model.layers.14.self_attn.k_proj.weight  model.layers.2.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.fc1.weight

  model.layers.14.self_attn.k_proj.bias
  model.layers.2.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.fc1.bias

  model.layers.14.self_attn.v_proj.weight
  model.layers.2.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.fc2.weight
  model.layers.14.self_attn.v_proj.bias

  model.layers.2.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.fc2.bias
  model.layers.14.self_attn.o_proj.weight

  model.layers.2.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight
  model.layers.14.mlp.gate_proj.weight

  model.layers.2.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias
  model.layers.14.mlp.up_proj.weight

  model.layers.2.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight
  model.layers.14.mlp.down_proj.weight

  model.layers.3.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight
  model.layers.14.input_layernorm.weight

  model.layers.3.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias
  model.layers.14.post_attention_layernorm.weight

  model.layers.3.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight
  model.layers.15.self_attn.q_proj.weight

  model.layers.3.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias
  model.layers.15.self_attn.q_proj.bias

  model.layers.3.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight
  model.layers.15.self_attn.k_proj.weight

  model.layers.3.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias

  model.layers.15.self_attn.k_proj.bias
  model.layers.3.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight

  model.layers.15.self_attn.v_proj.weight
  model.layers.3.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias
  model.layers.15.self_attn.v_proj.bias

  model.layers.3.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.fc1.weight
  model.layers.15.self_attn.o_proj.weight

  model.layers.3.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.fc1.bias
  model.layers.15.mlp.gate_proj.weight

  model.layers.3.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.18.fc2.weight

  model.layers.15.mlp.up_proj.weight
  model.layers.3.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.18.fc2.bias

  model.layers.15.mlp.down_proj.weight
  model.layers.4.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight
  model.layers.15.input_layernorm.weight

  model.layers.4.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias
  model.layers.15.post_attention_layernorm.weight

  model.layers.4.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight
  model.layers.16.self_attn.q_proj.weight

  model.layers.4.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight
  model.layers.16.self_attn.q_proj.bias

  model.layers.4.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias
  model.layers.16.self_attn.k_proj.weight

  model.layers.4.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight
  model.layers.16.self_attn.k_proj.bias

  model.layers.4.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias
  model.layers.16.self_attn.v_proj.weight

  model.layers.4.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight
  model.layers.16.self_attn.v_proj.bias

  model.layers.4.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias
  model.layers.16.self_attn.o_proj.weight
  model.layers.4.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight

  model.layers.16.mlp.gate_proj.weight  model.layers.4.input_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias

  model.layers.16.mlp.up_proj.weight  model.layers.4.post_attention_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.19.fc1.weight

  model.layers.16.mlp.down_proj.weight  model.layers.5.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.fc1.bias
  model.layers.16.input_layernorm.weight
  model.layers.5.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.19.fc2.weight  model.layers.16.post_attention_layernorm.weight
  model.layers.5.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.19.fc2.bias  model.layers.17.self_attn.q_proj.weight
  model.layers.5.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight  model.layers.17.self_attn.q_proj.bias
  model.layers.5.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias  model.layers.17.self_attn.k_proj.weight
  model.layers.5.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight  model.layers.17.self_attn.k_proj.bias
  model.layers.5.self_attn.o_proj.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight  model.layers.17.self_attn.v_proj.weight
  model.layers.5.mlp.gate_proj.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias  model.layers.17.self_attn.v_proj.bias
  model.layers.5.mlp.up_proj.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight  model.layers.17.self_attn.o_proj.weight
  model.layers.5.mlp.down_proj.weight

  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias  model.layers.17.mlp.gate_proj.weight
  model.layers.5.input_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight
  model.layers.17.mlp.up_proj.weight

  model.layers.5.post_attention_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias  model.layers.17.mlp.down_proj.weight

  model.layers.6.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight  model.layers.17.input_layernorm.weight
  model.layers.6.self_attn.q_proj.bias

  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias  model.layers.17.post_attention_layernorm.weight
  model.layers.6.self_attn.k_proj.weight

  model.audio_tower.audio_tower.encoder.layers.20.fc1.weight  model.layers.18.self_attn.q_proj.weight

  model.layers.6.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.20.fc1.bias  model.layers.18.self_attn.q_proj.bias

  model.layers.6.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.fc2.weight  model.layers.18.self_attn.k_proj.weight

  model.layers.6.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.20.fc2.bias
  model.layers.18.self_attn.k_proj.bias
  model.layers.6.self_attn.o_proj.weight  model.layers.18.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight
  model.layers.6.mlp.gate_proj.weight
  model.layers.18.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias  model.layers.6.mlp.up_proj.weight  model.layers.18.self_attn.o_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight  model.layers.6.mlp.down_proj.weight  model.layers.18.mlp.gate_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight  model.layers.6.input_layernorm.weight  model.layers.18.mlp.up_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias  model.layers.6.post_attention_layernorm.weight  model.layers.18.mlp.down_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight  model.layers.7.self_attn.q_proj.weight  model.layers.18.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias  model.layers.7.self_attn.q_proj.bias  model.layers.18.post_attention_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight  model.layers.19.self_attn.q_proj.weight  model.layers.7.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias  model.layers.19.self_attn.q_proj.bias
  model.layers.7.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight  model.layers.19.self_attn.k_proj.weight  model.layers.7.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias  model.layers.19.self_attn.k_proj.bias  model.layers.7.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.21.fc1.weight  model.layers.19.self_attn.v_proj.weight  model.layers.7.self_attn.o_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.fc1.bias  model.layers.19.self_attn.v_proj.bias  model.layers.7.mlp.gate_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.fc2.weight  model.layers.19.self_attn.o_proj.weight  model.layers.7.mlp.up_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.fc2.bias  model.layers.19.mlp.gate_proj.weight  model.layers.7.mlp.down_proj.weight


  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight  model.layers.19.mlp.up_proj.weight  model.layers.7.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias  model.layers.19.mlp.down_proj.weight  model.layers.7.post_attention_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight  model.layers.19.input_layernorm.weight  model.layers.8.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight  model.layers.19.post_attention_layernorm.weight  model.layers.8.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias  model.layers.20.self_attn.q_proj.weight  model.layers.8.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight  model.layers.20.self_attn.q_proj.bias  model.layers.8.self_attn.k_proj.bias


  model.layers.20.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias  model.layers.8.self_attn.v_proj.weight


  model.layers.20.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight  model.layers.8.self_attn.v_proj.bias


  model.layers.20.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias  model.layers.8.self_attn.o_proj.weight


  model.layers.20.self_attn.v_proj.bias  model.layers.8.mlp.gate_proj.weight

  model.layers.20.self_attn.o_proj.weight  model.layers.8.mlp.up_proj.weight

  model.layers.20.mlp.gate_proj.weight  model.layers.8.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight
  model.layers.20.mlp.up_proj.weight
  model.layers.8.input_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias  model.layers.20.mlp.down_proj.weight
  model.layers.8.post_attention_layernorm.weight

  model.audio_tower.audio_tower.encoder.layers.22.fc1.weight  model.layers.20.input_layernorm.weight  model.layers.9.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.fc1.bias  model.layers.20.post_attention_layernorm.weight  model.layers.9.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.22.fc2.weight  model.layers.21.self_attn.q_proj.weight  model.layers.9.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.fc2.bias  model.layers.21.self_attn.q_proj.bias  model.layers.9.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight  model.layers.21.self_attn.k_proj.weight  model.layers.9.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias  model.layers.21.self_attn.k_proj.bias  model.layers.9.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight  model.layers.21.self_attn.v_proj.weight  model.layers.9.self_attn.o_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight  model.layers.21.self_attn.v_proj.bias  model.layers.9.mlp.gate_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias  model.layers.21.self_attn.o_proj.weight  model.layers.9.mlp.up_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight  model.layers.21.mlp.gate_proj.weight  model.layers.9.mlp.down_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias  model.layers.9.input_layernorm.weight  model.layers.21.mlp.up_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight  model.layers.9.post_attention_layernorm.weight  model.layers.21.mlp.down_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias  model.layers.10.self_attn.q_proj.weight  model.layers.21.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight  model.layers.10.self_attn.q_proj.bias  model.layers.21.post_attention_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias  model.layers.10.self_attn.k_proj.weight  model.layers.22.self_attn.q_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.fc1.weight  model.layers.10.self_attn.k_proj.bias  model.layers.22.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.23.fc1.bias  model.layers.10.self_attn.v_proj.weight  model.layers.22.self_attn.k_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.fc2.weight  model.layers.10.self_attn.v_proj.bias  model.layers.22.self_attn.k_proj.bias


  model.audio_tower.audio_tower.encoder.layers.23.fc2.bias  model.layers.10.self_attn.o_proj.weight  model.layers.22.self_attn.v_proj.weight


  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight  model.layers.10.mlp.gate_proj.weight  model.layers.22.self_attn.v_proj.bias


  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias  model.layers.10.mlp.up_proj.weight  model.layers.22.self_attn.o_proj.weight


  model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight  model.layers.10.mlp.down_proj.weight  model.layers.22.mlp.gate_proj.weight


  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight  model.layers.10.input_layernorm.weight  model.layers.22.mlp.up_proj.weight


  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias  model.layers.10.post_attention_layernorm.weight  model.layers.22.mlp.down_proj.weight


  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight  model.layers.11.self_attn.q_proj.weight  model.layers.22.input_layernorm.weight


  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias  model.layers.11.self_attn.q_proj.bias

  model.layers.22.post_attention_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight  model.layers.23.self_attn.q_proj.weight

  model.layers.11.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias  model.layers.23.self_attn.q_proj.bias


  model.layers.11.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight  model.layers.23.self_attn.k_proj.weight


  model.layers.11.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias  model.layers.23.self_attn.k_proj.bias


  model.layers.11.self_attn.v_proj.bias  model.layers.23.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.fc1.weight


  model.layers.11.self_attn.o_proj.weight  model.layers.23.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.24.fc1.bias


  model.layers.11.mlp.gate_proj.weight  model.layers.23.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.fc2.weight


  model.layers.11.mlp.up_proj.weight  model.layers.23.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.fc2.bias


  model.layers.11.mlp.down_proj.weight  model.layers.23.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight


  model.layers.11.input_layernorm.weight  model.layers.23.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias


  model.layers.11.post_attention_layernorm.weight  model.layers.23.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight


  model.layers.12.self_attn.q_proj.weight  model.layers.23.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight


  model.layers.12.self_attn.q_proj.bias  model.norm.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias


  model.layers.12.self_attn.k_proj.weight  model.mm_projector.s1_video.b1.conv1.conv.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight


  model.layers.12.self_attn.k_proj.bias  model.mm_projector.s1_video.b1.conv1.bn.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias


  model.layers.12.self_attn.v_proj.weight  model.mm_projector.s1_video.b1.conv1.bn.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight


  model.mm_projector.s1_video.b1.conv2.conv.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias

  model.mm_projector.s1_video.b1.conv2.bn.weight  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight

  model.mm_projector.s1_video.b1.conv2.bn.bias  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias  model.layers.12.self_attn.v_proj.bias


  model.mm_projector.s1_video.b1.se.fc1.weight  model.audio_tower.audio_tower.encoder.layers.25.fc1.weight  model.layers.12.self_attn.o_proj.weight


  model.mm_projector.s1_video.b1.se.fc1.bias  model.audio_tower.audio_tower.encoder.layers.25.fc1.bias  model.layers.12.mlp.gate_proj.weight


  model.mm_projector.s1_video.b1.se.fc2.weight  model.audio_tower.audio_tower.encoder.layers.25.fc2.weight  model.layers.12.mlp.up_proj.weight


  model.mm_projector.s1_video.b1.se.fc2.bias  model.audio_tower.audio_tower.encoder.layers.25.fc2.bias  model.layers.12.mlp.down_proj.weight


  model.mm_projector.s1_video.b1.conv3.conv.weight  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight  model.layers.12.input_layernorm.weight


  model.mm_projector.s1_video.b1.conv3.bn.weight  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias  model.layers.12.post_attention_layernorm.weight


  model.mm_projector.s1_video.b1.conv3.bn.bias  model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight  model.layers.13.self_attn.q_proj.weight


  model.mm_projector.s1_video.b1.downsample.conv.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight  model.layers.13.self_attn.q_proj.bias


  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias  model.mm_projector.s1_video.b1.downsample.bn.weight

  model.layers.13.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight  model.layers.13.self_attn.k_proj.bias
  model.mm_projector.s1_video.b1.downsample.bn.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias
  model.layers.13.self_attn.v_proj.weight

  model.mm_projector.s1_video.b2.conv1.conv.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight
  model.layers.13.self_attn.v_proj.bias

  model.mm_projector.s1_video.b2.conv1.bn.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias
  model.layers.13.self_attn.o_proj.weight

  model.mm_projector.s1_video.b2.conv1.bn.bias  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight
  model.layers.13.mlp.gate_proj.weight

  model.mm_projector.s1_video.b2.conv2.conv.weight  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias
  model.layers.13.mlp.up_proj.weight

  model.mm_projector.s1_video.b2.conv2.bn.weight  model.audio_tower.audio_tower.encoder.layers.26.fc1.weight
  model.layers.13.mlp.down_proj.weight
  model.mm_projector.s1_video.b2.conv2.bn.bias
  model.audio_tower.audio_tower.encoder.layers.26.fc1.bias

  model.layers.13.input_layernorm.weight  model.mm_projector.s1_video.b2.se.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.26.fc2.weight
  model.layers.13.post_attention_layernorm.weight
  model.mm_projector.s1_video.b2.se.fc1.bias

  model.audio_tower.audio_tower.encoder.layers.26.fc2.bias  model.layers.14.self_attn.q_proj.weight
  model.mm_projector.s1_video.b2.se.fc2.weight

  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight  model.layers.14.self_attn.q_proj.bias

  model.mm_projector.s1_video.b2.se.fc2.bias  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias
  model.layers.14.self_attn.k_proj.weight

  model.mm_projector.s1_video.b2.conv3.conv.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight
  model.layers.14.self_attn.k_proj.bias

  model.mm_projector.s1_video.b2.conv3.bn.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight
  model.layers.14.self_attn.v_proj.weight

  model.mm_projector.s1_video.b2.conv3.bn.bias  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias
  model.layers.14.self_attn.v_proj.bias

  model.mm_projector.s1_body.b1.conv1.conv.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight
  model.layers.14.self_attn.o_proj.weight

  model.mm_projector.s1_body.b1.conv1.bn.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias
  model.layers.14.mlp.gate_proj.weight

  model.mm_projector.s1_body.b1.conv1.bn.bias  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight
  model.layers.14.mlp.up_proj.weight

  model.mm_projector.s1_body.b1.conv2.conv.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias
  model.layers.14.mlp.down_proj.weight

  model.mm_projector.s1_body.b1.conv2.bn.weight  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight
  model.layers.14.input_layernorm.weight

  model.mm_projector.s1_body.b1.conv2.bn.bias  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias

  model.layers.14.post_attention_layernorm.weight
  model.mm_projector.s1_body.b1.se.fc1.weight  model.audio_tower.audio_tower.encoder.layers.27.fc1.weight

  model.layers.15.self_attn.q_proj.weight
  model.mm_projector.s1_body.b1.se.fc1.bias  model.audio_tower.audio_tower.encoder.layers.27.fc1.bias
  model.layers.15.self_attn.q_proj.bias

  model.mm_projector.s1_body.b1.se.fc2.weight  model.audio_tower.audio_tower.encoder.layers.27.fc2.weight

  model.layers.15.self_attn.k_proj.weight
  model.mm_projector.s1_body.b1.se.fc2.bias  model.audio_tower.audio_tower.encoder.layers.27.fc2.bias

  model.layers.15.self_attn.k_proj.bias
  model.mm_projector.s1_body.b1.conv3.conv.weight  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight
  model.layers.15.self_attn.v_proj.weight

  model.mm_projector.s1_body.b1.conv3.bn.weight  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias
  model.layers.15.self_attn.v_proj.bias
  model.mm_projector.s1_body.b1.conv3.bn.bias

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight  model.layers.15.self_attn.o_proj.weight
  model.mm_projector.s1_body.b1.downsample.conv.weight

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight  model.layers.15.mlp.gate_proj.weight
  model.mm_projector.s1_body.b1.downsample.bn.weight

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias  model.layers.15.mlp.up_proj.weight
  model.mm_projector.s1_body.b1.downsample.bn.bias

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight  model.layers.15.mlp.down_proj.weight  model.mm_projector.s1_body.b2.conv1.conv.weight


  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias  model.layers.15.input_layernorm.weight
  model.mm_projector.s1_body.b2.conv1.bn.weight

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight  model.layers.15.post_attention_layernorm.weight
  model.mm_projector.s1_body.b2.conv1.bn.bias

  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias  model.layers.16.self_attn.q_proj.weight
  model.mm_projector.s1_body.b2.conv2.conv.weight

  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight  model.layers.16.self_attn.q_proj.bias
  model.mm_projector.s1_body.b2.conv2.bn.weight

  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias  model.layers.16.self_attn.k_proj.weight
  model.mm_projector.s1_body.b2.conv2.bn.bias

  model.audio_tower.audio_tower.encoder.layers.28.fc1.weight  model.layers.16.self_attn.k_proj.bias

  model.audio_tower.audio_tower.encoder.layers.28.fc1.bias  model.layers.16.self_attn.v_proj.weight

  model.audio_tower.audio_tower.encoder.layers.28.fc2.weight  model.layers.16.self_attn.v_proj.bias

  model.audio_tower.audio_tower.encoder.layers.28.fc2.bias  model.layers.16.self_attn.o_proj.weight

  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight  model.layers.16.mlp.gate_proj.weight

  model.mm_projector.s1_body.b2.se.fc1.weight  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias
  model.layers.16.mlp.up_proj.weight

  model.mm_projector.s1_body.b2.se.fc1.bias  model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight
  model.layers.16.mlp.down_proj.weight

  model.mm_projector.s1_body.b2.se.fc2.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight

  model.layers.16.input_layernorm.weight
  model.mm_projector.s1_body.b2.se.fc2.bias  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias

  model.layers.16.post_attention_layernorm.weight  model.mm_projector.s1_body.b2.conv3.conv.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight

  model.layers.17.self_attn.q_proj.weight  model.mm_projector.s1_body.b2.conv3.bn.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias

  model.layers.17.self_attn.q_proj.bias  model.mm_projector.s1_body.b2.conv3.bn.bias  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight


  model.layers.17.self_attn.k_proj.weight  model.mm_projector.sampler_video.0.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias


  model.layers.17.self_attn.k_proj.bias  model.mm_projector.sampler_video.0.bias  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight


  model.layers.17.self_attn.v_proj.weight  model.mm_projector.sampler_body.0.weight  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias


  model.layers.17.self_attn.v_proj.bias  model.mm_projector.sampler_body.0.bias  model.audio_tower.audio_tower.encoder.layers.29.fc1.weight


  model.layers.17.self_attn.o_proj.weight  model.mm_projector.s2_video.b1.conv1.conv.weight  model.audio_tower.audio_tower.encoder.layers.29.fc1.bias


  model.layers.17.mlp.gate_proj.weight  model.mm_projector.s2_video.b1.conv1.bn.weight

  model.layers.17.mlp.up_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.fc2.weight
  model.mm_projector.s2_video.b1.conv1.bn.bias  model.audio_tower.audio_tower.encoder.layers.29.fc2.bias  model.layers.17.mlp.down_proj.weight


  model.mm_projector.s2_video.b1.conv2.conv.weight  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight

  model.layers.17.input_layernorm.weight
  model.mm_projector.s2_video.b1.conv2.bn.weight  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias

  model.layers.17.post_attention_layernorm.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight  model.mm_projector.s2_video.b1.conv2.bn.bias  model.layers.18.self_attn.q_proj.weight


  model.mm_projector.s2_video.b1.se.fc1.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight  model.layers.18.self_attn.q_proj.bias


  model.mm_projector.s2_video.b1.se.fc1.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias  model.layers.18.self_attn.k_proj.weight


  model.mm_projector.s2_video.b1.se.fc2.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight  model.layers.18.self_attn.k_proj.bias


  model.mm_projector.s2_video.b1.se.fc2.bias  model.layers.18.self_attn.v_proj.weight

  model.mm_projector.s2_video.b1.conv3.conv.weight  model.layers.18.self_attn.v_proj.bias

  model.mm_projector.s2_video.b1.conv3.bn.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias  model.layers.18.self_attn.o_proj.weight


  model.mm_projector.s2_video.b1.conv3.bn.bias  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight  model.layers.18.mlp.gate_proj.weight


  model.mm_projector.s2_video.b2.conv1.conv.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias  model.layers.18.mlp.up_proj.weight


  model.mm_projector.s2_video.b2.conv1.bn.weight  model.layers.18.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight


  model.mm_projector.s2_video.b2.conv1.bn.bias  model.layers.18.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias


  model.mm_projector.s2_video.b2.conv2.conv.weight  model.layers.18.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.30.fc1.weight


  model.mm_projector.s2_video.b2.conv2.bn.weight  model.layers.19.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.30.fc1.bias


  model.mm_projector.s2_video.b2.conv2.bn.bias  model.layers.19.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.30.fc2.weight


  model.mm_projector.s2_video.b2.se.fc1.weight  model.layers.19.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.fc2.bias

  model.mm_projector.s2_video.b2.se.fc1.bias  model.layers.19.self_attn.k_proj.bias
  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight

  model.mm_projector.s2_video.b2.se.fc2.weight  model.layers.19.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias

  model.mm_projector.s2_video.b2.se.fc2.bias  model.layers.19.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight

  model.mm_projector.s2_video.b2.conv3.conv.weight  model.layers.19.self_attn.o_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight

  model.mm_projector.s2_video.b2.conv3.bn.weight  model.layers.19.mlp.gate_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias

  model.mm_projector.s2_video.b2.conv3.bn.bias  model.layers.19.mlp.up_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight

  model.mm_projector.s2_body.b1.conv1.conv.weight  model.layers.19.mlp.down_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias

  model.mm_projector.s2_body.b1.conv1.bn.weight  model.layers.19.input_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight


  model.mm_projector.s2_body.b1.conv1.bn.bias  model.layers.19.post_attention_layernorm.weight  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias


  model.mm_projector.s2_body.b1.conv2.conv.weight  model.layers.20.self_attn.q_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight


  model.mm_projector.s2_body.b1.conv2.bn.weight  model.layers.20.self_attn.q_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias


  model.mm_projector.s2_body.b1.conv2.bn.bias  model.layers.20.self_attn.k_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.fc1.weight


  model.mm_projector.s2_body.b1.se.fc1.weight  model.layers.20.self_attn.k_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.fc1.bias


  model.mm_projector.s2_body.b1.se.fc1.bias  model.layers.20.self_attn.v_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.fc2.weight


  model.mm_projector.s2_body.b1.se.fc2.weight  model.layers.20.self_attn.v_proj.bias  model.audio_tower.audio_tower.encoder.layers.31.fc2.bias


  model.mm_projector.s2_body.b1.se.fc2.bias  model.layers.20.self_attn.o_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight


  model.mm_projector.s2_body.b1.conv3.conv.weight  model.layers.20.mlp.gate_proj.weight  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias


  model.mm_projector.s2_body.b1.conv3.bn.weight  model.layers.20.mlp.up_proj.weight  model.audio_tower.audio_tower.encoder.layer_norm.weight


  model.mm_projector.s2_body.b1.conv3.bn.bias  model.layers.20.mlp.down_proj.weight  model.audio_tower.audio_tower.encoder.layer_norm.bias


  model.mm_projector.s2_body.b2.conv1.conv.weight  model.layers.20.input_layernorm.weight  model.audio_tower.audio_tower.projector.weight


  model.mm_projector.s2_body.b2.conv1.bn.weight  model.layers.20.post_attention_layernorm.weight  model.audio_tower.audio_tower.projector.bias


  model.mm_projector.s2_body.b2.conv1.bn.bias  model.layers.21.self_attn.q_proj.weight  model.audio_tower.audio_tower.classifier.weight


  model.mm_projector.s2_body.b2.conv2.conv.weight  model.layers.21.self_attn.q_proj.bias
  model.audio_tower.audio_tower.classifier.bias

  model.mm_projector.s2_body.b2.conv2.bn.weight  model.layers.21.self_attn.k_proj.weight

Trainable Parameters:

  model.mm_projector.s2_body.b2.conv2.bn.bias  model.layers.21.self_attn.k_proj.bias

  model.embed_tokens.weight  model.mm_projector.s2_body.b2.se.fc1.weight
  model.layers.21.self_attn.v_proj.weight

  model.layers.0.self_attn.q_proj.weight  model.mm_projector.s2_body.b2.se.fc1.bias
  model.layers.21.self_attn.v_proj.bias

  model.layers.0.self_attn.q_proj.bias  model.mm_projector.s2_body.b2.se.fc2.weight
  model.layers.21.self_attn.o_proj.weight

  model.layers.0.self_attn.k_proj.weight  model.mm_projector.s2_body.b2.se.fc2.bias
  model.layers.21.mlp.gate_proj.weight

  model.layers.0.self_attn.k_proj.bias  model.mm_projector.s2_body.b2.conv3.conv.weight
  model.layers.21.mlp.up_proj.weight

  model.layers.0.self_attn.v_proj.weight  model.mm_projector.s2_body.b2.conv3.bn.weight  model.layers.21.mlp.down_proj.weight


  model.layers.0.self_attn.v_proj.bias  model.mm_projector.s2_body.b2.conv3.bn.bias  model.layers.21.input_layernorm.weight


  model.layers.0.self_attn.o_proj.weight  model.mm_projector.readout_video.0.weight  model.layers.21.post_attention_layernorm.weight


  model.layers.0.mlp.gate_proj.weight  model.mm_projector.readout_video.0.bias  model.layers.22.self_attn.q_proj.weight


  model.layers.0.mlp.up_proj.weight  model.mm_projector.readout_video.2.weight  model.layers.22.self_attn.q_proj.bias


  model.layers.0.mlp.down_proj.weight  model.mm_projector.readout_video.2.bias  model.layers.22.self_attn.k_proj.weight


  model.layers.0.input_layernorm.weight  model.mm_projector.readout_body.0.weight  model.layers.22.self_attn.k_proj.bias


  model.layers.0.post_attention_layernorm.weight  model.mm_projector.readout_body.0.bias  model.layers.22.self_attn.v_proj.weight


  model.layers.1.self_attn.q_proj.weight  model.mm_projector.readout_body.2.weight  model.layers.22.self_attn.v_proj.bias


  model.layers.1.self_attn.q_proj.bias
  model.mm_projector.readout_body.2.bias
  model.layers.22.self_attn.o_proj.weight  model.mm_projector.mlp_2xgelu_face.0.weight
  model.layers.1.self_attn.k_proj.weight
  model.layers.22.mlp.gate_proj.weight
  model.mm_projector.mlp_2xgelu_face.0.bias

  model.layers.1.self_attn.k_proj.bias  model.layers.22.mlp.up_proj.weight
  model.mm_projector.mlp_2xgelu_face.2.weight

  model.layers.1.self_attn.v_proj.weight  model.layers.22.mlp.down_proj.weight
  model.mm_projector.mlp_2xgelu_face.2.bias

  model.layers.1.self_attn.v_proj.bias  model.layers.22.input_layernorm.weight
  model.bert_model.embeddings.word_embeddings.weight

  model.layers.1.self_attn.o_proj.weight  model.layers.22.post_attention_layernorm.weight
  model.bert_model.embeddings.position_embeddings.weight

  model.layers.1.mlp.gate_proj.weight  model.layers.23.self_attn.q_proj.weight

  model.bert_model.embeddings.token_type_embeddings.weight  model.layers.1.mlp.up_proj.weight
  model.layers.23.self_attn.q_proj.bias

  model.bert_model.embeddings.LayerNorm.weight  model.layers.1.mlp.down_proj.weight
  model.layers.23.self_attn.k_proj.weight

  model.bert_model.embeddings.LayerNorm.bias  model.layers.1.input_layernorm.weight
  model.layers.23.self_attn.k_proj.bias

  model.bert_model.encoder.layer.0.attention.self.query.weight  model.layers.1.post_attention_layernorm.weight
  model.layers.23.self_attn.v_proj.weight

  model.bert_model.encoder.layer.0.attention.self.query.bias  model.layers.2.self_attn.q_proj.weight
  model.layers.23.self_attn.v_proj.bias

  model.bert_model.encoder.layer.0.attention.self.key.weight  model.layers.2.self_attn.q_proj.bias
  model.layers.23.self_attn.o_proj.weight

  model.bert_model.encoder.layer.0.attention.self.key.bias  model.layers.2.self_attn.k_proj.weight
  model.layers.23.mlp.gate_proj.weight

  model.bert_model.encoder.layer.0.attention.self.value.weight  model.layers.2.self_attn.k_proj.bias
  model.layers.23.mlp.up_proj.weight

  model.bert_model.encoder.layer.0.attention.self.value.bias  model.layers.2.self_attn.v_proj.weight
  model.layers.23.mlp.down_proj.weight

  model.bert_model.encoder.layer.0.attention.output.dense.weight  model.layers.2.self_attn.v_proj.bias
  model.layers.23.input_layernorm.weight

  model.bert_model.encoder.layer.0.attention.output.dense.bias  model.layers.2.self_attn.o_proj.weight
  model.layers.23.post_attention_layernorm.weight

  model.bert_model.encoder.layer.0.attention.output.LayerNorm.weight  model.layers.2.mlp.gate_proj.weight
  model.norm.weight

  model.bert_model.encoder.layer.0.attention.output.LayerNorm.bias  model.layers.2.mlp.up_proj.weight
  model.mm_projector.s1_video.b1.conv1.conv.weight

  model.bert_model.encoder.layer.0.intermediate.dense.weight  model.layers.2.mlp.down_proj.weight
  model.mm_projector.s1_video.b1.conv1.bn.weight

  model.bert_model.encoder.layer.0.intermediate.dense.bias  model.layers.2.input_layernorm.weight
  model.mm_projector.s1_video.b1.conv1.bn.bias

  model.bert_model.encoder.layer.0.output.dense.weight  model.layers.2.post_attention_layernorm.weight
  model.mm_projector.s1_video.b1.conv2.conv.weight

  model.bert_model.encoder.layer.0.output.dense.bias  model.layers.3.self_attn.q_proj.weight
  model.mm_projector.s1_video.b1.conv2.bn.weight
  model.bert_model.encoder.layer.0.output.LayerNorm.weight

  model.layers.3.self_attn.q_proj.bias  model.mm_projector.s1_video.b1.conv2.bn.bias
  model.bert_model.encoder.layer.0.output.LayerNorm.bias

  model.layers.3.self_attn.k_proj.weight  model.mm_projector.s1_video.b1.se.fc1.weight
  model.bert_model.encoder.layer.1.attention.self.query.weight

  model.layers.3.self_attn.k_proj.bias  model.mm_projector.s1_video.b1.se.fc1.bias
  model.bert_model.encoder.layer.1.attention.self.query.bias

  model.layers.3.self_attn.v_proj.weight  model.mm_projector.s1_video.b1.se.fc2.weight
  model.bert_model.encoder.layer.1.attention.self.key.weight

  model.layers.3.self_attn.v_proj.bias  model.mm_projector.s1_video.b1.se.fc2.bias
  model.bert_model.encoder.layer.1.attention.self.key.bias

  model.layers.3.self_attn.o_proj.weight  model.mm_projector.s1_video.b1.conv3.conv.weight
  model.bert_model.encoder.layer.1.attention.self.value.weight

  model.layers.3.mlp.gate_proj.weight  model.mm_projector.s1_video.b1.conv3.bn.weight  model.bert_model.encoder.layer.1.attention.self.value.bias


  model.layers.3.mlp.up_proj.weight  model.mm_projector.s1_video.b1.conv3.bn.bias  model.bert_model.encoder.layer.1.attention.output.dense.weight


  model.layers.3.mlp.down_proj.weight  model.mm_projector.s1_video.b1.downsample.conv.weight  model.bert_model.encoder.layer.1.attention.output.dense.bias


  model.layers.3.input_layernorm.weight  model.mm_projector.s1_video.b1.downsample.bn.weight  model.bert_model.encoder.layer.1.attention.output.LayerNorm.weight


  model.layers.3.post_attention_layernorm.weight  model.mm_projector.s1_video.b1.downsample.bn.bias
  model.bert_model.encoder.layer.1.attention.output.LayerNorm.bias

  model.layers.4.self_attn.q_proj.weight  model.mm_projector.s1_video.b2.conv1.conv.weight
  model.bert_model.encoder.layer.1.intermediate.dense.weight

  model.layers.4.self_attn.q_proj.bias  model.mm_projector.s1_video.b2.conv1.bn.weight
  model.bert_model.encoder.layer.1.intermediate.dense.bias

  model.layers.4.self_attn.k_proj.weight  model.mm_projector.s1_video.b2.conv1.bn.bias
  model.bert_model.encoder.layer.1.output.dense.weight

  model.layers.4.self_attn.k_proj.bias  model.mm_projector.s1_video.b2.conv2.conv.weight
  model.bert_model.encoder.layer.1.output.dense.bias

  model.layers.4.self_attn.v_proj.weight  model.mm_projector.s1_video.b2.conv2.bn.weight
  model.bert_model.encoder.layer.1.output.LayerNorm.weight

  model.layers.4.self_attn.v_proj.bias  model.mm_projector.s1_video.b2.conv2.bn.bias
  model.bert_model.encoder.layer.1.output.LayerNorm.bias

  model.layers.4.self_attn.o_proj.weight  model.mm_projector.s1_video.b2.se.fc1.weight
  model.bert_model.encoder.layer.2.attention.self.query.weight
  model.layers.4.mlp.gate_proj.weight
  model.mm_projector.s1_video.b2.se.fc1.bias

  model.bert_model.encoder.layer.2.attention.self.query.bias  model.layers.4.mlp.up_proj.weight
  model.mm_projector.s1_video.b2.se.fc2.weight

  model.bert_model.encoder.layer.2.attention.self.key.weight  model.layers.4.mlp.down_proj.weight
  model.mm_projector.s1_video.b2.se.fc2.bias

  model.bert_model.encoder.layer.2.attention.self.key.bias  model.layers.4.input_layernorm.weight
  model.mm_projector.s1_video.b2.conv3.conv.weight

  model.bert_model.encoder.layer.2.attention.self.value.weight  model.layers.4.post_attention_layernorm.weight
  model.mm_projector.s1_video.b2.conv3.bn.weight

  model.bert_model.encoder.layer.2.attention.self.value.bias  model.layers.5.self_attn.q_proj.weight
  model.mm_projector.s1_video.b2.conv3.bn.bias

  model.bert_model.encoder.layer.2.attention.output.dense.weight  model.layers.5.self_attn.q_proj.bias
  model.mm_projector.s1_body.b1.conv1.conv.weight

  model.bert_model.encoder.layer.2.attention.output.dense.bias  model.layers.5.self_attn.k_proj.weight
  model.mm_projector.s1_body.b1.conv1.bn.weight

  model.bert_model.encoder.layer.2.attention.output.LayerNorm.weight  model.layers.5.self_attn.k_proj.bias

  model.mm_projector.s1_body.b1.conv1.bn.bias  model.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
  model.layers.5.self_attn.v_proj.weight

  model.mm_projector.s1_body.b1.conv2.conv.weight  model.bert_model.encoder.layer.2.intermediate.dense.weight
  model.layers.5.self_attn.v_proj.bias

  model.mm_projector.s1_body.b1.conv2.bn.weight  model.bert_model.encoder.layer.2.intermediate.dense.bias
  model.layers.5.self_attn.o_proj.weight

  model.mm_projector.s1_body.b1.conv2.bn.bias  model.bert_model.encoder.layer.2.output.dense.weight
  model.layers.5.mlp.gate_proj.weight

  model.mm_projector.s1_body.b1.se.fc1.weight  model.bert_model.encoder.layer.2.output.dense.bias
  model.layers.5.mlp.up_proj.weight

  model.mm_projector.s1_body.b1.se.fc1.bias  model.bert_model.encoder.layer.2.output.LayerNorm.weight
  model.layers.5.mlp.down_proj.weight

  model.mm_projector.s1_body.b1.se.fc2.weight  model.bert_model.encoder.layer.2.output.LayerNorm.bias
  model.layers.5.input_layernorm.weight

  model.mm_projector.s1_body.b1.se.fc2.bias
  model.bert_model.encoder.layer.3.attention.self.query.weight  model.mm_projector.s1_body.b1.conv3.conv.weight

  model.layers.5.post_attention_layernorm.weight  model.mm_projector.s1_body.b1.conv3.bn.weight  model.bert_model.encoder.layer.3.attention.self.query.bias


  model.layers.6.self_attn.q_proj.weight  model.mm_projector.s1_body.b1.conv3.bn.bias  model.bert_model.encoder.layer.3.attention.self.key.weight


  model.layers.6.self_attn.q_proj.bias  model.mm_projector.s1_body.b1.downsample.conv.weight  model.bert_model.encoder.layer.3.attention.self.key.bias


  model.layers.6.self_attn.k_proj.weight  model.mm_projector.s1_body.b1.downsample.bn.weight  model.bert_model.encoder.layer.3.attention.self.value.weight


  model.layers.6.self_attn.k_proj.bias  model.mm_projector.s1_body.b1.downsample.bn.bias  model.bert_model.encoder.layer.3.attention.self.value.bias


  model.layers.6.self_attn.v_proj.weight  model.mm_projector.s1_body.b2.conv1.conv.weight
  model.bert_model.encoder.layer.3.attention.output.dense.weight

  model.layers.6.self_attn.v_proj.bias  model.mm_projector.s1_body.b2.conv1.bn.weight
  model.bert_model.encoder.layer.3.attention.output.dense.bias

  model.layers.6.self_attn.o_proj.weight  model.mm_projector.s1_body.b2.conv1.bn.bias
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.weight

  model.layers.6.mlp.gate_proj.weight  model.mm_projector.s1_body.b2.conv2.conv.weight
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.bias

  model.layers.6.mlp.up_proj.weight  model.mm_projector.s1_body.b2.conv2.bn.weight
  model.bert_model.encoder.layer.3.intermediate.dense.weight

  model.layers.6.mlp.down_proj.weight  model.mm_projector.s1_body.b2.conv2.bn.bias
  model.bert_model.encoder.layer.3.intermediate.dense.bias

  model.layers.6.input_layernorm.weight
  model.bert_model.encoder.layer.3.output.dense.weight
  model.layers.6.post_attention_layernorm.weight
  model.bert_model.encoder.layer.3.output.dense.bias
  model.layers.7.self_attn.q_proj.weight
  model.bert_model.encoder.layer.3.output.LayerNorm.weight
  model.layers.7.self_attn.q_proj.bias  model.mm_projector.s1_body.b2.se.fc1.weight
  model.bert_model.encoder.layer.3.output.LayerNorm.bias

  model.layers.7.self_attn.k_proj.weight  model.mm_projector.s1_body.b2.se.fc1.bias
  model.bert_model.encoder.layer.4.attention.self.query.weight

  model.layers.7.self_attn.k_proj.bias  model.mm_projector.s1_body.b2.se.fc2.weight
  model.bert_model.encoder.layer.4.attention.self.query.bias

  model.layers.7.self_attn.v_proj.weight  model.mm_projector.s1_body.b2.se.fc2.bias  model.bert_model.encoder.layer.4.attention.self.key.weight


  model.layers.7.self_attn.v_proj.bias  model.mm_projector.s1_body.b2.conv3.conv.weight
  model.bert_model.encoder.layer.4.attention.self.key.bias

  model.layers.7.self_attn.o_proj.weight  model.mm_projector.s1_body.b2.conv3.bn.weight
  model.bert_model.encoder.layer.4.attention.self.value.weight

  model.layers.7.mlp.gate_proj.weight  model.mm_projector.s1_body.b2.conv3.bn.bias
  model.bert_model.encoder.layer.4.attention.self.value.bias

  model.layers.7.mlp.up_proj.weight  model.mm_projector.sampler_video.0.weight
  model.bert_model.encoder.layer.4.attention.output.dense.weight

  model.layers.7.mlp.down_proj.weight  model.mm_projector.sampler_video.0.bias
  model.bert_model.encoder.layer.4.attention.output.dense.bias

  model.layers.7.input_layernorm.weight  model.mm_projector.sampler_body.0.weight
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.weight

  model.layers.7.post_attention_layernorm.weight  model.mm_projector.sampler_body.0.bias

  model.layers.8.self_attn.q_proj.weight  model.mm_projector.s2_video.b1.conv1.conv.weight

  model.layers.8.self_attn.q_proj.bias  model.mm_projector.s2_video.b1.conv1.bn.weight

  model.layers.8.self_attn.k_proj.weight
  model.mm_projector.s2_video.b1.conv1.bn.bias
  model.layers.8.self_attn.k_proj.bias
  model.mm_projector.s2_video.b1.conv2.conv.weight  model.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
  model.layers.8.self_attn.v_proj.weight

  model.mm_projector.s2_video.b1.conv2.bn.weight  model.bert_model.encoder.layer.4.intermediate.dense.weight
  model.layers.8.self_attn.v_proj.bias

  model.mm_projector.s2_video.b1.conv2.bn.bias
  model.bert_model.encoder.layer.4.intermediate.dense.bias  model.layers.8.self_attn.o_proj.weight

  model.mm_projector.s2_video.b1.se.fc1.weight
  model.bert_model.encoder.layer.4.output.dense.weight  model.layers.8.mlp.gate_proj.weight

  model.mm_projector.s2_video.b1.se.fc1.bias
  model.bert_model.encoder.layer.4.output.dense.bias  model.layers.8.mlp.up_proj.weight
  model.mm_projector.s2_video.b1.se.fc2.weight

  model.bert_model.encoder.layer.4.output.LayerNorm.weight  model.layers.8.mlp.down_proj.weight
  model.mm_projector.s2_video.b1.se.fc2.bias

  model.bert_model.encoder.layer.4.output.LayerNorm.bias  model.layers.8.input_layernorm.weight

  model.mm_projector.s2_video.b1.conv3.conv.weight
  model.bert_model.encoder.layer.5.attention.self.query.weight  model.layers.8.post_attention_layernorm.weight

  model.mm_projector.s2_video.b1.conv3.bn.weight
  model.bert_model.encoder.layer.5.attention.self.query.bias  model.layers.9.self_attn.q_proj.weight

  model.mm_projector.s2_video.b1.conv3.bn.bias
  model.bert_model.encoder.layer.5.attention.self.key.weight  model.layers.9.self_attn.q_proj.bias

  model.mm_projector.s2_video.b2.conv1.conv.weight
  model.bert_model.encoder.layer.5.attention.self.key.bias  model.layers.9.self_attn.k_proj.weight

  model.mm_projector.s2_video.b2.conv1.bn.weight
  model.bert_model.encoder.layer.5.attention.self.value.weight  model.layers.9.self_attn.k_proj.bias

  model.mm_projector.s2_video.b2.conv1.bn.bias
  model.bert_model.encoder.layer.5.attention.self.value.bias  model.layers.9.self_attn.v_proj.weight

  model.mm_projector.s2_video.b2.conv2.conv.weight
  model.bert_model.encoder.layer.5.attention.output.dense.weight  model.layers.9.self_attn.v_proj.bias
  model.mm_projector.s2_video.b2.conv2.bn.weight

  model.bert_model.encoder.layer.5.attention.output.dense.bias  model.layers.9.self_attn.o_proj.weight
  model.mm_projector.s2_video.b2.conv2.bn.bias

  model.bert_model.encoder.layer.5.attention.output.LayerNorm.weight  model.layers.9.mlp.gate_proj.weight
  model.mm_projector.s2_video.b2.se.fc1.weight

  model.bert_model.encoder.layer.5.attention.output.LayerNorm.bias  model.layers.9.mlp.up_proj.weight

  model.mm_projector.s2_video.b2.se.fc1.bias  model.bert_model.encoder.layer.5.intermediate.dense.weight
  model.layers.9.mlp.down_proj.weight

  model.mm_projector.s2_video.b2.se.fc2.weight  model.bert_model.encoder.layer.5.intermediate.dense.bias
  model.layers.9.input_layernorm.weight

  model.mm_projector.s2_video.b2.se.fc2.bias  model.bert_model.encoder.layer.5.output.dense.weight
  model.layers.9.post_attention_layernorm.weight

  model.mm_projector.s2_video.b2.conv3.conv.weight  model.bert_model.encoder.layer.5.output.dense.bias
  model.layers.10.self_attn.q_proj.weight

  model.mm_projector.s2_video.b2.conv3.bn.weight  model.bert_model.encoder.layer.5.output.LayerNorm.weight
  model.layers.10.self_attn.q_proj.bias

  model.mm_projector.s2_video.b2.conv3.bn.bias  model.bert_model.encoder.layer.5.output.LayerNorm.bias
  model.layers.10.self_attn.k_proj.weight

  model.mm_projector.s2_body.b1.conv1.conv.weight  model.bert_model.encoder.layer.6.attention.self.query.weight  model.layers.10.self_attn.k_proj.bias


  model.bert_model.encoder.layer.6.attention.self.query.bias  model.mm_projector.s2_body.b1.conv1.bn.weight  model.layers.10.self_attn.v_proj.weight


  model.bert_model.encoder.layer.6.attention.self.key.weight  model.mm_projector.s2_body.b1.conv1.bn.bias  model.layers.10.self_attn.v_proj.bias


  model.bert_model.encoder.layer.6.attention.self.key.bias  model.layers.10.self_attn.o_proj.weight  model.mm_projector.s2_body.b1.conv2.conv.weight


  model.bert_model.encoder.layer.6.attention.self.value.weight  model.layers.10.mlp.gate_proj.weight
  model.mm_projector.s2_body.b1.conv2.bn.weight

  model.bert_model.encoder.layer.6.attention.self.value.bias  model.layers.10.mlp.up_proj.weight
  model.mm_projector.s2_body.b1.conv2.bn.bias
  model.bert_model.encoder.layer.6.attention.output.dense.weight

  model.layers.10.mlp.down_proj.weight  model.mm_projector.s2_body.b1.se.fc1.weight
  model.bert_model.encoder.layer.6.attention.output.dense.bias

  model.layers.10.input_layernorm.weight  model.mm_projector.s2_body.b1.se.fc1.bias
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.weight

  model.layers.10.post_attention_layernorm.weight  model.mm_projector.s2_body.b1.se.fc2.weight
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
  model.layers.11.self_attn.q_proj.weight
  model.mm_projector.s2_body.b1.se.fc2.bias
  model.bert_model.encoder.layer.6.intermediate.dense.weight
  model.layers.11.self_attn.q_proj.bias

  model.mm_projector.s2_body.b1.conv3.conv.weight  model.bert_model.encoder.layer.6.intermediate.dense.bias
  model.layers.11.self_attn.k_proj.weight

  model.mm_projector.s2_body.b1.conv3.bn.weight  model.bert_model.encoder.layer.6.output.dense.weight
  model.layers.11.self_attn.k_proj.bias

  model.mm_projector.s2_body.b1.conv3.bn.bias  model.bert_model.encoder.layer.6.output.dense.bias
  model.layers.11.self_attn.v_proj.weight

  model.mm_projector.s2_body.b2.conv1.conv.weight  model.bert_model.encoder.layer.6.output.LayerNorm.weight
  model.layers.11.self_attn.v_proj.bias

  model.mm_projector.s2_body.b2.conv1.bn.weight  model.bert_model.encoder.layer.6.output.LayerNorm.bias  model.layers.11.self_attn.o_proj.weight


  model.mm_projector.s2_body.b2.conv1.bn.bias  model.bert_model.encoder.layer.7.attention.self.query.weight  model.layers.11.mlp.gate_proj.weight


  model.mm_projector.s2_body.b2.conv2.conv.weight  model.bert_model.encoder.layer.7.attention.self.query.bias  model.layers.11.mlp.up_proj.weight


  model.mm_projector.s2_body.b2.conv2.bn.weight  model.bert_model.encoder.layer.7.attention.self.key.weight  model.layers.11.mlp.down_proj.weight


  model.mm_projector.s2_body.b2.conv2.bn.bias  model.bert_model.encoder.layer.7.attention.self.key.bias  model.layers.11.input_layernorm.weight


  model.mm_projector.s2_body.b2.se.fc1.weight  model.bert_model.encoder.layer.7.attention.self.value.weight  model.layers.11.post_attention_layernorm.weight


  model.mm_projector.s2_body.b2.se.fc1.bias  model.bert_model.encoder.layer.7.attention.self.value.bias  model.layers.12.self_attn.q_proj.weight


  model.mm_projector.s2_body.b2.se.fc2.weight  model.bert_model.encoder.layer.7.attention.output.dense.weight  model.layers.12.self_attn.q_proj.bias


  model.mm_projector.s2_body.b2.se.fc2.bias  model.bert_model.encoder.layer.7.attention.output.dense.bias  model.layers.12.self_attn.k_proj.weight


  model.mm_projector.s2_body.b2.conv3.conv.weight  model.bert_model.encoder.layer.7.attention.output.LayerNorm.weight  model.layers.12.self_attn.k_proj.bias


  model.mm_projector.s2_body.b2.conv3.bn.weight  model.bert_model.encoder.layer.7.attention.output.LayerNorm.bias  model.layers.12.self_attn.v_proj.weight


  model.mm_projector.s2_body.b2.conv3.bn.bias  model.bert_model.encoder.layer.7.intermediate.dense.weight

  model.mm_projector.readout_video.0.weight  model.bert_model.encoder.layer.7.intermediate.dense.bias

  model.mm_projector.readout_video.0.bias  model.bert_model.encoder.layer.7.output.dense.weight

  model.layers.12.self_attn.v_proj.bias  model.mm_projector.readout_video.2.weight
  model.bert_model.encoder.layer.7.output.dense.bias
  model.mm_projector.readout_video.2.bias
  model.layers.12.self_attn.o_proj.weight

  model.bert_model.encoder.layer.7.output.LayerNorm.weight  model.mm_projector.readout_body.0.weight
  model.layers.12.mlp.gate_proj.weight

  model.bert_model.encoder.layer.7.output.LayerNorm.bias  model.mm_projector.readout_body.0.bias
  model.layers.12.mlp.up_proj.weight

  model.bert_model.encoder.layer.8.attention.self.query.weight  model.mm_projector.readout_body.2.weight
  model.layers.12.mlp.down_proj.weight

  model.bert_model.encoder.layer.8.attention.self.query.bias  model.mm_projector.readout_body.2.bias
  model.layers.12.input_layernorm.weight

  model.bert_model.encoder.layer.8.attention.self.key.weight  model.mm_projector.mlp_2xgelu_face.0.weight
  model.layers.12.post_attention_layernorm.weight

  model.bert_model.encoder.layer.8.attention.self.key.bias  model.mm_projector.mlp_2xgelu_face.0.bias
  model.layers.13.self_attn.q_proj.weight

  model.bert_model.encoder.layer.8.attention.self.value.weight  model.mm_projector.mlp_2xgelu_face.2.weight
  model.layers.13.self_attn.q_proj.bias

  model.bert_model.encoder.layer.8.attention.self.value.bias  model.mm_projector.mlp_2xgelu_face.2.bias
  model.layers.13.self_attn.k_proj.weight
  model.bert_model.encoder.layer.8.attention.output.dense.weight
  model.bert_model.embeddings.word_embeddings.weight

  model.layers.13.self_attn.k_proj.bias  model.bert_model.encoder.layer.8.attention.output.dense.bias
  model.bert_model.embeddings.position_embeddings.weight

  model.layers.13.self_attn.v_proj.weight  model.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
  model.bert_model.embeddings.token_type_embeddings.weight
  model.layers.13.self_attn.v_proj.bias
  model.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
  model.bert_model.embeddings.LayerNorm.weight
  model.layers.13.self_attn.o_proj.weight
  model.bert_model.encoder.layer.8.intermediate.dense.weight

  model.bert_model.embeddings.LayerNorm.bias  model.layers.13.mlp.gate_proj.weight  model.bert_model.encoder.layer.8.intermediate.dense.bias


  model.bert_model.encoder.layer.0.attention.self.query.weight  model.layers.13.mlp.up_proj.weight  model.bert_model.encoder.layer.8.output.dense.weight


  model.bert_model.encoder.layer.0.attention.self.query.bias  model.layers.13.mlp.down_proj.weight  model.bert_model.encoder.layer.8.output.dense.bias


  model.bert_model.encoder.layer.0.attention.self.key.weight  model.layers.13.input_layernorm.weight  model.bert_model.encoder.layer.8.output.LayerNorm.weight


  model.bert_model.encoder.layer.0.attention.self.key.bias  model.layers.13.post_attention_layernorm.weight  model.bert_model.encoder.layer.8.output.LayerNorm.bias


  model.bert_model.encoder.layer.0.attention.self.value.weight  model.layers.14.self_attn.q_proj.weight  model.bert_model.encoder.layer.9.attention.self.query.weight


  model.bert_model.encoder.layer.0.attention.self.value.bias  model.layers.14.self_attn.q_proj.bias  model.bert_model.encoder.layer.9.attention.self.query.bias


  model.bert_model.encoder.layer.0.attention.output.dense.weight  model.layers.14.self_attn.k_proj.weight  model.bert_model.encoder.layer.9.attention.self.key.weight


  model.bert_model.encoder.layer.0.attention.output.dense.bias  model.layers.14.self_attn.k_proj.bias  model.bert_model.encoder.layer.9.attention.self.key.bias


  model.bert_model.encoder.layer.0.attention.output.LayerNorm.weight  model.layers.14.self_attn.v_proj.weight  model.bert_model.encoder.layer.9.attention.self.value.weight


  model.bert_model.encoder.layer.0.attention.output.LayerNorm.bias  model.layers.14.self_attn.v_proj.bias  model.bert_model.encoder.layer.9.attention.self.value.bias


  model.bert_model.encoder.layer.0.intermediate.dense.weight  model.layers.14.self_attn.o_proj.weight  model.bert_model.encoder.layer.9.attention.output.dense.weight


  model.bert_model.encoder.layer.0.intermediate.dense.bias  model.layers.14.mlp.gate_proj.weight  model.bert_model.encoder.layer.9.attention.output.dense.bias


  model.bert_model.encoder.layer.0.output.dense.weight  model.layers.14.mlp.up_proj.weight  model.bert_model.encoder.layer.9.attention.output.LayerNorm.weight


  model.bert_model.encoder.layer.0.output.dense.bias  model.layers.14.mlp.down_proj.weight  model.bert_model.encoder.layer.9.attention.output.LayerNorm.bias


  model.bert_model.encoder.layer.0.output.LayerNorm.weight  model.layers.14.input_layernorm.weight  model.bert_model.encoder.layer.9.intermediate.dense.weight


  model.bert_model.encoder.layer.0.output.LayerNorm.bias  model.layers.14.post_attention_layernorm.weight  model.bert_model.encoder.layer.9.intermediate.dense.bias


  model.bert_model.encoder.layer.1.attention.self.query.weight  model.layers.15.self_attn.q_proj.weight  model.bert_model.encoder.layer.9.output.dense.weight


  model.bert_model.encoder.layer.1.attention.self.query.bias  model.layers.15.self_attn.q_proj.bias  model.bert_model.encoder.layer.9.output.dense.bias


  model.bert_model.encoder.layer.1.attention.self.key.weight  model.layers.15.self_attn.k_proj.weight  model.bert_model.encoder.layer.9.output.LayerNorm.weight


  model.bert_model.encoder.layer.1.attention.self.key.bias  model.layers.15.self_attn.k_proj.bias
  model.bert_model.encoder.layer.9.output.LayerNorm.bias

  model.bert_model.encoder.layer.1.attention.self.value.weight  model.layers.15.self_attn.v_proj.weight  model.bert_model.encoder.layer.10.attention.self.query.weight


  model.layers.15.self_attn.v_proj.bias  model.bert_model.encoder.layer.1.attention.self.value.bias  model.bert_model.encoder.layer.10.attention.self.query.bias


  model.layers.15.self_attn.o_proj.weight  model.bert_model.encoder.layer.1.attention.output.dense.weight  model.bert_model.encoder.layer.10.attention.self.key.weight


  model.layers.15.mlp.gate_proj.weight  model.bert_model.encoder.layer.1.attention.output.dense.bias  model.bert_model.encoder.layer.10.attention.self.key.bias


  model.layers.15.mlp.up_proj.weight  model.bert_model.encoder.layer.1.attention.output.LayerNorm.weight  model.bert_model.encoder.layer.10.attention.self.value.weight


  model.layers.15.mlp.down_proj.weight  model.bert_model.encoder.layer.1.attention.output.LayerNorm.bias  model.bert_model.encoder.layer.10.attention.self.value.bias


  model.layers.15.input_layernorm.weight  model.bert_model.encoder.layer.1.intermediate.dense.weight  model.bert_model.encoder.layer.10.attention.output.dense.weight


  model.layers.15.post_attention_layernorm.weight  model.bert_model.encoder.layer.1.intermediate.dense.bias  model.bert_model.encoder.layer.10.attention.output.dense.bias


  model.layers.16.self_attn.q_proj.weight  model.bert_model.encoder.layer.1.output.dense.weight  model.bert_model.encoder.layer.10.attention.output.LayerNorm.weight


  model.layers.16.self_attn.q_proj.bias  model.bert_model.encoder.layer.1.output.dense.bias  model.bert_model.encoder.layer.10.attention.output.LayerNorm.bias


  model.layers.16.self_attn.k_proj.weight  model.bert_model.encoder.layer.1.output.LayerNorm.weight  model.bert_model.encoder.layer.10.intermediate.dense.weight


  model.layers.16.self_attn.k_proj.bias  model.bert_model.encoder.layer.1.output.LayerNorm.bias  model.bert_model.encoder.layer.10.intermediate.dense.bias


  model.layers.16.self_attn.v_proj.weight  model.bert_model.encoder.layer.2.attention.self.query.weight  model.bert_model.encoder.layer.10.output.dense.weight


  model.layers.16.self_attn.v_proj.bias  model.bert_model.encoder.layer.2.attention.self.query.bias  model.bert_model.encoder.layer.10.output.dense.bias


  model.layers.16.self_attn.o_proj.weight  model.bert_model.encoder.layer.2.attention.self.key.weight
  model.bert_model.encoder.layer.10.output.LayerNorm.weight

  model.layers.16.mlp.gate_proj.weight  model.bert_model.encoder.layer.2.attention.self.key.bias
  model.bert_model.encoder.layer.10.output.LayerNorm.bias

  model.layers.16.mlp.up_proj.weight  model.bert_model.encoder.layer.2.attention.self.value.weight
  model.bert_model.encoder.layer.11.attention.self.query.weight

  model.layers.16.mlp.down_proj.weight  model.bert_model.encoder.layer.2.attention.self.value.bias
  model.bert_model.encoder.layer.11.attention.self.query.bias

  model.layers.16.input_layernorm.weight  model.bert_model.encoder.layer.2.attention.output.dense.weight
  model.bert_model.encoder.layer.11.attention.self.key.weight

  model.layers.16.post_attention_layernorm.weight
  model.bert_model.encoder.layer.11.attention.self.key.bias  model.bert_model.encoder.layer.2.attention.output.dense.bias

  model.layers.17.self_attn.q_proj.weight
  model.bert_model.encoder.layer.11.attention.self.value.weight  model.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
  model.layers.17.self_attn.q_proj.bias

  model.bert_model.encoder.layer.11.attention.self.value.bias  model.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
  model.layers.17.self_attn.k_proj.weight

  model.bert_model.encoder.layer.11.attention.output.dense.weight  model.bert_model.encoder.layer.2.intermediate.dense.weight
  model.layers.17.self_attn.k_proj.bias

  model.bert_model.encoder.layer.11.attention.output.dense.bias  model.bert_model.encoder.layer.2.intermediate.dense.bias
  model.layers.17.self_attn.v_proj.weight

  model.bert_model.encoder.layer.11.attention.output.LayerNorm.weight  model.bert_model.encoder.layer.2.output.dense.weight
  model.layers.17.self_attn.v_proj.bias

  model.bert_model.encoder.layer.11.attention.output.LayerNorm.bias  model.bert_model.encoder.layer.2.output.dense.bias  model.layers.17.self_attn.o_proj.weight


  model.bert_model.encoder.layer.11.intermediate.dense.weight  model.bert_model.encoder.layer.2.output.LayerNorm.weight  model.layers.17.mlp.gate_proj.weight


  model.bert_model.encoder.layer.11.intermediate.dense.bias  model.bert_model.encoder.layer.2.output.LayerNorm.bias  model.layers.17.mlp.up_proj.weight


  model.bert_model.encoder.layer.11.output.dense.weight  model.bert_model.encoder.layer.3.attention.self.query.weight  model.layers.17.mlp.down_proj.weight


  model.bert_model.encoder.layer.11.output.dense.bias  model.bert_model.encoder.layer.3.attention.self.query.bias  model.layers.17.input_layernorm.weight


  model.bert_model.encoder.layer.11.output.LayerNorm.weight  model.bert_model.encoder.layer.3.attention.self.key.weight

  model.bert_model.encoder.layer.11.output.LayerNorm.bias  model.layers.17.post_attention_layernorm.weight  model.bert_model.encoder.layer.3.attention.self.key.bias


  model.bert_model.pooler.dense.weight  model.layers.18.self_attn.q_proj.weight  model.bert_model.encoder.layer.3.attention.self.value.weight


  model.bert_model.pooler.dense.bias  model.layers.18.self_attn.q_proj.bias  model.bert_model.encoder.layer.3.attention.self.value.bias


  model.bert_gate.0.weight  model.layers.18.self_attn.k_proj.weight  model.bert_model.encoder.layer.3.attention.output.dense.weight


  model.layers.18.self_attn.k_proj.bias  model.bert_gate.0.bias  model.bert_model.encoder.layer.3.attention.output.dense.bias


  model.bert_gate.2.weight  model.layers.18.self_attn.v_proj.weight  model.bert_model.encoder.layer.3.attention.output.LayerNorm.weight


  model.bert_gate.2.bias  model.layers.18.self_attn.v_proj.bias
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.bias

  model.audio_projector.0.weight  model.layers.18.self_attn.o_proj.weight
  model.bert_model.encoder.layer.3.intermediate.dense.weight

  model.audio_projector.0.bias  model.layers.18.mlp.gate_proj.weight

  model.bert_model.encoder.layer.3.intermediate.dense.bias
  model.audio_projector.2.weight  model.layers.18.mlp.up_proj.weight

  model.bert_model.encoder.layer.3.output.dense.weight
  model.audio_projector.2.bias  model.layers.18.mlp.down_proj.weight

  model.bert_model.encoder.layer.3.output.dense.bias
  model.layers.18.input_layernorm.weight
────────────  Summary  ────────────
  model.bert_model.encoder.layer.3.output.LayerNorm.weight

  model.layers.18.post_attention_layernorm.weight
  model.bert_model.encoder.layer.3.output.LayerNorm.bias
  model.layers.19.self_attn.q_proj.weight
Total parameters     : 1,372,935,109  model.bert_model.encoder.layer.4.attention.self.query.weight

  model.layers.19.self_attn.q_proj.bias
  model.bert_model.encoder.layer.4.attention.self.query.bias  model.layers.19.self_attn.k_proj.weight

Trainable parameters : 642,753,475  (46.8160%)  model.bert_model.encoder.layer.4.attention.self.key.weight  model.layers.19.self_attn.k_proj.bias


  model.bert_model.encoder.layer.4.attention.self.key.bias  model.layers.19.self_attn.v_proj.weightFrozen parameters    : 730,181,634


  model.bert_model.encoder.layer.4.attention.self.value.weight  model.layers.19.self_attn.v_proj.bias

  model.bert_model.encoder.layer.4.attention.self.value.bias  model.layers.19.self_attn.o_proj.weight

  model.bert_model.encoder.layer.4.attention.output.dense.weight  model.layers.19.mlp.gate_proj.weight

  model.bert_model.encoder.layer.4.attention.output.dense.bias  model.layers.19.mlp.up_proj.weight

  model.bert_model.encoder.layer.4.attention.output.LayerNorm.weight  model.layers.19.mlp.down_proj.weight

  model.layers.19.input_layernorm.weight
  model.layers.19.post_attention_layernorm.weight
  model.layers.20.self_attn.q_proj.weight
  model.layers.20.self_attn.q_proj.bias
  model.layers.20.self_attn.k_proj.weight
  model.layers.20.self_attn.k_proj.bias
  model.layers.20.self_attn.v_proj.weight
  model.layers.20.self_attn.v_proj.bias
  model.layers.20.self_attn.o_proj.weight
  model.layers.20.mlp.gate_proj.weight
  model.layers.20.mlp.up_proj.weight
  model.layers.20.mlp.down_proj.weight
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.bias  model.layers.20.input_layernorm.weight

  model.bert_model.encoder.layer.4.intermediate.dense.weight  model.layers.20.post_attention_layernorm.weight

  model.bert_model.encoder.layer.4.intermediate.dense.bias  model.layers.21.self_attn.q_proj.weight

  model.bert_model.encoder.layer.4.output.dense.weight  model.layers.21.self_attn.q_proj.bias

  model.layers.21.self_attn.k_proj.weight  model.bert_model.encoder.layer.4.output.dense.bias

  model.layers.21.self_attn.k_proj.bias  model.bert_model.encoder.layer.4.output.LayerNorm.weight

  model.layers.21.self_attn.v_proj.weight  model.bert_model.encoder.layer.4.output.LayerNorm.bias

  model.layers.21.self_attn.v_proj.bias  model.bert_model.encoder.layer.5.attention.self.query.weight

  model.layers.21.self_attn.o_proj.weight  model.bert_model.encoder.layer.5.attention.self.query.bias

  model.layers.21.mlp.gate_proj.weight
  model.bert_model.encoder.layer.5.attention.self.key.weight
  model.layers.21.mlp.up_proj.weight
  model.bert_model.encoder.layer.5.attention.self.key.bias
  model.layers.21.mlp.down_proj.weight
  model.bert_model.encoder.layer.5.attention.self.value.weight
  model.layers.21.input_layernorm.weight
  model.bert_model.encoder.layer.5.attention.self.value.bias
  model.layers.21.post_attention_layernorm.weight
  model.bert_model.encoder.layer.5.attention.output.dense.weight
  model.layers.22.self_attn.q_proj.weight
  model.bert_model.encoder.layer.5.attention.output.dense.bias  model.layers.22.self_attn.q_proj.bias

  model.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
  model.layers.22.self_attn.k_proj.weight
  model.bert_model.encoder.layer.5.attention.output.LayerNorm.bias  model.layers.22.self_attn.k_proj.bias

  model.bert_model.encoder.layer.5.intermediate.dense.weight  model.layers.22.self_attn.v_proj.weight

  model.bert_model.encoder.layer.5.intermediate.dense.bias  model.layers.22.self_attn.v_proj.bias

  model.bert_model.encoder.layer.5.output.dense.weight  model.layers.22.self_attn.o_proj.weight

  model.bert_model.encoder.layer.5.output.dense.bias  model.layers.22.mlp.gate_proj.weight

  model.bert_model.encoder.layer.5.output.LayerNorm.weight  model.layers.22.mlp.up_proj.weight

  model.bert_model.encoder.layer.5.output.LayerNorm.bias  model.layers.22.mlp.down_proj.weight

  model.bert_model.encoder.layer.6.attention.self.query.weight  model.layers.22.input_layernorm.weight

  model.bert_model.encoder.layer.6.attention.self.query.bias  model.layers.22.post_attention_layernorm.weight

  model.bert_model.encoder.layer.6.attention.self.key.weight  model.layers.23.self_attn.q_proj.weight

  model.bert_model.encoder.layer.6.attention.self.key.bias  model.layers.23.self_attn.q_proj.bias

  model.bert_model.encoder.layer.6.attention.self.value.weight  model.layers.23.self_attn.k_proj.weight

  model.bert_model.encoder.layer.6.attention.self.value.bias  model.layers.23.self_attn.k_proj.bias

  model.bert_model.encoder.layer.6.attention.output.dense.weight  model.layers.23.self_attn.v_proj.weight

  model.bert_model.encoder.layer.6.attention.output.dense.bias  model.layers.23.self_attn.v_proj.bias

  model.bert_model.encoder.layer.6.attention.output.LayerNorm.weight  model.layers.23.self_attn.o_proj.weight

  model.bert_model.encoder.layer.6.attention.output.LayerNorm.bias  model.layers.23.mlp.gate_proj.weight

  model.bert_model.encoder.layer.6.intermediate.dense.weight  model.layers.23.mlp.up_proj.weight

  model.bert_model.encoder.layer.6.intermediate.dense.bias  model.layers.23.mlp.down_proj.weight

  model.bert_model.encoder.layer.6.output.dense.weight  model.layers.23.input_layernorm.weight

  model.layers.23.post_attention_layernorm.weight  model.bert_model.encoder.layer.6.output.dense.bias

  model.norm.weight  model.bert_model.encoder.layer.6.output.LayerNorm.weight

  model.mm_projector.s1_video.b1.conv1.conv.weight  model.bert_model.encoder.layer.6.output.LayerNorm.bias

  model.mm_projector.s1_video.b1.conv1.bn.weight  model.bert_model.encoder.layer.7.attention.self.query.weight

  model.mm_projector.s1_video.b1.conv1.bn.bias  model.bert_model.encoder.layer.7.attention.self.query.bias

  model.mm_projector.s1_video.b1.conv2.conv.weight  model.bert_model.encoder.layer.7.attention.self.key.weight

  model.mm_projector.s1_video.b1.conv2.bn.weight  model.bert_model.encoder.layer.7.attention.self.key.bias

  model.mm_projector.s1_video.b1.conv2.bn.bias  model.bert_model.encoder.layer.7.attention.self.value.weight

  model.mm_projector.s1_video.b1.se.fc1.weight  model.bert_model.encoder.layer.7.attention.self.value.bias

  model.mm_projector.s1_video.b1.se.fc1.bias  model.bert_model.encoder.layer.7.attention.output.dense.weight

  model.mm_projector.s1_video.b1.se.fc2.weight  model.bert_model.encoder.layer.7.attention.output.dense.bias

  model.mm_projector.s1_video.b1.se.fc2.bias  model.bert_model.encoder.layer.7.attention.output.LayerNorm.weight

  model.mm_projector.s1_video.b1.conv3.conv.weight  model.bert_model.encoder.layer.7.attention.output.LayerNorm.bias

  model.mm_projector.s1_video.b1.conv3.bn.weight  model.bert_model.encoder.layer.7.intermediate.dense.weight

  model.mm_projector.s1_video.b1.conv3.bn.bias  model.bert_model.encoder.layer.7.intermediate.dense.bias

  model.mm_projector.s1_video.b1.downsample.conv.weight  model.bert_model.encoder.layer.7.output.dense.weight

  model.mm_projector.s1_video.b1.downsample.bn.weight  model.bert_model.encoder.layer.7.output.dense.bias

  model.mm_projector.s1_video.b1.downsample.bn.bias  model.bert_model.encoder.layer.7.output.LayerNorm.weight

  model.mm_projector.s1_video.b2.conv1.conv.weight  model.bert_model.encoder.layer.7.output.LayerNorm.bias

  model.mm_projector.s1_video.b2.conv1.bn.weight  model.bert_model.encoder.layer.8.attention.self.query.weight

  model.mm_projector.s1_video.b2.conv1.bn.bias
  model.bert_model.encoder.layer.8.attention.self.query.bias
  model.mm_projector.s1_video.b2.conv2.conv.weight  model.bert_model.encoder.layer.8.attention.self.key.weight

  model.mm_projector.s1_video.b2.conv2.bn.weight  model.bert_model.encoder.layer.8.attention.self.key.bias

  model.mm_projector.s1_video.b2.conv2.bn.bias  model.bert_model.encoder.layer.8.attention.self.value.weight

  model.mm_projector.s1_video.b2.se.fc1.weight  model.bert_model.encoder.layer.8.attention.self.value.bias

  model.mm_projector.s1_video.b2.se.fc1.bias  model.bert_model.encoder.layer.8.attention.output.dense.weight

  model.mm_projector.s1_video.b2.se.fc2.weight  model.bert_model.encoder.layer.8.attention.output.dense.bias

  model.mm_projector.s1_video.b2.se.fc2.bias  model.bert_model.encoder.layer.8.attention.output.LayerNorm.weight

  model.mm_projector.s1_video.b2.conv3.conv.weight  model.bert_model.encoder.layer.8.attention.output.LayerNorm.bias

  model.mm_projector.s1_video.b2.conv3.bn.weight  model.bert_model.encoder.layer.8.intermediate.dense.weight

  model.mm_projector.s1_video.b2.conv3.bn.bias  model.bert_model.encoder.layer.8.intermediate.dense.bias

  model.mm_projector.s1_body.b1.conv1.conv.weight  model.bert_model.encoder.layer.8.output.dense.weight

  model.mm_projector.s1_body.b1.conv1.bn.weight  model.bert_model.encoder.layer.8.output.dense.bias

  model.mm_projector.s1_body.b1.conv1.bn.bias  model.bert_model.encoder.layer.8.output.LayerNorm.weight

  model.mm_projector.s1_body.b1.conv2.conv.weight  model.bert_model.encoder.layer.8.output.LayerNorm.bias

  model.mm_projector.s1_body.b1.conv2.bn.weight  model.bert_model.encoder.layer.9.attention.self.query.weight

  model.mm_projector.s1_body.b1.conv2.bn.bias  model.bert_model.encoder.layer.9.attention.self.query.bias

  model.mm_projector.s1_body.b1.se.fc1.weight  model.bert_model.encoder.layer.9.attention.self.key.weight

  model.mm_projector.s1_body.b1.se.fc1.bias  model.bert_model.encoder.layer.9.attention.self.key.bias

  model.mm_projector.s1_body.b1.se.fc2.weight  model.bert_model.encoder.layer.9.attention.self.value.weight

  model.mm_projector.s1_body.b1.se.fc2.bias  model.bert_model.encoder.layer.9.attention.self.value.bias

  model.mm_projector.s1_body.b1.conv3.conv.weight  model.bert_model.encoder.layer.9.attention.output.dense.weight

  model.mm_projector.s1_body.b1.conv3.bn.weight  model.bert_model.encoder.layer.9.attention.output.dense.bias

  model.mm_projector.s1_body.b1.conv3.bn.bias  model.bert_model.encoder.layer.9.attention.output.LayerNorm.weight

  model.mm_projector.s1_body.b1.downsample.conv.weight  model.bert_model.encoder.layer.9.attention.output.LayerNorm.bias

  model.mm_projector.s1_body.b1.downsample.bn.weight  model.bert_model.encoder.layer.9.intermediate.dense.weight

  model.mm_projector.s1_body.b1.downsample.bn.bias  model.bert_model.encoder.layer.9.intermediate.dense.bias

  model.mm_projector.s1_body.b2.conv1.conv.weight  model.bert_model.encoder.layer.9.output.dense.weight

  model.mm_projector.s1_body.b2.conv1.bn.weight  model.bert_model.encoder.layer.9.output.dense.bias

  model.mm_projector.s1_body.b2.conv1.bn.bias  model.bert_model.encoder.layer.9.output.LayerNorm.weight

  model.mm_projector.s1_body.b2.conv2.conv.weight  model.bert_model.encoder.layer.9.output.LayerNorm.bias

  model.mm_projector.s1_body.b2.conv2.bn.weight  model.bert_model.encoder.layer.10.attention.self.query.weight

  model.mm_projector.s1_body.b2.conv2.bn.bias  model.bert_model.encoder.layer.10.attention.self.query.bias

  model.bert_model.encoder.layer.10.attention.self.key.weight
  model.bert_model.encoder.layer.10.attention.self.key.bias
  model.bert_model.encoder.layer.10.attention.self.value.weight
  model.mm_projector.s1_body.b2.se.fc1.weight  model.bert_model.encoder.layer.10.attention.self.value.bias

  model.mm_projector.s1_body.b2.se.fc1.bias  model.bert_model.encoder.layer.10.attention.output.dense.weight

  model.mm_projector.s1_body.b2.se.fc2.weight  model.bert_model.encoder.layer.10.attention.output.dense.bias

  model.mm_projector.s1_body.b2.se.fc2.bias  model.bert_model.encoder.layer.10.attention.output.LayerNorm.weight

  model.mm_projector.s1_body.b2.conv3.conv.weight  model.bert_model.encoder.layer.10.attention.output.LayerNorm.bias

  model.mm_projector.s1_body.b2.conv3.bn.weight  model.bert_model.encoder.layer.10.intermediate.dense.weight

  model.mm_projector.s1_body.b2.conv3.bn.bias  model.bert_model.encoder.layer.10.intermediate.dense.bias

  model.mm_projector.sampler_video.0.weight  model.bert_model.encoder.layer.10.output.dense.weight

  model.mm_projector.sampler_video.0.bias  model.bert_model.encoder.layer.10.output.dense.bias

  model.mm_projector.sampler_body.0.weight  model.bert_model.encoder.layer.10.output.LayerNorm.weight

  model.mm_projector.sampler_body.0.bias  model.bert_model.encoder.layer.10.output.LayerNorm.bias

  model.mm_projector.s2_video.b1.conv1.conv.weight  model.bert_model.encoder.layer.11.attention.self.query.weight

  model.mm_projector.s2_video.b1.conv1.bn.weight  model.bert_model.encoder.layer.11.attention.self.query.bias

  model.mm_projector.s2_video.b1.conv1.bn.bias  model.bert_model.encoder.layer.11.attention.self.key.weight

  model.mm_projector.s2_video.b1.conv2.conv.weight  model.bert_model.encoder.layer.11.attention.self.key.bias

  model.mm_projector.s2_video.b1.conv2.bn.weight  model.bert_model.encoder.layer.11.attention.self.value.weight

  model.mm_projector.s2_video.b1.conv2.bn.bias  model.bert_model.encoder.layer.11.attention.self.value.bias

  model.mm_projector.s2_video.b1.se.fc1.weight  model.bert_model.encoder.layer.11.attention.output.dense.weight

  model.mm_projector.s2_video.b1.se.fc1.bias  model.bert_model.encoder.layer.11.attention.output.dense.bias

  model.mm_projector.s2_video.b1.se.fc2.weight  model.bert_model.encoder.layer.11.attention.output.LayerNorm.weight

  model.mm_projector.s2_video.b1.se.fc2.bias  model.bert_model.encoder.layer.11.attention.output.LayerNorm.bias

  model.mm_projector.s2_video.b1.conv3.conv.weight  model.bert_model.encoder.layer.11.intermediate.dense.weight

  model.mm_projector.s2_video.b1.conv3.bn.weight  model.bert_model.encoder.layer.11.intermediate.dense.bias

  model.mm_projector.s2_video.b1.conv3.bn.bias  model.bert_model.encoder.layer.11.output.dense.weight

  model.mm_projector.s2_video.b2.conv1.conv.weight
  model.bert_model.encoder.layer.11.output.dense.bias
  model.mm_projector.s2_video.b2.conv1.bn.weight
  model.bert_model.encoder.layer.11.output.LayerNorm.weight
  model.mm_projector.s2_video.b2.conv1.bn.bias  model.bert_model.encoder.layer.11.output.LayerNorm.bias

  model.mm_projector.s2_video.b2.conv2.conv.weight  model.bert_model.pooler.dense.weight

  model.mm_projector.s2_video.b2.conv2.bn.weight
  model.bert_model.pooler.dense.bias
  model.mm_projector.s2_video.b2.conv2.bn.bias
  model.bert_gate.0.weight
  model.mm_projector.s2_video.b2.se.fc1.weight
  model.bert_gate.0.bias
  model.mm_projector.s2_video.b2.se.fc1.bias
  model.bert_gate.2.weight
  model.mm_projector.s2_video.b2.se.fc2.weight
  model.bert_gate.2.bias  model.mm_projector.s2_video.b2.se.fc2.bias

  model.audio_projector.0.weight  model.mm_projector.s2_video.b2.conv3.conv.weight

  model.audio_projector.0.bias  model.mm_projector.s2_video.b2.conv3.bn.weight

  model.audio_projector.2.weight  model.mm_projector.s2_video.b2.conv3.bn.bias

  model.audio_projector.2.bias  model.mm_projector.s2_body.b1.conv1.conv.weight


────────────  Summary  ────────────  model.mm_projector.s2_body.b1.conv1.bn.weight

  model.mm_projector.s2_body.b1.conv1.bn.bias
Total parameters     : 1,372,935,109  model.mm_projector.s2_body.b1.conv2.conv.weight

  model.mm_projector.s2_body.b1.conv2.bn.weight
Trainable parameters : 642,753,475  (46.8160%)  model.mm_projector.s2_body.b1.conv2.bn.bias

  model.mm_projector.s2_body.b1.se.fc1.weightFrozen parameters    : 730,181,634

  model.mm_projector.s2_body.b1.se.fc1.bias
  model.mm_projector.s2_body.b1.se.fc2.weight
  model.mm_projector.s2_body.b1.se.fc2.bias
  model.mm_projector.s2_body.b1.conv3.conv.weight
  model.mm_projector.s2_body.b1.conv3.bn.weight
  model.mm_projector.s2_body.b1.conv3.bn.bias
  model.mm_projector.s2_body.b2.conv1.conv.weight
  model.mm_projector.s2_body.b2.conv1.bn.weight
  model.mm_projector.s2_body.b2.conv1.bn.bias
  model.mm_projector.s2_body.b2.conv2.conv.weight
  model.mm_projector.s2_body.b2.conv2.bn.weight
  model.mm_projector.s2_body.b2.conv2.bn.bias
  model.mm_projector.s2_body.b2.se.fc1.weight
  model.mm_projector.s2_body.b2.se.fc1.bias
  model.mm_projector.s2_body.b2.se.fc2.weight
  model.mm_projector.s2_body.b2.se.fc2.bias
  model.mm_projector.s2_body.b2.conv3.conv.weight
  model.mm_projector.s2_body.b2.conv3.bn.weight
  model.mm_projector.s2_body.b2.conv3.bn.bias
  model.mm_projector.readout_video.0.weight
  model.mm_projector.readout_video.0.bias
  model.mm_projector.readout_video.2.weight
  model.mm_projector.readout_video.2.bias
  model.mm_projector.readout_body.0.weight
  model.mm_projector.readout_body.0.bias
  model.mm_projector.readout_body.2.weight
  model.mm_projector.readout_body.2.bias
  model.mm_projector.mlp_2xgelu_face.0.weight
  model.mm_projector.mlp_2xgelu_face.0.bias
  model.mm_projector.mlp_2xgelu_face.2.weight
  model.mm_projector.mlp_2xgelu_face.2.bias
  model.bert_model.embeddings.word_embeddings.weight
  model.bert_model.embeddings.position_embeddings.weight
  model.bert_model.embeddings.token_type_embeddings.weight
  model.bert_model.embeddings.LayerNorm.weight
  model.bert_model.embeddings.LayerNorm.bias
  model.bert_model.encoder.layer.0.attention.self.query.weight
  model.bert_model.encoder.layer.0.attention.self.query.bias
  model.bert_model.encoder.layer.0.attention.self.key.weight
  model.bert_model.encoder.layer.0.attention.self.key.bias
  model.bert_model.encoder.layer.0.attention.self.value.weight
  model.bert_model.encoder.layer.0.attention.self.value.bias
  model.bert_model.encoder.layer.0.attention.output.dense.weight
  model.bert_model.encoder.layer.0.attention.output.dense.bias
  model.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.0.intermediate.dense.weight
  model.bert_model.encoder.layer.0.intermediate.dense.bias
  model.bert_model.encoder.layer.0.output.dense.weight
  model.bert_model.encoder.layer.0.output.dense.bias
  model.bert_model.encoder.layer.0.output.LayerNorm.weight
  model.bert_model.encoder.layer.0.output.LayerNorm.bias
  model.bert_model.encoder.layer.1.attention.self.query.weight
  model.bert_model.encoder.layer.1.attention.self.query.bias
  model.bert_model.encoder.layer.1.attention.self.key.weight
  model.bert_model.encoder.layer.1.attention.self.key.bias
  model.bert_model.encoder.layer.1.attention.self.value.weight
  model.bert_model.encoder.layer.1.attention.self.value.bias
  model.bert_model.encoder.layer.1.attention.output.dense.weight
  model.bert_model.encoder.layer.1.attention.output.dense.bias
  model.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.1.intermediate.dense.weight
  model.bert_model.encoder.layer.1.intermediate.dense.bias
  model.bert_model.encoder.layer.1.output.dense.weight
  model.bert_model.encoder.layer.1.output.dense.bias
  model.bert_model.encoder.layer.1.output.LayerNorm.weight
  model.bert_model.encoder.layer.1.output.LayerNorm.bias
  model.bert_model.encoder.layer.2.attention.self.query.weight
  model.bert_model.encoder.layer.2.attention.self.query.bias
  model.bert_model.encoder.layer.2.attention.self.key.weight
  model.bert_model.encoder.layer.2.attention.self.key.bias
  model.bert_model.encoder.layer.2.attention.self.value.weight
  model.bert_model.encoder.layer.2.attention.self.value.bias
  model.bert_model.encoder.layer.2.attention.output.dense.weight
  model.bert_model.encoder.layer.2.attention.output.dense.bias
  model.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.2.intermediate.dense.weight
  model.bert_model.encoder.layer.2.intermediate.dense.bias
  model.bert_model.encoder.layer.2.output.dense.weight
  model.bert_model.encoder.layer.2.output.dense.bias
  model.bert_model.encoder.layer.2.output.LayerNorm.weight
  model.bert_model.encoder.layer.2.output.LayerNorm.bias
  model.bert_model.encoder.layer.3.attention.self.query.weight
  model.bert_model.encoder.layer.3.attention.self.query.bias
  model.bert_model.encoder.layer.3.attention.self.key.weight
  model.bert_model.encoder.layer.3.attention.self.key.bias
  model.bert_model.encoder.layer.3.attention.self.value.weight
  model.bert_model.encoder.layer.3.attention.self.value.bias
  model.bert_model.encoder.layer.3.attention.output.dense.weight
  model.bert_model.encoder.layer.3.attention.output.dense.bias
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.3.intermediate.dense.weight
  model.bert_model.encoder.layer.3.intermediate.dense.bias
  model.bert_model.encoder.layer.3.output.dense.weight
  model.bert_model.encoder.layer.3.output.dense.bias
  model.bert_model.encoder.layer.3.output.LayerNorm.weight
  model.bert_model.encoder.layer.3.output.LayerNorm.bias
  model.bert_model.encoder.layer.4.attention.self.query.weight
  model.bert_model.encoder.layer.4.attention.self.query.bias
  model.bert_model.encoder.layer.4.attention.self.key.weight
  model.bert_model.encoder.layer.4.attention.self.key.bias
  model.bert_model.encoder.layer.4.attention.self.value.weight
  model.bert_model.encoder.layer.4.attention.self.value.bias
  model.bert_model.encoder.layer.4.attention.output.dense.weight
  model.bert_model.encoder.layer.4.attention.output.dense.bias
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.4.intermediate.dense.weight
  model.bert_model.encoder.layer.4.intermediate.dense.bias
  model.bert_model.encoder.layer.4.output.dense.weight
  model.bert_model.encoder.layer.4.output.dense.bias
  model.bert_model.encoder.layer.4.output.LayerNorm.weight
  model.bert_model.encoder.layer.4.output.LayerNorm.bias
  model.bert_model.encoder.layer.5.attention.self.query.weight
  model.bert_model.encoder.layer.5.attention.self.query.bias
  model.bert_model.encoder.layer.5.attention.self.key.weight
  model.bert_model.encoder.layer.5.attention.self.key.bias
  model.bert_model.encoder.layer.5.attention.self.value.weight
  model.bert_model.encoder.layer.5.attention.self.value.bias
  model.bert_model.encoder.layer.5.attention.output.dense.weight
  model.bert_model.encoder.layer.5.attention.output.dense.bias
  model.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.5.intermediate.dense.weight
  model.bert_model.encoder.layer.5.intermediate.dense.bias
  model.bert_model.encoder.layer.5.output.dense.weight
  model.bert_model.encoder.layer.5.output.dense.bias
  model.bert_model.encoder.layer.5.output.LayerNorm.weight
  model.bert_model.encoder.layer.5.output.LayerNorm.bias
  model.bert_model.encoder.layer.6.attention.self.query.weight
  model.bert_model.encoder.layer.6.attention.self.query.bias
  model.bert_model.encoder.layer.6.attention.self.key.weight
  model.bert_model.encoder.layer.6.attention.self.key.bias
  model.bert_model.encoder.layer.6.attention.self.value.weight
  model.bert_model.encoder.layer.6.attention.self.value.bias
  model.bert_model.encoder.layer.6.attention.output.dense.weight
  model.bert_model.encoder.layer.6.attention.output.dense.bias
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.6.intermediate.dense.weight
  model.bert_model.encoder.layer.6.intermediate.dense.bias
  model.bert_model.encoder.layer.6.output.dense.weight
  model.bert_model.encoder.layer.6.output.dense.bias
  model.bert_model.encoder.layer.6.output.LayerNorm.weight
  model.bert_model.encoder.layer.6.output.LayerNorm.bias
  model.bert_model.encoder.layer.7.attention.self.query.weight
  model.bert_model.encoder.layer.7.attention.self.query.bias
  model.bert_model.encoder.layer.7.attention.self.key.weight
  model.bert_model.encoder.layer.7.attention.self.key.bias
  model.bert_model.encoder.layer.7.attention.self.value.weight
  model.bert_model.encoder.layer.7.attention.self.value.bias
  model.bert_model.encoder.layer.7.attention.output.dense.weight
  model.bert_model.encoder.layer.7.attention.output.dense.bias
  model.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.7.intermediate.dense.weight
  model.bert_model.encoder.layer.7.intermediate.dense.bias
  model.bert_model.encoder.layer.7.output.dense.weight
  model.bert_model.encoder.layer.7.output.dense.bias
  model.bert_model.encoder.layer.7.output.LayerNorm.weight
  model.bert_model.encoder.layer.7.output.LayerNorm.bias
  model.bert_model.encoder.layer.8.attention.self.query.weight
  model.bert_model.encoder.layer.8.attention.self.query.bias
  model.bert_model.encoder.layer.8.attention.self.key.weight
  model.bert_model.encoder.layer.8.attention.self.key.bias
  model.bert_model.encoder.layer.8.attention.self.value.weight
  model.bert_model.encoder.layer.8.attention.self.value.bias
  model.bert_model.encoder.layer.8.attention.output.dense.weight
  model.bert_model.encoder.layer.8.attention.output.dense.bias
  model.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.8.intermediate.dense.weight
  model.bert_model.encoder.layer.8.intermediate.dense.bias
  model.bert_model.encoder.layer.8.output.dense.weight
  model.bert_model.encoder.layer.8.output.dense.bias
  model.bert_model.encoder.layer.8.output.LayerNorm.weight
  model.bert_model.encoder.layer.8.output.LayerNorm.bias
  model.bert_model.encoder.layer.9.attention.self.query.weight
  model.bert_model.encoder.layer.9.attention.self.query.bias
  model.bert_model.encoder.layer.9.attention.self.key.weight
  model.bert_model.encoder.layer.9.attention.self.key.bias
  model.bert_model.encoder.layer.9.attention.self.value.weight
  model.bert_model.encoder.layer.9.attention.self.value.bias
  model.bert_model.encoder.layer.9.attention.output.dense.weight
  model.bert_model.encoder.layer.9.attention.output.dense.bias
  model.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.9.intermediate.dense.weight
  model.bert_model.encoder.layer.9.intermediate.dense.bias
  model.bert_model.encoder.layer.9.output.dense.weight
  model.bert_model.encoder.layer.9.output.dense.bias
  model.bert_model.encoder.layer.9.output.LayerNorm.weight
  model.bert_model.encoder.layer.9.output.LayerNorm.bias
  model.bert_model.encoder.layer.10.attention.self.query.weight
  model.bert_model.encoder.layer.10.attention.self.query.bias
  model.bert_model.encoder.layer.10.attention.self.key.weight
  model.bert_model.encoder.layer.10.attention.self.key.bias
  model.bert_model.encoder.layer.10.attention.self.value.weight
  model.bert_model.encoder.layer.10.attention.self.value.bias
  model.bert_model.encoder.layer.10.attention.output.dense.weight
  model.bert_model.encoder.layer.10.attention.output.dense.bias
  model.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.10.intermediate.dense.weight
  model.bert_model.encoder.layer.10.intermediate.dense.bias
  model.bert_model.encoder.layer.10.output.dense.weight
  model.bert_model.encoder.layer.10.output.dense.bias
  model.bert_model.encoder.layer.10.output.LayerNorm.weight
  model.bert_model.encoder.layer.10.output.LayerNorm.bias
  model.bert_model.encoder.layer.11.attention.self.query.weight
  model.bert_model.encoder.layer.11.attention.self.query.bias
  model.bert_model.encoder.layer.11.attention.self.key.weight
  model.bert_model.encoder.layer.11.attention.self.key.bias
  model.bert_model.encoder.layer.11.attention.self.value.weight
  model.bert_model.encoder.layer.11.attention.self.value.bias
  model.bert_model.encoder.layer.11.attention.output.dense.weight
  model.bert_model.encoder.layer.11.attention.output.dense.bias
  model.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.11.intermediate.dense.weight
  model.bert_model.encoder.layer.11.intermediate.dense.bias
  model.bert_model.encoder.layer.11.output.dense.weight
  model.bert_model.encoder.layer.11.output.dense.bias
  model.bert_model.encoder.layer.11.output.LayerNorm.weight
  model.bert_model.encoder.layer.11.output.LayerNorm.bias
  model.bert_model.pooler.dense.weight
  model.bert_model.pooler.dense.bias
  model.bert_gate.0.weight
  model.bert_gate.0.bias
  model.bert_gate.2.weight
  model.bert_gate.2.bias
  model.audio_projector.0.weight
  model.audio_projector.0.bias
  model.audio_projector.2.weight
  model.audio_projector.2.bias

────────────  Summary  ────────────
Total parameters     : 1,372,935,109
Trainable parameters : 642,753,475  (46.8160%)
Frozen parameters    : 730,181,634
Frozen Parameters:
  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
  model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias
  model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
  model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
  model.vision_tower.vision_tower.vision_model.post_layernorm.weight
  model.vision_tower.vision_tower.vision_model.post_layernorm.bias
  model.vision_tower.vision_tower.vision_model.head.probe
  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight
  model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias
  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight
  model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias
  model.vision_tower.vision_tower.vision_model.head.layernorm.weight
  model.vision_tower.vision_tower.vision_model.head.layernorm.bias
  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight
  model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias
  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight
  model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias
  model.audio_tower.audio_tower.encoder.conv1.weight
  model.audio_tower.audio_tower.encoder.conv1.bias
  model.audio_tower.audio_tower.encoder.conv2.weight
  model.audio_tower.audio_tower.encoder.conv2.bias
  model.audio_tower.audio_tower.encoder.embed_positions.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.0.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.0.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.0.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.0.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.1.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.1.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.1.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.1.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.2.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.2.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.2.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.2.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.3.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.3.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.3.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.3.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.4.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.4.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.4.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.4.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.5.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.5.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.5.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.5.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.6.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.6.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.6.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.6.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.7.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.7.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.7.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.7.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.8.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.8.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.8.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.8.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.9.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.9.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.9.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.9.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.10.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.10.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.10.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.10.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.11.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.11.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.11.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.11.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.12.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.12.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.12.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.12.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.13.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.13.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.13.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.13.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.14.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.14.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.14.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.14.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.15.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.15.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.15.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.15.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.16.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.16.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.16.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.16.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.17.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.17.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.17.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.17.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.18.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.18.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.18.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.18.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.19.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.19.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.19.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.19.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.20.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.20.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.20.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.20.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.21.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.21.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.21.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.21.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.22.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.22.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.22.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.22.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.23.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.23.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.23.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.23.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.24.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.24.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.24.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.24.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.25.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.25.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.25.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.25.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.26.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.26.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.26.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.26.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.27.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.27.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.27.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.27.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.28.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.28.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.28.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.28.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.29.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.29.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.29.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.29.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.30.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.30.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.30.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.30.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias
  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layers.31.fc1.weight
  model.audio_tower.audio_tower.encoder.layers.31.fc1.bias
  model.audio_tower.audio_tower.encoder.layers.31.fc2.weight
  model.audio_tower.audio_tower.encoder.layers.31.fc2.bias
  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight
  model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias
  model.audio_tower.audio_tower.encoder.layer_norm.weight
  model.audio_tower.audio_tower.encoder.layer_norm.bias
  model.audio_tower.audio_tower.projector.weight
  model.audio_tower.audio_tower.projector.bias
  model.audio_tower.audio_tower.classifier.weight
  model.audio_tower.audio_tower.classifier.bias

Trainable Parameters:
  model.embed_tokens.weight
  model.layers.0.self_attn.q_proj.weight
  model.layers.0.self_attn.q_proj.bias
  model.layers.0.self_attn.k_proj.weight
  model.layers.0.self_attn.k_proj.bias
  model.layers.0.self_attn.v_proj.weight
  model.layers.0.self_attn.v_proj.bias
  model.layers.0.self_attn.o_proj.weight
  model.layers.0.mlp.gate_proj.weight
  model.layers.0.mlp.up_proj.weight
  model.layers.0.mlp.down_proj.weight
  model.layers.0.input_layernorm.weight
  model.layers.0.post_attention_layernorm.weight
  model.layers.1.self_attn.q_proj.weight
  model.layers.1.self_attn.q_proj.bias
  model.layers.1.self_attn.k_proj.weight
  model.layers.1.self_attn.k_proj.bias
  model.layers.1.self_attn.v_proj.weight
  model.layers.1.self_attn.v_proj.bias
  model.layers.1.self_attn.o_proj.weight
  model.layers.1.mlp.gate_proj.weight
  model.layers.1.mlp.up_proj.weight
  model.layers.1.mlp.down_proj.weight
  model.layers.1.input_layernorm.weight
  model.layers.1.post_attention_layernorm.weight
  model.layers.2.self_attn.q_proj.weight
  model.layers.2.self_attn.q_proj.bias
  model.layers.2.self_attn.k_proj.weight
  model.layers.2.self_attn.k_proj.bias
  model.layers.2.self_attn.v_proj.weight
  model.layers.2.self_attn.v_proj.bias
  model.layers.2.self_attn.o_proj.weight
  model.layers.2.mlp.gate_proj.weight
  model.layers.2.mlp.up_proj.weight
  model.layers.2.mlp.down_proj.weight
  model.layers.2.input_layernorm.weight
  model.layers.2.post_attention_layernorm.weight
  model.layers.3.self_attn.q_proj.weight
  model.layers.3.self_attn.q_proj.bias
  model.layers.3.self_attn.k_proj.weight
  model.layers.3.self_attn.k_proj.bias
  model.layers.3.self_attn.v_proj.weight
  model.layers.3.self_attn.v_proj.bias
  model.layers.3.self_attn.o_proj.weight
  model.layers.3.mlp.gate_proj.weight
  model.layers.3.mlp.up_proj.weight
  model.layers.3.mlp.down_proj.weight
  model.layers.3.input_layernorm.weight
  model.layers.3.post_attention_layernorm.weight
  model.layers.4.self_attn.q_proj.weight
  model.layers.4.self_attn.q_proj.bias
  model.layers.4.self_attn.k_proj.weight
  model.layers.4.self_attn.k_proj.bias
  model.layers.4.self_attn.v_proj.weight
  model.layers.4.self_attn.v_proj.bias
  model.layers.4.self_attn.o_proj.weight
  model.layers.4.mlp.gate_proj.weight
  model.layers.4.mlp.up_proj.weight
  model.layers.4.mlp.down_proj.weight
  model.layers.4.input_layernorm.weight
  model.layers.4.post_attention_layernorm.weight
  model.layers.5.self_attn.q_proj.weight
  model.layers.5.self_attn.q_proj.bias
  model.layers.5.self_attn.k_proj.weight
  model.layers.5.self_attn.k_proj.bias
  model.layers.5.self_attn.v_proj.weight
  model.layers.5.self_attn.v_proj.bias
  model.layers.5.self_attn.o_proj.weight
  model.layers.5.mlp.gate_proj.weight
  model.layers.5.mlp.up_proj.weight
  model.layers.5.mlp.down_proj.weight
  model.layers.5.input_layernorm.weight
  model.layers.5.post_attention_layernorm.weight
  model.layers.6.self_attn.q_proj.weight
  model.layers.6.self_attn.q_proj.bias
  model.layers.6.self_attn.k_proj.weight
  model.layers.6.self_attn.k_proj.bias
  model.layers.6.self_attn.v_proj.weight
  model.layers.6.self_attn.v_proj.bias
  model.layers.6.self_attn.o_proj.weight
  model.layers.6.mlp.gate_proj.weight
  model.layers.6.mlp.up_proj.weight
  model.layers.6.mlp.down_proj.weight
  model.layers.6.input_layernorm.weight
  model.layers.6.post_attention_layernorm.weight
  model.layers.7.self_attn.q_proj.weight
  model.layers.7.self_attn.q_proj.bias
  model.layers.7.self_attn.k_proj.weight
  model.layers.7.self_attn.k_proj.bias
  model.layers.7.self_attn.v_proj.weight
  model.layers.7.self_attn.v_proj.bias
  model.layers.7.self_attn.o_proj.weight
  model.layers.7.mlp.gate_proj.weight
  model.layers.7.mlp.up_proj.weight
  model.layers.7.mlp.down_proj.weight
  model.layers.7.input_layernorm.weight
  model.layers.7.post_attention_layernorm.weight
  model.layers.8.self_attn.q_proj.weight
  model.layers.8.self_attn.q_proj.bias
  model.layers.8.self_attn.k_proj.weight
  model.layers.8.self_attn.k_proj.bias
  model.layers.8.self_attn.v_proj.weight
  model.layers.8.self_attn.v_proj.bias
  model.layers.8.self_attn.o_proj.weight
  model.layers.8.mlp.gate_proj.weight
  model.layers.8.mlp.up_proj.weight
  model.layers.8.mlp.down_proj.weight
  model.layers.8.input_layernorm.weight
  model.layers.8.post_attention_layernorm.weight
  model.layers.9.self_attn.q_proj.weight
  model.layers.9.self_attn.q_proj.bias
  model.layers.9.self_attn.k_proj.weight
  model.layers.9.self_attn.k_proj.bias
  model.layers.9.self_attn.v_proj.weight
  model.layers.9.self_attn.v_proj.bias
  model.layers.9.self_attn.o_proj.weight
  model.layers.9.mlp.gate_proj.weight
  model.layers.9.mlp.up_proj.weight
  model.layers.9.mlp.down_proj.weight
  model.layers.9.input_layernorm.weight
  model.layers.9.post_attention_layernorm.weight
  model.layers.10.self_attn.q_proj.weight
  model.layers.10.self_attn.q_proj.bias
  model.layers.10.self_attn.k_proj.weight
  model.layers.10.self_attn.k_proj.bias
  model.layers.10.self_attn.v_proj.weight
  model.layers.10.self_attn.v_proj.bias
  model.layers.10.self_attn.o_proj.weight
  model.layers.10.mlp.gate_proj.weight
  model.layers.10.mlp.up_proj.weight
  model.layers.10.mlp.down_proj.weight
  model.layers.10.input_layernorm.weight
  model.layers.10.post_attention_layernorm.weight
  model.layers.11.self_attn.q_proj.weight
  model.layers.11.self_attn.q_proj.bias
  model.layers.11.self_attn.k_proj.weight
  model.layers.11.self_attn.k_proj.bias
  model.layers.11.self_attn.v_proj.weight
  model.layers.11.self_attn.v_proj.bias
  model.layers.11.self_attn.o_proj.weight
  model.layers.11.mlp.gate_proj.weight
  model.layers.11.mlp.up_proj.weight
  model.layers.11.mlp.down_proj.weight
  model.layers.11.input_layernorm.weight
  model.layers.11.post_attention_layernorm.weight
  model.layers.12.self_attn.q_proj.weight
  model.layers.12.self_attn.q_proj.bias
  model.layers.12.self_attn.k_proj.weight
  model.layers.12.self_attn.k_proj.bias
  model.layers.12.self_attn.v_proj.weight
  model.layers.12.self_attn.v_proj.bias
  model.layers.12.self_attn.o_proj.weight
  model.layers.12.mlp.gate_proj.weight
  model.layers.12.mlp.up_proj.weight
  model.layers.12.mlp.down_proj.weight
  model.layers.12.input_layernorm.weight
  model.layers.12.post_attention_layernorm.weight
  model.layers.13.self_attn.q_proj.weight
  model.layers.13.self_attn.q_proj.bias
  model.layers.13.self_attn.k_proj.weight
  model.layers.13.self_attn.k_proj.bias
  model.layers.13.self_attn.v_proj.weight
  model.layers.13.self_attn.v_proj.bias
  model.layers.13.self_attn.o_proj.weight
  model.layers.13.mlp.gate_proj.weight
  model.layers.13.mlp.up_proj.weight
  model.layers.13.mlp.down_proj.weight
  model.layers.13.input_layernorm.weight
  model.layers.13.post_attention_layernorm.weight
  model.layers.14.self_attn.q_proj.weight
  model.layers.14.self_attn.q_proj.bias
  model.layers.14.self_attn.k_proj.weight
  model.layers.14.self_attn.k_proj.bias
  model.layers.14.self_attn.v_proj.weight
  model.layers.14.self_attn.v_proj.bias
  model.layers.14.self_attn.o_proj.weight
  model.layers.14.mlp.gate_proj.weight
  model.layers.14.mlp.up_proj.weight
  model.layers.14.mlp.down_proj.weight
  model.layers.14.input_layernorm.weight
  model.layers.14.post_attention_layernorm.weight
  model.layers.15.self_attn.q_proj.weight
  model.layers.15.self_attn.q_proj.bias
  model.layers.15.self_attn.k_proj.weight
  model.layers.15.self_attn.k_proj.bias
  model.layers.15.self_attn.v_proj.weight
  model.layers.15.self_attn.v_proj.bias
  model.layers.15.self_attn.o_proj.weight
  model.layers.15.mlp.gate_proj.weight
  model.layers.15.mlp.up_proj.weight
  model.layers.15.mlp.down_proj.weight
  model.layers.15.input_layernorm.weight
  model.layers.15.post_attention_layernorm.weight
  model.layers.16.self_attn.q_proj.weight
  model.layers.16.self_attn.q_proj.bias
  model.layers.16.self_attn.k_proj.weight
  model.layers.16.self_attn.k_proj.bias
  model.layers.16.self_attn.v_proj.weight
  model.layers.16.self_attn.v_proj.bias
  model.layers.16.self_attn.o_proj.weight
  model.layers.16.mlp.gate_proj.weight
  model.layers.16.mlp.up_proj.weight
  model.layers.16.mlp.down_proj.weight
  model.layers.16.input_layernorm.weight
  model.layers.16.post_attention_layernorm.weight
  model.layers.17.self_attn.q_proj.weight
  model.layers.17.self_attn.q_proj.bias
  model.layers.17.self_attn.k_proj.weight
  model.layers.17.self_attn.k_proj.bias
  model.layers.17.self_attn.v_proj.weight
  model.layers.17.self_attn.v_proj.bias
  model.layers.17.self_attn.o_proj.weight
  model.layers.17.mlp.gate_proj.weight
  model.layers.17.mlp.up_proj.weight
  model.layers.17.mlp.down_proj.weight
  model.layers.17.input_layernorm.weight
  model.layers.17.post_attention_layernorm.weight
  model.layers.18.self_attn.q_proj.weight
  model.layers.18.self_attn.q_proj.bias
  model.layers.18.self_attn.k_proj.weight
  model.layers.18.self_attn.k_proj.bias
  model.layers.18.self_attn.v_proj.weight
  model.layers.18.self_attn.v_proj.bias
  model.layers.18.self_attn.o_proj.weight
  model.layers.18.mlp.gate_proj.weight
  model.layers.18.mlp.up_proj.weight
  model.layers.18.mlp.down_proj.weight
  model.layers.18.input_layernorm.weight
  model.layers.18.post_attention_layernorm.weight
  model.layers.19.self_attn.q_proj.weight
  model.layers.19.self_attn.q_proj.bias
  model.layers.19.self_attn.k_proj.weight
  model.layers.19.self_attn.k_proj.bias
  model.layers.19.self_attn.v_proj.weight
  model.layers.19.self_attn.v_proj.bias
  model.layers.19.self_attn.o_proj.weight
  model.layers.19.mlp.gate_proj.weight
  model.layers.19.mlp.up_proj.weight
  model.layers.19.mlp.down_proj.weight
  model.layers.19.input_layernorm.weight
  model.layers.19.post_attention_layernorm.weight
  model.layers.20.self_attn.q_proj.weight
  model.layers.20.self_attn.q_proj.bias
  model.layers.20.self_attn.k_proj.weight
  model.layers.20.self_attn.k_proj.bias
  model.layers.20.self_attn.v_proj.weight
  model.layers.20.self_attn.v_proj.bias
  model.layers.20.self_attn.o_proj.weight
  model.layers.20.mlp.gate_proj.weight
  model.layers.20.mlp.up_proj.weight
  model.layers.20.mlp.down_proj.weight
  model.layers.20.input_layernorm.weight
  model.layers.20.post_attention_layernorm.weight
  model.layers.21.self_attn.q_proj.weight
  model.layers.21.self_attn.q_proj.bias
  model.layers.21.self_attn.k_proj.weight
  model.layers.21.self_attn.k_proj.bias
  model.layers.21.self_attn.v_proj.weight
  model.layers.21.self_attn.v_proj.bias
  model.layers.21.self_attn.o_proj.weight
  model.layers.21.mlp.gate_proj.weight
  model.layers.21.mlp.up_proj.weight
  model.layers.21.mlp.down_proj.weight
  model.layers.21.input_layernorm.weight
  model.layers.21.post_attention_layernorm.weight
  model.layers.22.self_attn.q_proj.weight
  model.layers.22.self_attn.q_proj.bias
  model.layers.22.self_attn.k_proj.weight
  model.layers.22.self_attn.k_proj.bias
  model.layers.22.self_attn.v_proj.weight
  model.layers.22.self_attn.v_proj.bias
  model.layers.22.self_attn.o_proj.weight
  model.layers.22.mlp.gate_proj.weight
  model.layers.22.mlp.up_proj.weight
  model.layers.22.mlp.down_proj.weight
  model.layers.22.input_layernorm.weight
  model.layers.22.post_attention_layernorm.weight
  model.layers.23.self_attn.q_proj.weight
  model.layers.23.self_attn.q_proj.bias
  model.layers.23.self_attn.k_proj.weight
  model.layers.23.self_attn.k_proj.bias
  model.layers.23.self_attn.v_proj.weight
  model.layers.23.self_attn.v_proj.bias
  model.layers.23.self_attn.o_proj.weight
  model.layers.23.mlp.gate_proj.weight
  model.layers.23.mlp.up_proj.weight
  model.layers.23.mlp.down_proj.weight
  model.layers.23.input_layernorm.weight
  model.layers.23.post_attention_layernorm.weight
  model.norm.weight
  model.mm_projector.s1_video.b1.conv1.conv.weight
  model.mm_projector.s1_video.b1.conv1.bn.weight
  model.mm_projector.s1_video.b1.conv1.bn.bias
  model.mm_projector.s1_video.b1.conv2.conv.weight
  model.mm_projector.s1_video.b1.conv2.bn.weight
  model.mm_projector.s1_video.b1.conv2.bn.bias
  model.mm_projector.s1_video.b1.se.fc1.weight
  model.mm_projector.s1_video.b1.se.fc1.bias
  model.mm_projector.s1_video.b1.se.fc2.weight
  model.mm_projector.s1_video.b1.se.fc2.bias
  model.mm_projector.s1_video.b1.conv3.conv.weight
  model.mm_projector.s1_video.b1.conv3.bn.weight
  model.mm_projector.s1_video.b1.conv3.bn.bias
  model.mm_projector.s1_video.b1.downsample.conv.weight
  model.mm_projector.s1_video.b1.downsample.bn.weight
  model.mm_projector.s1_video.b1.downsample.bn.bias
  model.mm_projector.s1_video.b2.conv1.conv.weight
  model.mm_projector.s1_video.b2.conv1.bn.weight
  model.mm_projector.s1_video.b2.conv1.bn.bias
  model.mm_projector.s1_video.b2.conv2.conv.weight
  model.mm_projector.s1_video.b2.conv2.bn.weight
  model.mm_projector.s1_video.b2.conv2.bn.bias
  model.mm_projector.s1_video.b2.se.fc1.weight
  model.mm_projector.s1_video.b2.se.fc1.bias
  model.mm_projector.s1_video.b2.se.fc2.weight
  model.mm_projector.s1_video.b2.se.fc2.bias
  model.mm_projector.s1_video.b2.conv3.conv.weight
  model.mm_projector.s1_video.b2.conv3.bn.weight
  model.mm_projector.s1_video.b2.conv3.bn.bias
  model.mm_projector.s1_body.b1.conv1.conv.weight
  model.mm_projector.s1_body.b1.conv1.bn.weight
  model.mm_projector.s1_body.b1.conv1.bn.bias
  model.mm_projector.s1_body.b1.conv2.conv.weight
  model.mm_projector.s1_body.b1.conv2.bn.weight
  model.mm_projector.s1_body.b1.conv2.bn.bias
  model.mm_projector.s1_body.b1.se.fc1.weight
  model.mm_projector.s1_body.b1.se.fc1.bias
  model.mm_projector.s1_body.b1.se.fc2.weight
  model.mm_projector.s1_body.b1.se.fc2.bias
  model.mm_projector.s1_body.b1.conv3.conv.weight
  model.mm_projector.s1_body.b1.conv3.bn.weight
  model.mm_projector.s1_body.b1.conv3.bn.bias
  model.mm_projector.s1_body.b1.downsample.conv.weight
  model.mm_projector.s1_body.b1.downsample.bn.weight
  model.mm_projector.s1_body.b1.downsample.bn.bias
  model.mm_projector.s1_body.b2.conv1.conv.weight
  model.mm_projector.s1_body.b2.conv1.bn.weight
  model.mm_projector.s1_body.b2.conv1.bn.bias
  model.mm_projector.s1_body.b2.conv2.conv.weight
  model.mm_projector.s1_body.b2.conv2.bn.weight
  model.mm_projector.s1_body.b2.conv2.bn.bias
  model.mm_projector.s1_body.b2.se.fc1.weight
  model.mm_projector.s1_body.b2.se.fc1.bias
  model.mm_projector.s1_body.b2.se.fc2.weight
  model.mm_projector.s1_body.b2.se.fc2.bias
  model.mm_projector.s1_body.b2.conv3.conv.weight
  model.mm_projector.s1_body.b2.conv3.bn.weight
  model.mm_projector.s1_body.b2.conv3.bn.bias
  model.mm_projector.sampler_video.0.weight
  model.mm_projector.sampler_video.0.bias
  model.mm_projector.sampler_body.0.weight
  model.mm_projector.sampler_body.0.bias
  model.mm_projector.s2_video.b1.conv1.conv.weight
  model.mm_projector.s2_video.b1.conv1.bn.weight
  model.mm_projector.s2_video.b1.conv1.bn.bias
  model.mm_projector.s2_video.b1.conv2.conv.weight
  model.mm_projector.s2_video.b1.conv2.bn.weight
  model.mm_projector.s2_video.b1.conv2.bn.bias
  model.mm_projector.s2_video.b1.se.fc1.weight
  model.mm_projector.s2_video.b1.se.fc1.bias
  model.mm_projector.s2_video.b1.se.fc2.weight
  model.mm_projector.s2_video.b1.se.fc2.bias
  model.mm_projector.s2_video.b1.conv3.conv.weight
  model.mm_projector.s2_video.b1.conv3.bn.weight
  model.mm_projector.s2_video.b1.conv3.bn.bias
  model.mm_projector.s2_video.b2.conv1.conv.weight
  model.mm_projector.s2_video.b2.conv1.bn.weight
  model.mm_projector.s2_video.b2.conv1.bn.bias
  model.mm_projector.s2_video.b2.conv2.conv.weight
  model.mm_projector.s2_video.b2.conv2.bn.weight
  model.mm_projector.s2_video.b2.conv2.bn.bias
  model.mm_projector.s2_video.b2.se.fc1.weight
  model.mm_projector.s2_video.b2.se.fc1.bias
  model.mm_projector.s2_video.b2.se.fc2.weight
  model.mm_projector.s2_video.b2.se.fc2.bias
  model.mm_projector.s2_video.b2.conv3.conv.weight
  model.mm_projector.s2_video.b2.conv3.bn.weight
  model.mm_projector.s2_video.b2.conv3.bn.bias
  model.mm_projector.s2_body.b1.conv1.conv.weight
  model.mm_projector.s2_body.b1.conv1.bn.weight
  model.mm_projector.s2_body.b1.conv1.bn.bias
  model.mm_projector.s2_body.b1.conv2.conv.weight
  model.mm_projector.s2_body.b1.conv2.bn.weight
  model.mm_projector.s2_body.b1.conv2.bn.bias
  model.mm_projector.s2_body.b1.se.fc1.weight
  model.mm_projector.s2_body.b1.se.fc1.bias
  model.mm_projector.s2_body.b1.se.fc2.weight
  model.mm_projector.s2_body.b1.se.fc2.bias
  model.mm_projector.s2_body.b1.conv3.conv.weight
  model.mm_projector.s2_body.b1.conv3.bn.weight
  model.mm_projector.s2_body.b1.conv3.bn.bias
  model.mm_projector.s2_body.b2.conv1.conv.weight
  model.mm_projector.s2_body.b2.conv1.bn.weight
  model.mm_projector.s2_body.b2.conv1.bn.bias
  model.mm_projector.s2_body.b2.conv2.conv.weight
  model.mm_projector.s2_body.b2.conv2.bn.weight
  model.mm_projector.s2_body.b2.conv2.bn.bias
  model.mm_projector.s2_body.b2.se.fc1.weight
  model.mm_projector.s2_body.b2.se.fc1.bias
  model.mm_projector.s2_body.b2.se.fc2.weight
  model.mm_projector.s2_body.b2.se.fc2.bias
  model.mm_projector.s2_body.b2.conv3.conv.weight
  model.mm_projector.s2_body.b2.conv3.bn.weight
  model.mm_projector.s2_body.b2.conv3.bn.bias
  model.mm_projector.readout_video.0.weight
  model.mm_projector.readout_video.0.bias
  model.mm_projector.readout_video.2.weight
  model.mm_projector.readout_video.2.bias
  model.mm_projector.readout_body.0.weight
  model.mm_projector.readout_body.0.bias
  model.mm_projector.readout_body.2.weight
  model.mm_projector.readout_body.2.bias
  model.mm_projector.mlp_2xgelu_face.0.weight
  model.mm_projector.mlp_2xgelu_face.0.bias
  model.mm_projector.mlp_2xgelu_face.2.weight
  model.mm_projector.mlp_2xgelu_face.2.bias
  model.bert_model.embeddings.word_embeddings.weight
  model.bert_model.embeddings.position_embeddings.weight
  model.bert_model.embeddings.token_type_embeddings.weight
  model.bert_model.embeddings.LayerNorm.weight
  model.bert_model.embeddings.LayerNorm.bias
  model.bert_model.encoder.layer.0.attention.self.query.weight
  model.bert_model.encoder.layer.0.attention.self.query.bias
  model.bert_model.encoder.layer.0.attention.self.key.weight
  model.bert_model.encoder.layer.0.attention.self.key.bias
  model.bert_model.encoder.layer.0.attention.self.value.weight
  model.bert_model.encoder.layer.0.attention.self.value.bias
  model.bert_model.encoder.layer.0.attention.output.dense.weight
  model.bert_model.encoder.layer.0.attention.output.dense.bias
  model.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.0.intermediate.dense.weight
  model.bert_model.encoder.layer.0.intermediate.dense.bias
  model.bert_model.encoder.layer.0.output.dense.weight
  model.bert_model.encoder.layer.0.output.dense.bias
  model.bert_model.encoder.layer.0.output.LayerNorm.weight
  model.bert_model.encoder.layer.0.output.LayerNorm.bias
  model.bert_model.encoder.layer.1.attention.self.query.weight
  model.bert_model.encoder.layer.1.attention.self.query.bias
  model.bert_model.encoder.layer.1.attention.self.key.weight
  model.bert_model.encoder.layer.1.attention.self.key.bias
  model.bert_model.encoder.layer.1.attention.self.value.weight
  model.bert_model.encoder.layer.1.attention.self.value.bias
  model.bert_model.encoder.layer.1.attention.output.dense.weight
  model.bert_model.encoder.layer.1.attention.output.dense.bias
  model.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.1.intermediate.dense.weight
  model.bert_model.encoder.layer.1.intermediate.dense.bias
  model.bert_model.encoder.layer.1.output.dense.weight
  model.bert_model.encoder.layer.1.output.dense.bias
  model.bert_model.encoder.layer.1.output.LayerNorm.weight
  model.bert_model.encoder.layer.1.output.LayerNorm.bias
  model.bert_model.encoder.layer.2.attention.self.query.weight
  model.bert_model.encoder.layer.2.attention.self.query.bias
  model.bert_model.encoder.layer.2.attention.self.key.weight
  model.bert_model.encoder.layer.2.attention.self.key.bias
  model.bert_model.encoder.layer.2.attention.self.value.weight
  model.bert_model.encoder.layer.2.attention.self.value.bias
  model.bert_model.encoder.layer.2.attention.output.dense.weight
  model.bert_model.encoder.layer.2.attention.output.dense.bias
  model.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.2.intermediate.dense.weight
  model.bert_model.encoder.layer.2.intermediate.dense.bias
  model.bert_model.encoder.layer.2.output.dense.weight
  model.bert_model.encoder.layer.2.output.dense.bias
  model.bert_model.encoder.layer.2.output.LayerNorm.weight
  model.bert_model.encoder.layer.2.output.LayerNorm.bias
  model.bert_model.encoder.layer.3.attention.self.query.weight
  model.bert_model.encoder.layer.3.attention.self.query.bias
  model.bert_model.encoder.layer.3.attention.self.key.weight
  model.bert_model.encoder.layer.3.attention.self.key.bias
  model.bert_model.encoder.layer.3.attention.self.value.weight
  model.bert_model.encoder.layer.3.attention.self.value.bias
  model.bert_model.encoder.layer.3.attention.output.dense.weight
  model.bert_model.encoder.layer.3.attention.output.dense.bias
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.3.intermediate.dense.weight
  model.bert_model.encoder.layer.3.intermediate.dense.bias
  model.bert_model.encoder.layer.3.output.dense.weight
  model.bert_model.encoder.layer.3.output.dense.bias
  model.bert_model.encoder.layer.3.output.LayerNorm.weight
  model.bert_model.encoder.layer.3.output.LayerNorm.bias
  model.bert_model.encoder.layer.4.attention.self.query.weight
  model.bert_model.encoder.layer.4.attention.self.query.bias
  model.bert_model.encoder.layer.4.attention.self.key.weight
  model.bert_model.encoder.layer.4.attention.self.key.bias
  model.bert_model.encoder.layer.4.attention.self.value.weight
  model.bert_model.encoder.layer.4.attention.self.value.bias
  model.bert_model.encoder.layer.4.attention.output.dense.weight
  model.bert_model.encoder.layer.4.attention.output.dense.bias
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.4.intermediate.dense.weight
  model.bert_model.encoder.layer.4.intermediate.dense.bias
  model.bert_model.encoder.layer.4.output.dense.weight
  model.bert_model.encoder.layer.4.output.dense.bias
  model.bert_model.encoder.layer.4.output.LayerNorm.weight
  model.bert_model.encoder.layer.4.output.LayerNorm.bias
  model.bert_model.encoder.layer.5.attention.self.query.weight
  model.bert_model.encoder.layer.5.attention.self.query.bias
  model.bert_model.encoder.layer.5.attention.self.key.weight
  model.bert_model.encoder.layer.5.attention.self.key.bias
  model.bert_model.encoder.layer.5.attention.self.value.weight
  model.bert_model.encoder.layer.5.attention.self.value.bias
  model.bert_model.encoder.layer.5.attention.output.dense.weight
  model.bert_model.encoder.layer.5.attention.output.dense.bias
  model.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.5.intermediate.dense.weight
  model.bert_model.encoder.layer.5.intermediate.dense.bias
  model.bert_model.encoder.layer.5.output.dense.weight
  model.bert_model.encoder.layer.5.output.dense.bias
  model.bert_model.encoder.layer.5.output.LayerNorm.weight
  model.bert_model.encoder.layer.5.output.LayerNorm.bias
  model.bert_model.encoder.layer.6.attention.self.query.weight
  model.bert_model.encoder.layer.6.attention.self.query.bias
  model.bert_model.encoder.layer.6.attention.self.key.weight
  model.bert_model.encoder.layer.6.attention.self.key.bias
  model.bert_model.encoder.layer.6.attention.self.value.weight
  model.bert_model.encoder.layer.6.attention.self.value.bias
  model.bert_model.encoder.layer.6.attention.output.dense.weight
  model.bert_model.encoder.layer.6.attention.output.dense.bias
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.6.intermediate.dense.weight
  model.bert_model.encoder.layer.6.intermediate.dense.bias
  model.bert_model.encoder.layer.6.output.dense.weight
  model.bert_model.encoder.layer.6.output.dense.bias
  model.bert_model.encoder.layer.6.output.LayerNorm.weight
  model.bert_model.encoder.layer.6.output.LayerNorm.bias
  model.bert_model.encoder.layer.7.attention.self.query.weight
  model.bert_model.encoder.layer.7.attention.self.query.bias
  model.bert_model.encoder.layer.7.attention.self.key.weight
  model.bert_model.encoder.layer.7.attention.self.key.bias
  model.bert_model.encoder.layer.7.attention.self.value.weight
  model.bert_model.encoder.layer.7.attention.self.value.bias
  model.bert_model.encoder.layer.7.attention.output.dense.weight
  model.bert_model.encoder.layer.7.attention.output.dense.bias
  model.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.7.intermediate.dense.weight
  model.bert_model.encoder.layer.7.intermediate.dense.bias
  model.bert_model.encoder.layer.7.output.dense.weight
  model.bert_model.encoder.layer.7.output.dense.bias
  model.bert_model.encoder.layer.7.output.LayerNorm.weight
  model.bert_model.encoder.layer.7.output.LayerNorm.bias
  model.bert_model.encoder.layer.8.attention.self.query.weight
  model.bert_model.encoder.layer.8.attention.self.query.bias
  model.bert_model.encoder.layer.8.attention.self.key.weight
  model.bert_model.encoder.layer.8.attention.self.key.bias
  model.bert_model.encoder.layer.8.attention.self.value.weight
  model.bert_model.encoder.layer.8.attention.self.value.bias
  model.bert_model.encoder.layer.8.attention.output.dense.weight
  model.bert_model.encoder.layer.8.attention.output.dense.bias
  model.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.8.intermediate.dense.weight
  model.bert_model.encoder.layer.8.intermediate.dense.bias
  model.bert_model.encoder.layer.8.output.dense.weight
  model.bert_model.encoder.layer.8.output.dense.bias
  model.bert_model.encoder.layer.8.output.LayerNorm.weight
  model.bert_model.encoder.layer.8.output.LayerNorm.bias
  model.bert_model.encoder.layer.9.attention.self.query.weight
  model.bert_model.encoder.layer.9.attention.self.query.bias
  model.bert_model.encoder.layer.9.attention.self.key.weight
  model.bert_model.encoder.layer.9.attention.self.key.bias
  model.bert_model.encoder.layer.9.attention.self.value.weight
  model.bert_model.encoder.layer.9.attention.self.value.bias
  model.bert_model.encoder.layer.9.attention.output.dense.weight
  model.bert_model.encoder.layer.9.attention.output.dense.bias
  model.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.9.intermediate.dense.weight
  model.bert_model.encoder.layer.9.intermediate.dense.bias
  model.bert_model.encoder.layer.9.output.dense.weight
  model.bert_model.encoder.layer.9.output.dense.bias
  model.bert_model.encoder.layer.9.output.LayerNorm.weight
  model.bert_model.encoder.layer.9.output.LayerNorm.bias
  model.bert_model.encoder.layer.10.attention.self.query.weight
  model.bert_model.encoder.layer.10.attention.self.query.bias
  model.bert_model.encoder.layer.10.attention.self.key.weight
  model.bert_model.encoder.layer.10.attention.self.key.bias
  model.bert_model.encoder.layer.10.attention.self.value.weight
  model.bert_model.encoder.layer.10.attention.self.value.bias
  model.bert_model.encoder.layer.10.attention.output.dense.weight
  model.bert_model.encoder.layer.10.attention.output.dense.bias
  model.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.10.intermediate.dense.weight
  model.bert_model.encoder.layer.10.intermediate.dense.bias
  model.bert_model.encoder.layer.10.output.dense.weight
  model.bert_model.encoder.layer.10.output.dense.bias
  model.bert_model.encoder.layer.10.output.LayerNorm.weight
  model.bert_model.encoder.layer.10.output.LayerNorm.bias
  model.bert_model.encoder.layer.11.attention.self.query.weight
  model.bert_model.encoder.layer.11.attention.self.query.bias
  model.bert_model.encoder.layer.11.attention.self.key.weight
  model.bert_model.encoder.layer.11.attention.self.key.bias
  model.bert_model.encoder.layer.11.attention.self.value.weight
  model.bert_model.encoder.layer.11.attention.self.value.bias
  model.bert_model.encoder.layer.11.attention.output.dense.weight
  model.bert_model.encoder.layer.11.attention.output.dense.bias
  model.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
  model.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
  model.bert_model.encoder.layer.11.intermediate.dense.weight
  model.bert_model.encoder.layer.11.intermediate.dense.bias
  model.bert_model.encoder.layer.11.output.dense.weight
  model.bert_model.encoder.layer.11.output.dense.bias
  model.bert_model.encoder.layer.11.output.LayerNorm.weight
  model.bert_model.encoder.layer.11.output.LayerNorm.bias
  model.bert_model.pooler.dense.weight
  model.bert_model.pooler.dense.bias
  model.bert_gate.0.weight
  model.bert_gate.0.bias
  model.bert_gate.2.weight
  model.bert_gate.2.bias
  model.audio_projector.0.weight
  model.audio_projector.0.bias
  model.audio_projector.2.weight
  model.audio_projector.2.bias

────────────  Summary  ────────────
Total parameters     : 1,372,935,109
Trainable parameters : 642,753,475  (46.8160%)
Frozen parameters    : 730,181,634
[2025-11-07 02:09:19,344] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:19,344] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:19,344] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:19,358] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:19,923] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1947, num_elems = 2.29B
Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.classifier.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13Some weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.projector.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.classifier.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower..self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_toweraudio_tower.encoder.layers.19.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_toSome weights of the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/work_dirs/humanomniqwen2_siglip/finetune_11-06-2025-EMER_MAFW_DFEW-ERV_cold_start_new_format_av-bs36-ga3-HumanOmni-EPOCH5-LR2e-5-mm_language_model,mm_mlp_adapter,audio_projector,mm_vision_tower/checkpoint-200 were not used when initializing HumanOmniQwen2ForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.projector.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_pro.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.wewer.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_j.weight', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_ight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encomodel.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.projector.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', norm.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.conv1.bias', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.classifier.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.classifier.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_pder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.audio_tower.audio_tower.encoder.conv1.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.projector.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
'model.audio_tower.audio_tower.encoder.conv2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.30.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.26.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.enroj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.1.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.conv2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.audicoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.2.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.7.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.4.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.16.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.1.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', o_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.27.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.27.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.24.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.22.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.3.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.18.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.31.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.28.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.final_layer_norm.weight', 'm'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.28.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.4.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.audio_tower.audio_tower.encoder.layers.5.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.3.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.k_proj.weight'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
odel.audio_tower.audio_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.29.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.29.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.7.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.5.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.29.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.31.fc2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.26.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.30.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.11.fc2.bias', 'model.audio_tower.audio_tower.encoder.embed_positions.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.4.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.21.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.13.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.26.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.21.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.31.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.23.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.10.final_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.6.final_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.10.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.9.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.30.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.19.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.18.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.8.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.27.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.17.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.2.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.11.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.20.fc1.weight', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.28.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.24.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.audio_tower.audio_tower.encoder.layers.12.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.19.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.12.final_layer_norm.bias', 'model.audio_tower.audio_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.25.self_attn.out_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.6.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.22.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.23.final_layer_norm.weight', 'model.audio_tower.audio_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.audio_tower.audio_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.audio_tower.audio_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.audio_tower.audio_tower.encoder.layers.15.fc1.bias', 'model.audio_tower.audio_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.audio_tower.audio_tower.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight'}
- This IS expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing HumanOmniQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2025-11-07 02:09:20,829] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:20,839] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:20,847] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:20,873] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:21,307] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 2155, num_elems = 2.38B
[2025-11-07 02:09:21,510] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:21,510] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:21,510] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:21,519] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:21,720] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 2646, num_elems = 3.02B
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at /mnt/ssd_hs/Exp/R1-Omni/pre-trained/whisper-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 02:09:25,356] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:25,359] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed info: version=0.15.3, git-hash=unknown, git-branch=unknown
[2025-11-07 02:09:25,360] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-11-07 02:09:25,386] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-11-07 02:09:25,390] [INFO] [logging.py:129:log_dist] [Rank 0] Creating ZeRO Offload
[RESUME] output_dir does not exist or is empty, starting fresh.
Trainer class used: <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
train() defined in: /mnt/ssd_hs/envs/r1-v/lib/python3.11/site-packages/transformers/trainer.py
get_train_dataloader() defined in: /mnt/ssd_hs/Exp/R1-Omni/src/r1-v/src/open_r1/trainer/humanOmni_grpo_text_trainer_attention_AU.py
[2025-11-07 02:09:25,444] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[RESUME] output_dir does not exist or is empty, starting fresh.
Trainer class used: <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
train() defined in: /mnt/ssd_hs/envs/r1-v/lib/python3.11/site-packages/transformers/trainer.py
get_train_dataloader() defined in: /mnt/ssd_hs/Exp/R1-Omni/src/r1-v/src/open_r1/trainer/humanOmni_grpo_text_trainer_attention_AU.py
[2025-11-07 02:09:25,815] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-11-07 02:09:25,816] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 3.11 GB         CA 3.73 GB         Max_CA 4 GB 
[2025-11-07 02:09:25,816] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 132.32 GB, percent = 35.1%
[2025-11-07 02:09:25,820] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Parameter Offload: Total persistent parameters: 967237 in 759 params
[RESUME] output_dir does not exist or is empty, starting fresh.
Trainer class used: <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
train() defined in: /mnt/ssd_hs/envs/r1-v/lib/python3.11/site-packages/transformers/trainer.py
get_train_dataloader() defined in: /mnt/ssd_hs/Exp/R1-Omni/src/r1-v/src/open_r1/trainer/humanOmni_grpo_text_trainer_attention_AU.py
[2025-11-07 02:09:26,190] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-11-07 02:09:26,191] [INFO] [utils.py:782:see_memory_usage] MA 2.95 GB         Max_MA 2.95 GB         CA 3.73 GB         Max_CA 4 GB 
[2025-11-07 02:09:26,191] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 132.36 GB, percent = 35.2%
[2025-11-07 02:09:26,194] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7308318750>
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-11-07 02:09:26,195] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   train_batch_size ............. 16
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  2
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   world_size ................... 4
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-11-07 02:09:26,196] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-11-07 02:09:26,196] [INFO] [config.py:989:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": "auto", 
        "stage3_prefetch_bucket_size": "auto", 
        "stage3_param_persistence_threshold": "auto", 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "zero_optimization.reduce_bucket_size": 8.028160e+05, 
    "zero_optimization.stage3_param_persistence_threshold": 8.960000e+03, 
    "zero_optimization.stage3_prefetch_bucket_size": 7.225344e+05
}
[RESUME] output_dir does not exist or is empty, starting fresh.
Trainer class used: <class 'open_r1.trainer.humanOmni_grpo_text_trainer_attention_AU.HumanOmniVLGRPO_TEXT_Attention_AU_Trainer'>
train() defined in: /mnt/ssd_hs/envs/r1-v/lib/python3.11/site-packages/transformers/trainer.py
get_train_dataloader() defined in: /mnt/ssd_hs/Exp/R1-Omni/src/r1-v/src/open_r1/trainer/humanOmni_grpo_text_trainer_attention_AU.py
Parameter Offload: Total persistent parameters: 956485 in 758 params
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: rhsnu7200 (rhs_ivl) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /mnt/ssd_hs/Exp/R1-Omni/src/r1-v/wandb/run-20251107_020931-h2tu9zcl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run results-11-07-0209-TRI-av-SFT-0.5B-af_a_tawn-epoch2-ratio1.0-grpo16-lr1e-6-bs2-gradaccum2-beta0.04-layermean
wandb: ⭐️ View project at https://wandb.ai/rhs_ivl/huggingface
wandb: 🚀 View run at https://wandb.ai/rhs_ivl/huggingface/runs/h2tu9zcl
  0%|          | 0/2088 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/10700.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00948.mp4
[DEBUG] Step 0
[DEBUG] prompt_completion_ids shape: torch.Size([32, 336])
[DEBUG] Shortest output (len=185): <vis_desc>In the video, we see a close-up of a male character's face. His facial expression is fierce, with a serious and angry tone in his voice. He seems to be engaged in a heated argument with someone.</vis_desc>
<aud_desc>In the audio, the emphasis is placed on the third-person argument, "Is it possible for you to use it to twist me?" This sentence may be the character's questioning or complaining to the other person.</aud_desc>
<think>Based on the intense argument and serious tone of the male character in the video, as well as the emphasis on the third-person argument in the audio, we can infer that this sentence carries a sense of anger or dissatisfaction. The male character may be expressing his displeasure or impatience towards the other person's actions or remarks, hence conveying a tone of anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=336): <vis_desc>In the video, we see a woman with a furrowed brow and a contorted facial posture, seemingly emphasizing a point or expressing dissatisfaction. Her mouth moves and her gaze doesn't stop fixed on the other person, indicating that she is engaged in a heated argument with the person opposite her.</vis_desc>
<aud_desc>In the audio, the character's voice starts off heavy and slow, with obvious stuttering at the beginning of the speech, suggesting that the character is emotionally excited or has some mental discomfort. Towards the end of the speech, the character's volume reduces, and overall enthusiasm is lost, indicating that she may be expressing a certain emotional state. In the text, the subtitle says, "Ai Bao Qiu was completely innocent, why was he punished?" This sentence may be the woman's questioning or complaint about a certain decision or action.</aud_desc>
<think>Based on the video clues of the woman's furrowed brow, contorted facial posture, and movement of the mouth, it can be inferred that she is expressing dissatisfaction and is engaged in an heated argument with the person opposite her. Additionally, based on the audio clues of the character's heavy and slow voice, obvious stuttering at the beginning of the speech, and reduced volume at the end of the speech, it can be further confirmed that her emotional state is excited or has some mental discomfort. Taking these clues together, it can be inferred that this sentence expresses the woman's dissatisfaction and questioning towards a certain decision or action, with a particular emphasis onAi Bao Qiu's innocence and his unfair punishment.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08917.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05061.mp4
[DEBUG] Step 0
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=175): <vis_desc>In the video, we see a male character sitting indoors. His facial expression appears surprised and he is looking at the other person, seemingly speaking to them.</vis_desc>
<aud_desc>In the audio, there seems to be laughter in the character's voice, indicating that he is surprised by the other person's words or actions. In the text, the subtitle says, "Ah, Xiaoping." This sentence may be the male character's reaction to the other person.</aud_desc>
<think>Based on the surprised facial expression and look at the other person in the video, as well as the laughter in the character's voice in the audio, we can infer that this sentence means "Ah, Xiaoping," which is likely the male character's surprise at realizing or knowing the other person through laughter.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the screen shows a woman being interviewed. In the video, her eyes are wide open and her mouth is slightly upwardturned, which typically indicates that she is speaking or answering a question. At the end of the video, she raises her arm to shake her hand, which is a sign of agreement or acceptance.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle reads: "We just needed to be a family, you know, and we were away from our family. So, to be honest, like there are so many moments in the show that... And to be honest, like there are so many moments that... You know, I don't have a mother, but I have a father, and there are many moments together." This sentence expresses the woman's dissatisfaction and confusion about her family relationships, as well as her belief in friendships.</aud_desc>
<think>Based on the woman's wide-open eyes and slightly upwardturned mouth in the video, as well as her action of raising her arm to shake her hand, we can infer that she is positively responding to the question and showing a positive attitude. The woman's description of being a "family" and away from her family also implies a sense of being close or friends. Additionally, the woman's expression of "like there are so many moments in the show that..." suggests that she has experienced many interesting or positive moments in the show. Based on the woman's positive emotional indicators and the negative reactions to her family relationships in the video clues, we can deduce that this sentence is the woman expressing her family relationships and feelings towards friends in the show, while also indicating her
[2025-11-07 02:10:38,890] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 1/2088 [01:06<38:31:59, 66.47s/it]                                                   {'loss': 0.0001, 'grad_norm': 1.9955381944941637, 'learning_rate': 9.995210727969349e-07, 'completion_length': 247.09765625, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.5087699890136719, 'reward': 1.7158012986183167, 'reward_std': 0.7036769688129425, 'kl': 0.00157928466796875, 'epoch': 0.0}
  0%|          | 1/2088 [01:06<38:31:59, 66.47s/it][2025-11-07 02:11:31,971] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 2/2088 [01:59<33:57:04, 58.59s/it]                                                   {'loss': 0.0001, 'grad_norm': 2.0395590562244235, 'learning_rate': 9.990421455938696e-07, 'completion_length': 243.58984375, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.43754875659942627, 'reward': 1.7305175065994263, 'reward_std': 0.691722184419632, 'kl': 0.001552581787109375, 'epoch': 0.0}
  0%|          | 2/2088 [01:59<33:57:04, 58.59s/it][2025-11-07 02:12:26,048] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 3/2088 [02:53<32:44:26, 56.53s/it]                                                   {'loss': 0.0001, 'grad_norm': 1.9816948240510799, 'learning_rate': 9.985632183908045e-07, 'completion_length': 247.171875, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.4607250541448593, 'reward': 1.7888500690460205, 'reward_std': 0.6573010385036469, 'kl': 0.001529693603515625, 'epoch': 0.0}
  0%|          | 3/2088 [02:53<32:44:26, 56.53s/it][2025-11-07 02:13:16,458] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 4/2088 [03:44<31:19:35, 54.11s/it]                                                   {'loss': 0.0001, 'grad_norm': 1.9654328565022143, 'learning_rate': 9.980842911877395e-07, 'completion_length': 241.578125, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.35546875, 'rewards/think_step_av_with_neutral_reward': 0.4030531495809555, 'reward': 1.4772717952728271, 'reward_std': 0.610075831413269, 'kl': 0.00151824951171875, 'epoch': 0.0}
  0%|          | 4/2088 [03:44<31:19:35, 54.11s/it][2025-11-07 02:14:15,065] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 5/2088 [04:42<32:14:54, 55.73s/it]                                                   {'loss': 0.0001, 'grad_norm': 1.9641715084266054, 'learning_rate': 9.976053639846744e-07, 'completion_length': 240.765625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.46484375, 'rewards/think_step_av_with_neutral_reward': 0.4587693363428116, 'reward': 1.6384567618370056, 'reward_std': 0.6636292040348053, 'kl': 0.001678466796875, 'epoch': 0.0}
  0%|          | 5/2088 [04:42<32:14:54, 55.73s/it][2025-11-07 02:15:09,033] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 6/2088 [05:36<31:53:08, 55.13s/it]                                                   {'loss': 0.0001, 'grad_norm': 2.009718847060632, 'learning_rate': 9.97126436781609e-07, 'completion_length': 246.34765625, 'rewards/av_format_reward': 0.640625, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4075510948896408, 'reward': 1.528644859790802, 'reward_std': 0.7624731659889221, 'kl': 0.00164031982421875, 'epoch': 0.01}
  0%|          | 6/2088 [05:36<31:53:08, 55.13s/it][2025-11-07 02:16:00,405] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 7/2088 [06:27<31:09:34, 53.90s/it]                                                   {'loss': 0.0001, 'grad_norm': 2.0925477062182214, 'learning_rate': 9.96647509578544e-07, 'completion_length': 237.5859375, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.3953956365585327, 'reward': 1.6375831961631775, 'reward_std': 0.48787418007850647, 'kl': 0.00157928466796875, 'epoch': 0.01}
  0%|          | 7/2088 [06:27<31:09:34, 53.90s/it][2025-11-07 02:16:58,940] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 8/2088 [07:26<31:59:47, 55.38s/it]                                                   {'loss': 0.0001, 'grad_norm': 2.055364351848698, 'learning_rate': 9.961685823754787e-07, 'completion_length': 249.07421875, 'rewards/av_format_reward': 0.6015625, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.4602539390325546, 'reward': 1.4993164539337158, 'reward_std': 0.6964649260044098, 'kl': 0.001789093017578125, 'epoch': 0.01}
  0%|          | 8/2088 [07:26<31:59:47, 55.38s/it][2025-11-07 02:17:55,142] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 9/2088 [08:22<32:07:46, 55.64s/it]                                                   {'loss': 0.0001, 'grad_norm': 2.8968685442968343, 'learning_rate': 9.956896551724137e-07, 'completion_length': 249.86328125, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.4202420264482498, 'reward': 1.6038358211517334, 'reward_std': 0.7113139033317566, 'kl': 0.0019073486328125, 'epoch': 0.01}
  0%|          | 9/2088 [08:22<32:07:46, 55.64s/it][2025-11-07 02:18:47,113] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 10/2088 [09:14<31:27:39, 54.50s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.9664766248346073, 'learning_rate': 9.952107279693486e-07, 'completion_length': 243.23046875, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4443914443254471, 'reward': 1.7139226794242859, 'reward_std': 0.5383987128734589, 'kl': 0.0017547607421875, 'epoch': 0.01}
  0%|          | 10/2088 [09:14<31:27:39, 54.50s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00486.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8585.mp4
[DEBUG] Step 10
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=189): <vis_desc>In the video, the scene shows a woman using her phone to make a call. She has a gentle expression on her face, with a smile at the end of the call, giving a relaxed and comfortable feeling.</vis_desc>
<aud_desc>In the audio, the tone is calm and soothing. In the text, the subtitle says, "Yes, but here's the thing, it's simple, I don't like complexity." This sentence may be the lady's response to the person on the other end of the phone.</aud_desc>
<think>Based on the woman's relaxation and comfortable feeling in the video clues, as well as the calm and soothing tone described in the audio clues, we can infer that this sentence may have a relaxed and humorous tone. The lady may be commenting on something simple that she doesn't like complexity, expressing a relaxed and comfortable attitude.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see two characters in an outdoor setting. The character on the left is looking at the other person, while the character on the right has their back to the camera. We mainly analyze the emotional state of the character on the left. In the video, their facial expression appears serious and focused, seemingly engaged in a tense conversation with the person on the right opposite them. The corners of their mouth are slightly downturned, as if speaking softly or waiting for a better moment. Overall, it seems like the character on the left is possibly discussing a serious topic, trying to persuade or argue with the other person.</vis_desc>
<aud_desc>In the audio, the voice is deep and the tone is heavy. Combined with the text content, it seems that the character may be expressing concern and sadness about the situation, possibly for Zhang Fei. In the text, the subtitle reads: "In the case of a siege, we may have more people to rely on." This sentence may be the left character expressing concerns and sadness about the situation.</aud_desc>
<think>Based on the serious and focused facial expression of the character on the left in the video, as well as the slight downturn of the corners of their mouth, it can be inferred that their tone is soft or they are waiting for a better moment. Additionally, based on the description of the character's low voice and heavy tone in the audio clues, it can be further confirmed that this sentence carries a sense of concern and sadness. Therefore, this sentence expresses the character's plea and concern for the other person's situation, aligning with the emotional states displayed by the left character in the video and audio clues.</think>
<answer>sad</
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11033.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3064.mp4
[DEBUG] Step 10
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=181): <aud_desc>In the audio, the tone and intonation are relatively calm. In the text, the subtitle reads: "I'm free, huh?" This sentence may be the woman's reaction to something.</aud_desc>
<vis_desc>In the video, we see a woman standing in a garage filled with various items. Her facial expression is smiling, her eyes are shining, and she is waving her hand up and down in a joyful manner.</vis_desc>
<think>Based on the woman's smiling, waving her hand up and down in the video, and the description of a calm tone and intonation in the audio, we can infer that this sentence may have a relaxed or humorous tone. The woman may be surprised or in a happy mood, implying that she is free. Therefore, this sentence expresses the woman's happy or relaxed emotions.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, the character's voice is deep and carries a sense of powerlessness. The character's tone seems to be increasingly angry and agitated as the conversation progresses. Overall, the character is expressing a great deal of anger and determination tount hekai in this dialogue.</aud_desc>
<vis_desc>In the video, there are two characters. The woman on the left (wearing a black sleeveless top) and the woman on the right (wearing a red long-sleeved top) are facing each other. We mainly analyze the emotional state of the woman on the left. The background shows an indoor environment with a closed door and a chair, possibly an appointment room. In the video, she is subtly turning towards the other person, with a lowered head and parted ears, seemingly waiting for a response. Her mouth moves slightly, her gaze does not meet the other person's eye, and her facial expression appears somewhat surprised. Her body posture is relaxed, but leaning forward, as if emphasizing a certain point of view or opinion.</vis_desc>
<think>The emotional state of the woman on the left in this video can be inferred through her facial expressions and body language. Visually, she has a surprised and then an angry expression. Her mouth moves slightly, indicating that she is speaking with some force or is responding to the other person. Her gaze does not meet the other person's eye, suggesting that she is considering the response before proceeding. Her body posture is relaxed, but leaning forward implies emphasis. Audibly, her voice is deep and her tone carries a sense of powerlessness, reinforcing her desire for defiance and anger. Although her facial expression becomes more prominent and her mouth opens in the subsequent speech
[2025-11-07 02:19:41,328] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 11/2088 [10:08<31:23:42, 54.42s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.951444890792561, 'learning_rate': 9.947318007662835e-07, 'completion_length': 252.2890625, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.4345548152923584, 'reward': 1.5595548152923584, 'reward_std': 0.6974680423736572, 'kl': 0.001712799072265625, 'epoch': 0.01}
  1%|          | 11/2088 [10:08<31:23:42, 54.42s/it][2025-11-07 02:20:37,830] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 12/2088 [11:05<31:44:44, 55.05s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.9485652299864542, 'learning_rate': 9.942528735632182e-07, 'completion_length': 243.92578125, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5156617909669876, 'reward': 1.8594118356704712, 'reward_std': 0.6357097029685974, 'kl': 0.00176239013671875, 'epoch': 0.01}
  1%|          | 12/2088 [11:05<31:44:44, 55.05s/it][2025-11-07 02:21:30,661] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 13/2088 [11:58<31:20:34, 54.38s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.2269135673484306, 'learning_rate': 9.937739463601532e-07, 'completion_length': 245.3671875, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.388186976313591, 'reward': 1.5288119912147522, 'reward_std': 0.617024302482605, 'kl': 0.002040863037109375, 'epoch': 0.01}
  1%|          | 13/2088 [11:58<31:20:34, 54.38s/it][2025-11-07 02:22:23,715] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 14/2088 [12:51<31:05:51, 53.98s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.035117969916336, 'learning_rate': 9.93295019157088e-07, 'completion_length': 244.5546875, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.328125, 'rewards/think_step_av_with_neutral_reward': 0.32060718536376953, 'reward': 1.3479509353637695, 'reward_std': 0.5953788459300995, 'kl': 0.002044677734375, 'epoch': 0.01}
  1%|          | 14/2088 [12:51<31:05:51, 53.98s/it][2025-11-07 02:23:17,664] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 15/2088 [13:45<31:04:38, 53.97s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.07150257007831, 'learning_rate': 9.92816091954023e-07, 'completion_length': 244.8125, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.3203125, 'rewards/think_step_av_with_neutral_reward': 0.33970974385738373, 'reward': 1.320178508758545, 'reward_std': 0.6796048283576965, 'kl': 0.001972198486328125, 'epoch': 0.01}
  1%|          | 15/2088 [13:45<31:04:38, 53.97s/it][2025-11-07 02:24:14,401] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 16/2088 [14:41<31:32:32, 54.80s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0561142797843552, 'learning_rate': 9.92337164750958e-07, 'completion_length': 240.79296875, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.34375, 'rewards/think_step_av_with_neutral_reward': 0.3912191689014435, 'reward': 1.4849691987037659, 'reward_std': 0.6092090308666229, 'kl': 0.00217437744140625, 'epoch': 0.02}
  1%|          | 16/2088 [14:41<31:32:32, 54.80s/it][2025-11-07 02:25:09,095] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 17/2088 [15:36<31:30:27, 54.77s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0895524501739335, 'learning_rate': 9.918582375478927e-07, 'completion_length': 247.296875, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.3999169170856476, 'reward': 1.5210106372833252, 'reward_std': 0.6353135704994202, 'kl': 0.0022125244140625, 'epoch': 0.02}
  1%|          | 17/2088 [15:36<31:30:27, 54.77s/it][2025-11-07 02:26:01,918] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 18/2088 [16:29<31:09:22, 54.18s/it]                                                    {'loss': 0.0001, 'grad_norm': 22.817219498608782, 'learning_rate': 9.913793103448276e-07, 'completion_length': 248.5390625, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.47746196389198303, 'reward': 1.8173056840896606, 'reward_std': 0.5806602090597153, 'kl': 0.00226593017578125, 'epoch': 0.02}
  1%|          | 18/2088 [16:29<31:09:22, 54.18s/it][2025-11-07 02:27:01,182] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 19/2088 [17:28<32:01:03, 55.71s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.9663594231828987, 'learning_rate': 9.909003831417623e-07, 'completion_length': 259.05078125, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4110001027584076, 'reward': 1.5906875133514404, 'reward_std': 0.6721153855323792, 'kl': 0.002197265625, 'epoch': 0.02}
  1%|          | 19/2088 [17:28<32:01:03, 55.71s/it][2025-11-07 02:27:53,359] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 20/2088 [18:20<31:23:34, 54.65s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0254856185262264, 'learning_rate': 9.904214559386972e-07, 'completion_length': 243.80078125, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.66015625, 'rewards/think_step_av_with_neutral_reward': 0.5355382561683655, 'reward': 1.9535069465637207, 'reward_std': 0.6784664988517761, 'kl': 0.0020294189453125, 'epoch': 0.02}
  1%|          | 20/2088 [18:20<31:23:34, 54.65s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03932.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01905.mp4
[DEBUG] Step 20
[DEBUG] prompt_completion_ids shape: torch.Size([32, 319])
[DEBUG] Shortest output (len=150): <aud_desc>In the audio, the character's tone and intonation are relatively calm. In the text, the subtitle reads: "We will meet later."</aud_desc>
<vis_desc>In the video, we see a woman with a neutral facial expression, looking directly at the other person, seemingly engaged in a serious conversation.</vis_desc>
<think>Based on the woman's neutral facial expression and direct eye contact in the video clues, as well as the calm tone and intonation of the character in the audio clues, we can infer that this sentence may carry a serious or thoughtful tone. Therefore, this sentence may be the woman expressing her anticipation and focus about meeting the other personlater.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=319): <vis_desc>In the video, we see a woman whose facial expression appears somewhat nervous and serious. Her eyebrows are slightly furrowed and the corners of her mouth are slightly downturned, implying that she may be engaged in a serious conversation or debate.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, the subtitle says, "He's always talking about his ability, actually he doesn't have much." This sentence may be spoken by the woman to someone.</aud_desc>
<think>Based on the woman's tense and serious facial expression in the video clues, as well as the suggestion to "Actually he doesn't have much," we can infer that this sentence may carry a negative tone. The woman may be criticizing ordisappointed with the other person's speech or behavior. Since the audio clues cannot determine the emotional state of the person, we cannot determine the tone of this sentence. However, based on the woman's tense and serious facial expression in the video clues, we can infer that this sentence may carry a negative tone. Therefore, this sentence may imply that the woman is dissatisfied or disappointed with the other person's words or actions, but at the same time, she also mentions having "Actually he doesn't have much," which may imply that she realizes that the other person's words or actions do not meet her expectations or standards. Therefore, this sentence may carry a complex emotional state, making it difficult to determine the individual's true emotional inclination.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07743.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04376.mp4
[DEBUG] Step 20
[DEBUG] prompt_completion_ids shape: torch.Size([32, 347])
[DEBUG] Shortest output (len=183): <vis_desc>In the video, we see a woman whose facial expression appears nervous and busy. Her eyebrows are slightly furrowed, and her mouth is slightly open, indicating that she is involved in a tense conversation.</vis_desc>
<aud_desc>In the audio, the tone is low and monotonous, with pauses between sentences. In the text, the subtitle says, "It's always such a shame." This sentence may be the woman's self-reflection or response to someone or something.</aud_desc>
<think>Based on the woman's tense and busy facial expression in the video clues, as well as the low and monotonous tone in the audio clues, we can infer that this sentence may carry a sense of regret or self-blame. The woman may feel sad and unhappy about a certain thing or situation, hence expressing this sentence.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=347): <vis_desc>In the video, the screen shows a woman against a black background. At the beginning of the video, her facial expression is relatively neutral. In the following scenes, she starts to smile, revealing a pleasant emotion. Her eyes shift from side to side, which may be a natural eye movement while smiling. The corners of her mouth droop, she puts less pressure on her lips, and her overall expression remains friendly, indicating that she has realized something pleasant.</vis_desc>
<aud_desc>In the audio, the tone is calm and gentle. Combined with the text content, it seems to carry a sense of humor and satisfaction. The character says, "As you can see, you really do have fun." This sentence may be the woman's evaluation or reaction to herself or someone she knows. Based on the smile and pleasant expression in the video clues, as well as the description of a calm and gentle tone in the audio clue, we can infer that this sentence is a positive evaluation or reaction from the woman to herself or someone she knows. Therefore, this sentence expresses the woman's satisfaction and joy.</aud_desc>
<think>Overall, the video provides a clear and cohesive picture of a woman in a relaxed and pleasant emotional state. From her neutral facial expression at the beginning to the smile and pleasant expression at the end, as well as her natural eye movement and friendly overall expression, it can be inferred that she has realized something pleasant. Combined with the description of a calm and gentle tone in the audio clue, it can be felt that she is expressing a joyful and satisfied emotion. Therefore, this sentence expresses the woman's appreciation and enjoyment of her friendliness.</think>
<answer>happy</answer>
[2025-11-07 02:28:46,350] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 21/2088 [19:13<31:05:31, 54.15s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0257660803088635, 'learning_rate': 9.899425287356322e-07, 'completion_length': 230.83203125, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.4453125, 'rewards/think_step_av_with_neutral_reward': 0.4479049742221832, 'reward': 1.6002486944198608, 'reward_std': 0.7053445279598236, 'kl': 0.0021514892578125, 'epoch': 0.02}
  1%|          | 21/2088 [19:13<31:05:31, 54.15s/it][2025-11-07 02:29:45,297] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 22/2088 [20:12<31:54:11, 55.59s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.022571307936468, 'learning_rate': 9.89463601532567e-07, 'completion_length': 243.74609375, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4825773239135742, 'reward': 1.6896085739135742, 'reward_std': 0.6851692199707031, 'kl': 0.0023651123046875, 'epoch': 0.02}
  1%|          | 22/2088 [20:12<31:54:11, 55.59s/it][2025-11-07 02:30:40,431] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 23/2088 [21:08<31:48:33, 55.45s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0921626740624393, 'learning_rate': 9.889846743295018e-07, 'completion_length': 242.40625, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.40979088842868805, 'reward': 1.6871346831321716, 'reward_std': 0.5463762581348419, 'kl': 0.002410888671875, 'epoch': 0.02}
  1%|          | 23/2088 [21:08<31:48:33, 55.45s/it][2025-11-07 02:31:39,624] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 24/2088 [22:07<32:26:12, 56.58s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.059838150908962, 'learning_rate': 9.885057471264367e-07, 'completion_length': 240.98046875, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.33203125, 'rewards/think_step_av_with_neutral_reward': 0.32006703317165375, 'reward': 1.4020982384681702, 'reward_std': 0.634907990694046, 'kl': 0.00260162353515625, 'epoch': 0.02}
  1%|          | 24/2088 [22:07<32:26:12, 56.58s/it][2025-11-07 02:32:33,066] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 25/2088 [23:00<31:52:55, 55.64s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.1165546519029297, 'learning_rate': 9.880268199233715e-07, 'completion_length': 239.28125, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.375273659825325, 'reward': 1.6096486449241638, 'reward_std': 0.5597821176052094, 'kl': 0.00305938720703125, 'epoch': 0.02}
  1%|          | 25/2088 [23:00<31:52:55, 55.64s/it][2025-11-07 02:33:26,062] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 26/2088 [23:53<31:24:46, 54.84s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.1097587032222758, 'learning_rate': 9.875478927203064e-07, 'completion_length': 243.40234375, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.46484375, 'rewards/think_step_av_with_neutral_reward': 0.4770762622356415, 'reward': 1.742701232433319, 'reward_std': 0.7460986077785492, 'kl': 0.002716064453125, 'epoch': 0.02}
  1%|          | 26/2088 [23:53<31:24:46, 54.84s/it][2025-11-07 02:34:17,610] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 27/2088 [24:45<30:49:54, 53.85s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.062101999433011, 'learning_rate': 9.870689655172413e-07, 'completion_length': 249.7890625, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5405558198690414, 'reward': 1.9116495847702026, 'reward_std': 0.5901678204536438, 'kl': 0.0030975341796875, 'epoch': 0.03}
  1%|▏         | 27/2088 [24:45<30:49:54, 53.85s/it][2025-11-07 02:35:09,875] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 28/2088 [25:37<30:32:38, 53.38s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.149054621799541, 'learning_rate': 9.865900383141762e-07, 'completion_length': 238.125, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.44140625, 'rewards/think_step_av_with_neutral_reward': 0.3656824976205826, 'reward': 1.6195887923240662, 'reward_std': 0.5549165904521942, 'kl': 0.0029754638671875, 'epoch': 0.03}
  1%|▏         | 28/2088 [25:37<30:32:38, 53.38s/it][2025-11-07 02:36:04,259] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 29/2088 [26:31<30:42:06, 53.68s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0610994629252226, 'learning_rate': 9.861111111111112e-07, 'completion_length': 243.1640625, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.45500317215919495, 'reward': 1.6073468923568726, 'reward_std': 0.6820292770862579, 'kl': 0.00298309326171875, 'epoch': 0.03}
  1%|▏         | 29/2088 [26:31<30:42:06, 53.68s/it][2025-11-07 02:36:57,733] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 30/2088 [27:25<30:39:05, 53.62s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.1460474767979942, 'learning_rate': 9.856321839080459e-07, 'completion_length': 238.8515625, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.39825206995010376, 'reward': 1.570127010345459, 'reward_std': 0.7166770696640015, 'kl': 0.00301361083984375, 'epoch': 0.03}
  1%|▏         | 30/2088 [27:25<30:39:05, 53.62s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1283.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09297.mp4
[DEBUG] Step 30
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=184): <vis_desc>In the video, we see a woman with a red mark on her face, tears streaming down her face. Her facial expression appears very painful and sad.</vis_desc>
<aud_desc>In the audio, the character's voice is extremely weak and sounds like crying out loud. In the text, the subtitle reads, "Lala, lala, la LAAR!" This sentence may be the woman's reaction to a sudden emergency or unexpected event.</aud_desc>
<think>Based on the video clues of the woman's red mark on her face, tears streaming down her face, and the audio clue of the character's extremely weak voice and crying out loud tone, we can infer that this sentence carries a sense of distress and fear. The woman may be experiencing a sudden emergency, such as an accident or a disaster, which triggered her emotional reaction.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the screen shows a male character wearing formal attire, with curtains and a window in the background, indicating that it is inside a home. In the video, his eyes are wide open and his facial expression appears somewhat surprised. He seems to be speaking to a woman on the right side of the screen, who is not shown in the frame. We primarily analyze the emotional state of the male character. In the initial stage of the video, he looks directly at the woman, with his mouth slightly open, showing a surprised and slightly confused facial expression. As the video progresses, he quickly turns his head and looks down, with his mouth wide open, displaying anger and excitement. At the same time, his body moves back and forth, as if reacting to something the woman is saying or doing.</vis_desc>
<aud_desc>In the audio, the volume is loud and there is fast talking, indicating that the character's emotions are intense. Combined with the text content, the character questions the necessity and masculinity of the current situation, as well as the woman's existence. In the text, the subtitle reads: "Is this terrifying? I'm already living a miserable life, getting worse until I'm no longer readable. This is actually where I'm stopping." This sentence expresses the male character's anger and questioning emotions.</aud_desc>
<think>Based on the surprised and confused facial expression of the male character in the video clues, as well as the description of loud volume, fast talking in the audio clues, we can infer that the male character is expressing anger and questioning emotions in this sentence. He may hold a dissatisfied or angerful attitude towards the current situation, believing that it is meaningless or undesirable. The
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09396.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4956.mp4
[DEBUG] Step 30
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=191): <vis_desc>In the video, we see a man wearing a dark suit, tie, and shirt, with a furrowed brow and downturned mouth, seemingly expressing dissatisfaction and anger.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle says, "You told me the phone was brought around, no explanation was given, just a few words were spoken, is that understood?" This sentence may be a kind of reminder or criticism from the man to the other person.</aud_desc>
<think>Based on the video clues of the man's furrowed brow and downturned mouth, as well as the audio clue of no valuable emotional clues, we can infer that this sentence may carry a sense of anger or dissatisfaction. The man may be unhappy with the other person's behavior or decisions and expresses his dissatisfaction through this sentence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, his tone is low and his emotions are more excited. There is a "two" at the beginning of the speech, but it should be "oi". Combined with the text content, it seems that he is accusing someone. In the text, the subtitle says, "But I received the phone yesterday, without the phone, I wouldn't have spoken a word. What are you arguing about?" This sentence may be the man's accusation or questioning towards another person.</aud_desc>
<vis_desc>In the video, the screen shows a man wearing a formal suit. In the opening scene, his brows are tightly furrowed and his facial expression appears serious and concerned. His grip on the suit is tight, seemingly pulling himself forward with great force, possibly indicating an effort to stand up or get up from the chair. As time passes, his head moves forward but maintains a hunched position, and his grip on the suit remains tight, suggesting that he is either talking to someone or trying to stand up but being unable to do so. He appears to be pulling himself forward or trying to move forward while maintaining a tense posture, possibly anticipating an immediate response or reaction from the other person. Taking these scenes together, it can be inferred that the man is likely in a formal setting, such as an office or a similar environment. An initial tense and serious expression transitions into a more excited and aggressive one, indicating a heated argument.</vis_desc>
<think>Based on these scenes, one can infer that the man is likely engaged in a formal discussion or argument in the formal environment of an office or similar setting. The man's initial tense and serious expression transitions into a more excited and aggressive one, indicating an
[2025-11-07 02:37:53,984] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|▏         | 31/2088 [28:21<31:05:19, 54.41s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.9607853185464352, 'learning_rate': 9.851532567049808e-07, 'completion_length': 249.390625, 'rewards/av_format_reward': 0.83203125, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.3394376337528229, 'reward': 1.5972501635551453, 'reward_std': 0.5609578490257263, 'kl': 0.0034637451171875, 'epoch': 0.03}
  1%|▏         | 31/2088 [28:21<31:05:19, 54.41s/it][2025-11-07 02:38:48,207] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 32/2088 [29:15<31:02:32, 54.35s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.1519424026698393, 'learning_rate': 9.846743295019157e-07, 'completion_length': 225.6796875, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.44140779972076416, 'reward': 1.6601577997207642, 'reward_std': 0.6805067360401154, 'kl': 0.0032196044921875, 'epoch': 0.03}
  2%|▏         | 32/2088 [29:15<31:02:32, 54.35s/it][2025-11-07 02:39:43,565] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 33/2088 [30:11<31:11:56, 54.66s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.1147360955815078, 'learning_rate': 9.841954022988505e-07, 'completion_length': 235.16015625, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.2734375, 'rewards/think_step_av_with_neutral_reward': 0.3195044994354248, 'reward': 1.2218482494354248, 'reward_std': 0.650574117898941, 'kl': 0.00331878662109375, 'epoch': 0.03}
  2%|▏         | 33/2088 [30:11<31:11:56, 54.66s/it][2025-11-07 02:40:38,927] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 34/2088 [31:06<31:18:13, 54.87s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0337925080300745, 'learning_rate': 9.837164750957854e-07, 'completion_length': 240.453125, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.27734375, 'rewards/think_step_av_with_neutral_reward': 0.26838333904743195, 'reward': 1.205883264541626, 'reward_std': 0.608694314956665, 'kl': 0.00357818603515625, 'epoch': 0.03}
  2%|▏         | 34/2088 [31:06<31:18:13, 54.87s/it][2025-11-07 02:41:31,747] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 35/2088 [31:59<30:56:20, 54.25s/it]                                                    {'loss': 0.0001, 'grad_norm': 2.0501484185942704, 'learning_rate': 9.832375478927203e-07, 'completion_length': 246.15625, 'rewards/av_format_reward': 0.66796875, 'rewards/accuracy_reward': 0.37890625, 'rewards/think_step_av_with_neutral_reward': 0.4237506091594696, 'reward': 1.470625638961792, 'reward_std': 0.7761351764202118, 'kl': 0.00347900390625, 'epoch': 0.03}
  2%|▏         | 35/2088 [31:59<30:56:20, 54.25s/it][2025-11-07 02:42:24,224] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 36/2088 [32:51<30:37:12, 53.72s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0065937938508625, 'learning_rate': 9.82758620689655e-07, 'completion_length': 236.82421875, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.4060792028903961, 'reward': 1.4177979230880737, 'reward_std': 0.6550859808921814, 'kl': 0.003753662109375, 'epoch': 0.03}
  2%|▏         | 36/2088 [32:51<30:37:12, 53.72s/it][2025-11-07 02:43:17,760] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 37/2088 [33:45<30:34:31, 53.67s/it]                                                    {'loss': 0.0001, 'grad_norm': 1.9954916484598244, 'learning_rate': 9.8227969348659e-07, 'completion_length': 232.90234375, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.4195931404829025, 'reward': 1.6149057149887085, 'reward_std': 0.6434472501277924, 'kl': 0.003662109375, 'epoch': 0.04}
  2%|▏         | 37/2088 [33:45<30:34:31, 53.67s/it][2025-11-07 02:44:10,244] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 38/2088 [34:37<30:21:24, 53.31s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.084313088336864, 'learning_rate': 9.818007662835249e-07, 'completion_length': 234.0, 'rewards/av_format_reward': 0.65625, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.3751017302274704, 'reward': 1.5118204355239868, 'reward_std': 0.6288307309150696, 'kl': 0.0039215087890625, 'epoch': 0.04}
  2%|▏         | 38/2088 [34:37<30:21:24, 53.31s/it][2025-11-07 02:45:01,663] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 39/2088 [35:29<30:01:09, 52.74s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.1524539716615614, 'learning_rate': 9.813218390804598e-07, 'completion_length': 230.546875, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.375, 'rewards/think_step_av_with_neutral_reward': 0.3369898796081543, 'reward': 1.4815211296081543, 'reward_std': 0.6027525961399078, 'kl': 0.00406646728515625, 'epoch': 0.04}
  2%|▏         | 39/2088 [35:29<30:01:09, 52.74s/it][2025-11-07 02:45:57,448] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 40/2088 [36:25<30:31:25, 53.66s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0501863884223477, 'learning_rate': 9.808429118773945e-07, 'completion_length': 239.75390625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.49092043936252594, 'reward': 1.8385766744613647, 'reward_std': 0.6781026422977448, 'kl': 0.0039825439453125, 'epoch': 0.04}
  2%|▏         | 40/2088 [36:25<30:31:25, 53.66s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02831.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00109.mp4
[DEBUG] Step 40
[DEBUG] prompt_completion_ids shape: torch.Size([32, 244])
[DEBUG] Shortest output (len=163): <vis_desc>In the video, we see a woman with slightly widened eyes and slightly open mouth, appearing surprised or confused.</vis_desc>
<aud_desc>In the audio, there is a questioning tone, expressing the character's surprised and confused emotions. In the text, the subtitle says, "Is this your school?" This sentence may be the woman's inquiry or reaction to someone or something.</aud_desc>
<think>Based on the video clues of the woman's slightly widened eyes and slightly open mouth, as well as the audio clue of the character's puzzled and surprised tone, we can infer that this sentence may carry a sense of curiosity or confusion. The woman may feel surprised or confused about something happening to someone or something, hence asking this question.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=244): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned corners of the mouth, seemingly expressing surprise or confusion. Her gaze seems to be fixed on a certain place, possibly focusing on an unexpected object or person in the environment.</vis_desc>
<aud_desc>In the audio, the character's voice trembles and shakes, with a fast pace, implying that the character is somewhat nervous. In the text, the subtitle reads, "Is thatSoCastrated?" This sentence may be the female character's response to some unexpected situation or information.</aud_desc>
<think>Based on the video clues of the female character's furrowed brows and downturned corners of the mouth, as well as her focused gaze on something outside of the frame, we can infer that this sentence may carry a sense of surprise or confusion. Additionally, the changes in the character's voice and tone in the audio indicate that she is nervous, as she speaks quickly and has a trembling and shook voice. Therefore, this sentence may suggest the female character expressing her surprise and confusion about something that she does not understand or is concerned about.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11447.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10301.mp4
[DEBUG] Step 40
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=193): <vis_desc>In the video, we see a woman with bloodstains on her skin, and her eyes are half-closed, appearing somewhat surprised.</vis_desc>
<aud_desc>In the audio, the character's tone is low and carries a sense of sadness. In the text, the subtitle reads: "Hey, okay, you can forget about it, it's all in the deep part, you're just not interested." This sentence may be the woman's response to someone or something.</aud_desc>
<think>Based on the surprised expression and half-closed eye of the woman in the video clues, as well as the low and sad tone of the character in the audio clues, we can infer that this sentence may carry a sense of surprise and disappointment. The woman may feel surprised and dissatisfied with the other person's behavior or remarks, and she doesn't want to engage with them.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the scene includes three individuals sitting across from each other. We mainly analyze the emotional state of the person on the right side. He is wearing a black shirt and seems to be giving a speech or report to the other two people. His mouth moves a lot, indicating that he is talking for a long time and supporting his words with his gestures. His facial expression is serious, which usually indicates that he is engaged in a formal and tense situation, such as a conference or discussion. Based on these scenes, it can be inferred that the person on the right side in this video is likely in a formal and tense situation, with a serious facial expression and a lot of movement in his mouth and facial expression.</vis_desc>
<aud_desc>In the audio, the tone of the character is stern and aggressive. Combined with the text content, it can be felt that he is accusing the other person. The subtitle in the text says, "I'll put my soul on the line and show you who you are. If you can finish examining it next week, I'll let you leave." This sentence can be inferred from the mention of "showing the other person who they are" in the subtitle.</aud_desc>
<think>Based on the serious facial expression and lot of movement in the person's mouth and facial expression in the video clues, as well as the stern and aggressive tone of the character in the audio clues, we can infer that this sentence carries a sense of anger or dissatisfaction. The person on the right side is likely accusing the other person for their lack of completion or inability to finish a certain task, expressing his discontent and anger towards them.</think>
<answer>angry</answer>
[2025-11-07 02:46:49,737] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 41/2088 [37:17<30:16:33, 53.25s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0584627336339207, 'learning_rate': 9.803639846743295e-07, 'completion_length': 230.3984375, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.452244833111763, 'reward': 1.6202136278152466, 'reward_std': 0.6089679002761841, 'kl': 0.0045013427734375, 'epoch': 0.04}
  2%|▏         | 41/2088 [37:17<30:16:33, 53.25s/it][2025-11-07 02:47:41,987] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 42/2088 [38:09<30:05:29, 52.95s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.288617473906603, 'learning_rate': 9.798850574712644e-07, 'completion_length': 234.6015625, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.4204159379005432, 'reward': 1.6469783782958984, 'reward_std': 0.6730508804321289, 'kl': 0.0047149658203125, 'epoch': 0.04}
  2%|▏         | 42/2088 [38:09<30:05:29, 52.95s/it][2025-11-07 02:48:34,848] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 43/2088 [39:02<30:03:42, 52.92s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.1316916199105376, 'learning_rate': 9.794061302681991e-07, 'completion_length': 235.9140625, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.33984375, 'rewards/think_step_av_with_neutral_reward': 0.30286741256713867, 'reward': 1.459117352962494, 'reward_std': 0.5376666486263275, 'kl': 0.004608154296875, 'epoch': 0.04}
  2%|▏         | 43/2088 [39:02<30:03:42, 52.92s/it][2025-11-07 02:49:27,964] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 44/2088 [39:55<30:04:49, 52.98s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0194668037727572, 'learning_rate': 9.78927203065134e-07, 'completion_length': 232.546875, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.3827458620071411, 'reward': 1.5389959216117859, 'reward_std': 0.6345645785331726, 'kl': 0.004791259765625, 'epoch': 0.04}
  2%|▏         | 44/2088 [39:55<30:04:49, 52.98s/it][2025-11-07 02:50:16,416] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 45/2088 [40:43<29:17:42, 51.62s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0295917705164555, 'learning_rate': 9.78448275862069e-07, 'completion_length': 228.90234375, 'rewards/av_format_reward': 0.6484375, 'rewards/accuracy_reward': 0.69921875, 'rewards/think_step_av_with_neutral_reward': 0.5896930992603302, 'reward': 1.9373494982719421, 'reward_std': 0.4558122456073761, 'kl': 0.0050048828125, 'epoch': 0.04}
  2%|▏         | 45/2088 [40:44<29:17:42, 51.62s/it][2025-11-07 02:51:08,727] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 46/2088 [41:36<29:23:55, 51.83s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.1589088686591063, 'learning_rate': 9.779693486590039e-07, 'completion_length': 228.44140625, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.41317911446094513, 'reward': 1.694429099559784, 'reward_std': 0.5337492227554321, 'kl': 0.0050506591796875, 'epoch': 0.04}
  2%|▏         | 46/2088 [41:36<29:23:55, 51.83s/it][2025-11-07 02:52:03,286] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 47/2088 [42:30<29:50:52, 52.65s/it]                                                    {'loss': 0.0002, 'grad_norm': 1.9567758439536354, 'learning_rate': 9.774904214559386e-07, 'completion_length': 231.26953125, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.40753234922885895, 'reward': 1.7317511439323425, 'reward_std': 0.454708993434906, 'kl': 0.0054931640625, 'epoch': 0.05}
  2%|▏         | 47/2088 [42:30<29:50:52, 52.65s/it][2025-11-07 02:52:54,522] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 48/2088 [43:22<29:35:35, 52.22s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.00174915868344, 'learning_rate': 9.770114942528735e-07, 'completion_length': 227.296875, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.33203125, 'rewards/think_step_av_with_neutral_reward': 0.3115228861570358, 'reward': 1.3857415914535522, 'reward_std': 0.5035898834466934, 'kl': 0.005279541015625, 'epoch': 0.05}
  2%|▏         | 48/2088 [43:22<29:35:35, 52.22s/it][2025-11-07 02:53:45,165] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 49/2088 [44:12<29:18:37, 51.75s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.016494875065014, 'learning_rate': 9.765325670498083e-07, 'completion_length': 229.984375, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.25390625, 'rewards/think_step_av_with_neutral_reward': 0.2775743007659912, 'reward': 1.3244493007659912, 'reward_std': 0.4775680750608444, 'kl': 0.0058441162109375, 'epoch': 0.05}
  2%|▏         | 49/2088 [44:12<29:18:37, 51.75s/it][2025-11-07 02:54:39,058] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 50/2088 [45:06<29:39:36, 52.39s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.0877851960733342, 'learning_rate': 9.760536398467432e-07, 'completion_length': 230.74609375, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.38159453868865967, 'reward': 1.6081570386886597, 'reward_std': 0.6821675896644592, 'kl': 0.005645751953125, 'epoch': 0.05}
  2%|▏         | 50/2088 [45:06<29:39:36, 52.39s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05931.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1229.mp4
[DEBUG] Step 50
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a man driving a car at night. His facial expression is neutral, his gaze is straight ahead, and his body posture is steady. This indicates that he is focused on driving and maintaining control of the car.</vis_desc>
<aud_desc>In the audio, the tone is calm. In the text, the subtitle says, "Okay, good." This sentence is likely spoken by the man during the driving process.</aud_desc>
<think>Based on the neutral facial expression, straight-ahead gaze, and steady body posture of the man in the video clues, as well as the calm tone in the audio clue, we can infer that this sentence is the man's response to a certain situation or command, expressing confidence and control. Therefore, this sentence expresses the man's satisfaction and determination.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see two characters sitting in the back of a car. The character on the left has his back to the camera, while the character on the right is facing the camera. We mainly analyze the emotional state of the character on the right. The environment appears to be outside the car, possibly on a highway. The background features windows with blurry green lights, indicating that the car is driving on a highway with some activity. In the video, he frowns, his mouth moves, and his eyes are not looking at the camera, suggesting that he is actively communicating with someone. His hands are crossed, and his posture has a sort of leaning feeling, usually interpreted as being told to rest or taking a break. Overall, he seems to be experiencing some inner struggle.</vis_desc>
<aud_desc>In the audio, the character's voice starts off urgent and fast-paced, with a stern and aggressive tone. In the text, the subtitle reads, "Don't, I'm going. Mr. Dong, Mr. Dong, let's forget it." This sentence may be a warning or reminder from the character on the right to the person opposite him.</aud_desc>
<think>Based on the facial expressions and actions of the character on the right in the video, he displays a sense of urgency, tension, or provocation. His frowny face, open mouth, moving mouth, and tired-looking eyes indicate that he may be having an unpleasant conversation or arguing. At the same time, his posture and crossed hands suggest that he is reminding someone not to forget something. Therefore, this sentence may carry a serious or threatening tone, consistent with the overall sense of tension displayed by the character on the right.</
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01125.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08467.mp4
[DEBUG] Step 50
[DEBUG] prompt_completion_ids shape: torch.Size([32, 243])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a man with an furrowed brow and an angry facial expression. His mouth is open and his gaze is directed towards someone off-screen.</vis_desc>
<aud_desc>In the audio, the volume is very loud and the tone is intense, suggesting that the character is expressing strong emotions. In the text, the subtitle reads, "Everything has been decided by me." This sentence may be the man's expression of regret or regretfulness.</aud_desc>
<think>Based on the visual clues of the man's furrowed brow and angry facial expression, as well as the audio clue of a very loud and intense tone, we can infer that this sentence carries a sense of anger or regret. The man may feel very upset or dissatisfied with something, hence expressing this sentence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=243): <vis_desc>In the video, the screen shows a man and a woman standing outdoors. We mainly analyze the man's emotional state. In the video, his eyebrows are furrowed and the corners of his mouth are slightly downturned, indicating a negative emotion. His gaze is directly focused on the other person, suggesting that he is engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone is low and the character's mood seems heavy. In the text, the subtitle reads, "All of you, including me, will go to Guanzhong." This sentence may be a request or command from the man to the other person.</aud_desc>
<think>Based on the visual clues of the man's furrowed eyebrows and downturned corners of the mouth, as well as his direct gaze on the other person, it can be inferred that he is expressing a feeling of sadness or despair. At the same time, the audio clue describing the low tone and heavy mood also support this inference. Therefore, this sentence may be the man requesting the other person to go to Guanzhong out of sadness and despair.</think>
<answer>sad</answer>
[2025-11-07 02:55:28,047] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 51/2088 [45:55<29:04:03, 51.37s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.131328901547298, 'learning_rate': 9.755747126436781e-07, 'completion_length': 225.84375, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.421875, 'rewards/think_step_av_with_neutral_reward': 0.34417007863521576, 'reward': 1.559013843536377, 'reward_std': 0.4505402743816376, 'kl': 0.005584716796875, 'epoch': 0.05}
  2%|▏         | 51/2088 [45:55<29:04:03, 51.37s/it][2025-11-07 02:56:18,229] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  2%|▏         | 52/2088 [46:45<28:51:05, 51.01s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1036372189955697, 'learning_rate': 9.75095785440613e-07, 'completion_length': 231.05078125, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.32421875, 'rewards/think_step_av_with_neutral_reward': 0.33248739689588547, 'reward': 1.2856123447418213, 'reward_std': 0.5785611271858215, 'kl': 0.006744384765625, 'epoch': 0.05}
  2%|▏         | 52/2088 [46:45<28:51:05, 51.01s/it][2025-11-07 02:57:10,124] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 53/2088 [47:37<28:59:12, 51.28s/it]                                                    {'loss': 0.0002, 'grad_norm': 2.042113166044093, 'learning_rate': 9.746168582375478e-07, 'completion_length': 237.6796875, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.48720675706863403, 'reward': 1.659081757068634, 'reward_std': 0.6531185805797577, 'kl': 0.0062103271484375, 'epoch': 0.05}
  3%|▎         | 53/2088 [47:37<28:59:12, 51.28s/it][2025-11-07 02:58:02,228] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 54/2088 [48:29<29:06:45, 51.53s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.0657586159050942, 'learning_rate': 9.741379310344827e-07, 'completion_length': 230.19140625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.38194769620895386, 'reward': 1.6006977558135986, 'reward_std': 0.6074593365192413, 'kl': 0.006622314453125, 'epoch': 0.05}
  3%|▎         | 54/2088 [48:29<29:06:45, 51.53s/it][2025-11-07 02:58:51,889] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 55/2088 [49:19<28:46:56, 50.97s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.017206053844014, 'learning_rate': 9.736590038314176e-07, 'completion_length': 217.19140625, 'rewards/av_format_reward': 0.6328125, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.46784135699272156, 'reward': 1.6240912675857544, 'reward_std': 0.5385119318962097, 'kl': 0.0070648193359375, 'epoch': 0.05}
  3%|▎         | 55/2088 [49:19<28:46:56, 50.97s/it][2025-11-07 02:59:47,390] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 56/2088 [50:14<29:32:07, 52.33s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1235799178134784, 'learning_rate': 9.731800766283525e-07, 'completion_length': 233.08203125, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.5094999969005585, 'reward': 1.8962187767028809, 'reward_std': 0.550171285867691, 'kl': 0.0070037841796875, 'epoch': 0.05}
  3%|▎         | 56/2088 [50:14<29:32:07, 52.33s/it][2025-11-07 03:00:44,088] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 57/2088 [51:11<30:15:39, 53.64s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1014445289483654, 'learning_rate': 9.727011494252873e-07, 'completion_length': 238.6875, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.5058221518993378, 'reward': 1.8495723009109497, 'reward_std': 0.5435653179883957, 'kl': 0.0071258544921875, 'epoch': 0.05}
  3%|▎         | 57/2088 [51:11<30:15:39, 53.64s/it][2025-11-07 03:01:38,458] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 58/2088 [52:06<30:22:13, 53.86s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.0783874473507473, 'learning_rate': 9.722222222222222e-07, 'completion_length': 228.9921875, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.44089725613594055, 'reward': 1.8080848455429077, 'reward_std': 0.5215449184179306, 'kl': 0.00677490234375, 'epoch': 0.06}
  3%|▎         | 58/2088 [52:06<30:22:13, 53.86s/it][2025-11-07 03:02:33,245] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 59/2088 [53:00<30:30:48, 54.14s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.0899375124858626, 'learning_rate': 9.717432950191571e-07, 'completion_length': 234.8203125, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.37128598988056183, 'reward': 1.5665985345840454, 'reward_std': 0.6145696938037872, 'kl': 0.0073699951171875, 'epoch': 0.06}
  3%|▎         | 59/2088 [53:00<30:30:48, 54.14s/it][2025-11-07 03:03:23,612] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 60/2088 [53:51<29:51:32, 53.00s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.0400625945358772, 'learning_rate': 9.712643678160918e-07, 'completion_length': 226.19921875, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.4885471314191818, 'reward': 1.9143284559249878, 'reward_std': 0.5631731152534485, 'kl': 0.006805419921875, 'epoch': 0.06}
  3%|▎         | 60/2088 [53:51<29:51:32, 53.00s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09585.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5455.mp4
[DEBUG] Step 60
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=170): <aud_desc>In the audio, the character's voice sounds cheerful and optimistic. In the text, the subtitle says, "Little one, happy baby!" This line is likely a greeting or celebration towards the character.</aud_desc>
<vis_desc>In the video, we see a child wearing a protective helmet, with their head turned towards another character, seemingly engaging in a pleasant conversation.</vis_desc>
<think>Based on the video clue of the child's head turning towards another character and their joyful facial expression, as well as the audio clue of the character's cheerful and optimistic voice, we can infer that this line is a happy and positive way of greeting. The child's joyful mood and friendly gestures align with what we would expect when someone introduces someone who is new or not familiar to them.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=302): <vis_desc>In the video, we see a man and a little boy standing in the crosswalk of a busy street. The boy is wearing a red and blue jacket, while the man is in a black outfit. The background shows a large group of people crossing the street, presumably children participating in some kind of activity. At the beginning of the video, the boy raises his hand to his head in a pleased manner, appearing to wipe away a sweat, which may indicate that he just completed a easy or very easy jump. In the following scenes, the boy closes his eyes and smiles, appearing to feel satisfied or joyful.</vis_desc>
<aud_desc>In the audio, the character's voice is very calm and soothing, with a positive and optimistic tone. In the text, the subtitle says, "Let's go, Papa." This sentence is likely what the little boy said to the man after they got off the bridge.</aud_desc>
<think>Based on the video clue of the boy raising his hand to his head in a pleased manner and then closing his eyes and smiling, as well as the audio clue of the character's calm and soothing voice with a positive and optimistic tone, we can infer that this sentence is the boy's way of expressing excitement and joyful emotions. The boy may be very happy to get off thebridge and go somewhere else, and this sentence is a positive comment from the boy to the man, expressing his excitement.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07809.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8494.mp4
[DEBUG] Step 60
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=189): <vis_desc>In the video, the screen shows a male character. In the video, his eyes are wide open and his facial expression appears surprised, indicating that he may have heard some unexpected news.</vis_desc>
<aud_desc>In the audio, the character's voice sounds surprised when saying "Ah." In the text, the subtitle says, "Yes, I heard you're from the same restaurant as me! Ah, this time it's for you." This sentence is likely the male character's response to the other person.</aud_desc>
<think>Based on the surprised facial expression of the male character in the video clues, as well as the description of surprised when saying "Ah" in the audio clues, we can infer that the male character may have heard some unexpected news from the other person, and he is very surprised when hearing about being from the same restaurant as him.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, the character's voice is accompanied by obvious laughter, expressing a strong sense of humor and enjoyment. In the text, the subtitle says, "A-e-e-e, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, and this is not true. On the contrary, you are choosing the wrong one for yourself." This sentence is likely to directly challenge the other person's viewpoint or decision.</aud_desc>
<vis_desc>In the video, a male character appears in the frame, and the scene is indoors. At the beginning of the video, his eyebrows are raised, the veins on his head are prominent, and his eyes are wide open, showing a surprised expression. In the following scenes, he points his finger at the other person, seemingly emphasizing his viewpoint or opinion.</vis_desc>
<think>Based on the surprised expression and wide-open eyes of the male character in the video clues, as well as the obvious laughter and strong sense of humor and enjoyment in the audio clues, we can infer that this sentence carries a negative emotion. The male character is most likely engaged in a sarcast
[2025-11-07 03:04:22,918] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 61/2088 [54:50<30:54:31, 54.89s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1142552285672287, 'learning_rate': 9.707854406130268e-07, 'completion_length': 231.47265625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.3773961812257767, 'reward': 1.6508337259292603, 'reward_std': 0.5083301663398743, 'kl': 0.0072784423828125, 'epoch': 0.06}
  3%|▎         | 61/2088 [54:50<30:54:31, 54.89s/it][2025-11-07 03:05:15,491] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 62/2088 [55:43<30:30:06, 54.20s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.2718201709774055, 'learning_rate': 9.703065134099617e-07, 'completion_length': 222.09375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.3612091988325119, 'reward': 1.5057405233383179, 'reward_std': 0.5159347057342529, 'kl': 0.008026123046875, 'epoch': 0.06}
  3%|▎         | 62/2088 [55:43<30:30:06, 54.20s/it][2025-11-07 03:06:05,020] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 63/2088 [56:32<29:41:55, 52.80s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1072378393317486, 'learning_rate': 9.698275862068966e-07, 'completion_length': 223.5703125, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4737890064716339, 'reward': 1.7355077266693115, 'reward_std': 0.4735405892133713, 'kl': 0.0075531005859375, 'epoch': 0.06}
  3%|▎         | 63/2088 [56:32<29:41:55, 52.80s/it][2025-11-07 03:06:53,772] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 64/2088 [57:21<29:00:05, 51.58s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.060626928734124, 'learning_rate': 9.693486590038313e-07, 'completion_length': 227.46875, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.44656336307525635, 'reward': 1.6887508630752563, 'reward_std': 0.5748862624168396, 'kl': 0.0080108642578125, 'epoch': 0.06}
  3%|▎         | 64/2088 [57:21<29:00:05, 51.58s/it][2025-11-07 03:07:46,329] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 65/2088 [58:13<29:09:05, 51.88s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1046148439651478, 'learning_rate': 9.688697318007663e-07, 'completion_length': 229.49609375, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5028778463602066, 'reward': 1.9989716410636902, 'reward_std': 0.440923735499382, 'kl': 0.0081787109375, 'epoch': 0.06}
  3%|▎         | 65/2088 [58:13<29:09:05, 51.88s/it][2025-11-07 03:08:39,233] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 66/2088 [59:06<29:18:36, 52.18s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1788296825791105, 'learning_rate': 9.68390804597701e-07, 'completion_length': 230.34375, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.5186836421489716, 'reward': 1.7804023623466492, 'reward_std': 0.5563914179801941, 'kl': 0.008148193359375, 'epoch': 0.06}
  3%|▎         | 66/2088 [59:06<29:18:36, 52.18s/it][2025-11-07 03:09:30,339] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 67/2088 [59:57<29:06:50, 51.86s/it]                                                    {'loss': 0.0003, 'grad_norm': 2.1250522149054496, 'learning_rate': 9.67911877394636e-07, 'completion_length': 225.1953125, 'rewards/av_format_reward': 0.66796875, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.47934451699256897, 'reward': 1.6590319871902466, 'reward_std': 0.601643517613411, 'kl': 0.00848388671875, 'epoch': 0.06}
  3%|▎         | 67/2088 [59:57<29:06:50, 51.86s/it][2025-11-07 03:10:22,919] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 68/2088 [1:00:50<29:13:15, 52.08s/it]                                                      {'loss': 0.0003, 'grad_norm': 2.0793032143093546, 'learning_rate': 9.674329501915708e-07, 'completion_length': 236.57421875, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.472006618976593, 'reward': 1.800131618976593, 'reward_std': 0.5801276564598083, 'kl': 0.0079193115234375, 'epoch': 0.07}
  3%|▎         | 68/2088 [1:00:50<29:13:15, 52.08s/it][2025-11-07 03:11:15,562] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 69/2088 [1:01:43<29:18:06, 52.25s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.054368019806048, 'learning_rate': 9.669540229885058e-07, 'completion_length': 226.94921875, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.4553770422935486, 'reward': 1.7210020422935486, 'reward_std': 0.5660976767539978, 'kl': 0.00921630859375, 'epoch': 0.07}
  3%|▎         | 69/2088 [1:01:43<29:18:06, 52.25s/it][2025-11-07 03:12:07,808] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 70/2088 [1:02:35<29:17:13, 52.25s/it]                                                      {'loss': 0.0003, 'grad_norm': 2.079732940403215, 'learning_rate': 9.664750957854407e-07, 'completion_length': 235.1328125, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.4437676668167114, 'reward': 1.6234551072120667, 'reward_std': 0.5353159010410309, 'kl': 0.008544921875, 'epoch': 0.07}
  3%|▎         | 70/2088 [1:02:35<29:17:13, 52.25s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7472.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7816.mp4
[DEBUG] Step 70
[DEBUG] prompt_completion_ids shape: torch.Size([32, 303])
[DEBUG] Shortest output (len=146): <aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "Hey, it's ok for kites and storage facilities!">'
<think>Based on the video clue of the woman's noticeable surprise and wonder, 'Hey, it's ok for kites and storage facilities!', and the audio clue of the inability to determine the character's emotional state, we can infer that this sentence may be spoken by the woman when recalling a past experience or surprised by a current situation. Since the woman's emotions need to be analyzed in context and jointly, combining the video and audio clues would provide a more comprehensive picture.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=303): <vis_desc>In the video, in the opening scene, we see a character wearing a yellow hat, which is usually considered a character's special attire. The character's facial expression is extremely twisted and hurt, showing a clear state of sadness and despair. As time passes, the character's expression becomes more and more soaring passions. Overall, the character's emotional state is quite heavy and tragic.</vis_desc>
<aud_desc>In the audio, the character's voice is quite loud and clear, with a stern and powerful tone. Overall, the character's emotions are quite excited. In the text, the subtitle says, "You're so kind, let's make a fuss." This sentence may be a kind words spoken by the character to another person.</aud_desc>
<think>Based on the video clues of the character's extremely twisted and hurt facial expression, as well as the overall display of sadness and despair, it can be inferred that the character may be experiencing a sad moment in their life. Furthermore, the audio clue of the character's voice being quite loud and clear, as well as his stern and powerful tone, supports the idea that this sentence carries a sense of criticism or dissatisfaction. Therefore, this sentence may express the character's gratitude and acceptance towards another person, as well as a warning and request from the character to others to have a nice mood and celebrate together, which aligns with the emotional states described in the video and audio clues.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12781.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12659.mp4
[DEBUG] Step 70
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=175): <vis_desc>In the video, the screen shows a female character. In the video, she is looking down with a furrowed brow and slightly downturned mouth, seemingly experiencing an emotional challenge.</vis_desc>
<aud_desc>In the audio, the character's voice is accompanied by a heavy breathing sound, indicating strong emotions. In the text, the subtitle reads, "Someone stole my money, oh no." This sentence expresses the female character's dissatisfaction and anxiety.</aud_desc>
<think>Based on the video clues of the female character's furrowed brow and downturned mouth, as well as the audio clue of the character's heavy breathing sound, we can infer that the female character is experiencing an emotional challenge. She may feel worried, anxious, or confused, as this sentence expresses her dissatisfaction and anxiety.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, at the beginning of the video, we see a female character with a furrowed brow and slightly downturned mouth, tears streaming down her face. Her facial expression shows sadness and pain. The background is indistinctly depicting a home environment, which may indicate that she is sad because of someone's death or facing a distressing situation. As time passes, we see her smile again in the following scenes, with a noticeably happier facial expression. Her smile may be a response to a positive event or the comforting gaze of a loved one. Taking these scenes together, we can infer that the emotional state of the female character in this video is one of sadness and pain.</vis_desc>
<aud_desc>In the audio, the tone is extremely emotional, with a high pitch and a loud volume, giving a sensation of being suddenly kicked in the teeth. In the text, the subtitle says, "She's so arrogant, you can stack wood like a pigeon!" This sentence may be a female character's evaluation or accusation of someone.</aud_desc>
<think>Based on the video clues of the female character's furrowed brow, downturned mouth, and tears, as well as her smiling expression in the following scenes, we can infer that the emotional state of the female character in this video is one of sadness and pain. At the same time, based on the audio clue of the character's extremely emotional tone, high pitch, loud volume, and the feeling of being suddenly kicked in the teeth, we can speculate that this sentence carries a sense of anger or mockery. Therefore, this sentence may be a reproach or a sarcastic evaluation of someone's arrogance, expressing the female character's dissatisfaction and anger
[2025-11-07 03:12:57,151] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 71/2088 [1:03:24<28:47:04, 51.38s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.1205228006990793, 'learning_rate': 9.659961685823754e-07, 'completion_length': 221.671875, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.41015625, 'rewards/think_step_av_with_neutral_reward': 0.3970973640680313, 'reward': 1.5689723491668701, 'reward_std': 0.5652205348014832, 'kl': 0.0089111328125, 'epoch': 0.07}
  3%|▎         | 71/2088 [1:03:24<28:47:04, 51.38s/it][2025-11-07 03:13:55,433] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 72/2088 [1:04:23<29:55:50, 53.45s/it]                                                      {'loss': 0.0003, 'grad_norm': 2.0112688932782126, 'learning_rate': 9.655172413793103e-07, 'completion_length': 236.91796875, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.48482000827789307, 'reward': 1.7856013178825378, 'reward_std': 0.5688804686069489, 'kl': 0.008514404296875, 'epoch': 0.07}
  3%|▎         | 72/2088 [1:04:23<29:55:50, 53.45s/it][2025-11-07 03:14:48,526] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  3%|▎         | 73/2088 [1:05:16<29:51:21, 53.34s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.0657309714621004, 'learning_rate': 9.65038314176245e-07, 'completion_length': 229.8671875, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.4416666775941849, 'reward': 1.6057292222976685, 'reward_std': 0.5473838448524475, 'kl': 0.0089111328125, 'epoch': 0.07}
  3%|▎         | 73/2088 [1:05:16<29:51:21, 53.34s/it][2025-11-07 03:15:40,592] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 74/2088 [1:06:08<29:37:38, 52.96s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.1137177702233276, 'learning_rate': 9.6455938697318e-07, 'completion_length': 234.78125, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.42931602895259857, 'reward': 1.7496286034584045, 'reward_std': 0.6075761914253235, 'kl': 0.009552001953125, 'epoch': 0.07}
  4%|▎         | 74/2088 [1:06:08<29:37:38, 52.96s/it][2025-11-07 03:16:24,846] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 75/2088 [1:06:52<28:09:08, 50.35s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.1274200709492987, 'learning_rate': 9.64080459770115e-07, 'completion_length': 215.046875, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.3971337378025055, 'reward': 1.565102458000183, 'reward_std': 0.6076868772506714, 'kl': 0.009521484375, 'epoch': 0.07}
  4%|▎         | 75/2088 [1:06:52<28:09:08, 50.35s/it][2025-11-07 03:17:17,891] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 76/2088 [1:07:45<28:35:26, 51.16s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.0671563969226088, 'learning_rate': 9.636015325670498e-07, 'completion_length': 228.01953125, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5639314353466034, 'reward': 1.9662752747535706, 'reward_std': 0.5004195868968964, 'kl': 0.00897216796875, 'epoch': 0.07}
  4%|▎         | 76/2088 [1:07:45<28:35:26, 51.16s/it][2025-11-07 03:18:12,551] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 77/2088 [1:08:40<29:09:49, 52.21s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.1941224500841745, 'learning_rate': 9.631226053639845e-07, 'completion_length': 225.66015625, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.49365706741809845, 'reward': 1.8881883025169373, 'reward_std': 0.5801783800125122, 'kl': 0.010009765625, 'epoch': 0.07}
  4%|▎         | 77/2088 [1:08:40<29:09:49, 52.21s/it][2025-11-07 03:19:06,607] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▎         | 78/2088 [1:09:34<29:27:32, 52.76s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.0973613791380714, 'learning_rate': 9.626436781609195e-07, 'completion_length': 224.4921875, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.38531234860420227, 'reward': 1.6587498188018799, 'reward_std': 0.5519422888755798, 'kl': 0.009918212890625, 'epoch': 0.07}
  4%|▎         | 78/2088 [1:09:34<29:27:32, 52.76s/it][mpeg4 @ 0x9cc087c0] Error at MB: 1739
[2025-11-07 03:20:00,411] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 79/2088 [1:10:27<29:37:06, 53.07s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.139476785389005, 'learning_rate': 9.621647509578544e-07, 'completion_length': 224.9375, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.60546875, 'rewards/think_step_av_with_neutral_reward': 0.5251080393791199, 'reward': 1.9665144085884094, 'reward_std': 0.5502209961414337, 'kl': 0.00982666015625, 'epoch': 0.08}
  4%|▍         | 79/2088 [1:10:28<29:37:06, 53.07s/it][2025-11-07 03:20:57,375] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 80/2088 [1:11:24<30:15:16, 54.24s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.020747812362536, 'learning_rate': 9.616858237547893e-07, 'completion_length': 235.6953125, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.40625, 'rewards/think_step_av_with_neutral_reward': 0.3777732700109482, 'reward': 1.4441794753074646, 'reward_std': 0.47091126441955566, 'kl': 0.0098876953125, 'epoch': 0.08}
  4%|▍         | 80/2088 [1:11:24<30:15:16, 54.24s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11609.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/352.mp4
[DEBUG] Step 80
[DEBUG] prompt_completion_ids shape: torch.Size([32, 319])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a woman whose facial expression appears gentle and friendly. Her eyes look slightly friendly, and her smile is subtle, as if she is enjoying the conversation.</vis_desc>
<aud_desc>In the audio, the tone and intonation are relatively calm. In the text, the subtitle reads: "It is not true, I will explain it to you." This sentence may be the woman's response to the other person in the conversation.</aud_desc>
<think>Based on the woman's gentle and friendly facial expression in the video clues, as well as the description of a calm tone and intonation in the audio clues, we can infer that this sentence may carry a serious or sincere tone. The woman may be emphasizing her viewpoint or explaining a certain issue seriously, rather than lying or joking.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=319): <vis_desc>In the video, a woman is seen standing in a garden full of flowers. At the beginning of the video, she is looking down at the flowers with a smiling face and a contented posture, indicating that she is enjoying the pleasant environment and the blooms of the flowers. In the following scenes, her gaze lifts back up to look at the flowers, her expression still smiling, and her posture unchanged, indicating that she has stopped thinking and is deeply engaged in observing the flowers. Her eyes are looking intently at the blooms, possibly taking in the essence of the natural world around her.</vis_desc>
<aud_desc>In the audio, when expressing "you can eat," the voice rises, implying a sense of pleasure, and this emotion aligns with the facial expression of the character. In the text, the subtitle reads "You can eat!" this sentence is likely the woman expressing this sentence to the person she is talking to.</aud_desc>
<think>Based on the woman's smiling and contented posture in the video, as well as her direct and focused gaze at the flowers, it can be inferred that she is deeply engaged in observing the flowers and finds them appealing or healthy. Additionally, based on the rising intonation and sense of pleasure in the audio clue, it can be speculated that the woman's emotional state when saying this sentence is happy and pleasant. Therefore, this sentence is likely the woman's invitation to the person she is talking to to eat the flowers, expressing her happiness and engagement with the environment.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07775.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7674.mp4
[DEBUG] Step 80
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=181): <vis_desc>In the video, we see a male character with a furrowed brow, slightly downturned mouth, and tightly closed eyes, showing a sad facial expression.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The male character’s visual cues strongly convey sadness: his furrowed brow, downturned mouth, and tightly closed eyes are classic, challenging facial expressions seen in individuals experiencing profound emotional distress. Although the audio and subtitle content do not offer additional emotional information, the clear visual signal of sadness is enough to infer it. Together, these multimodal signals—primarily the visual appearance supported by the male character’s facial expressions—are sufficient to confidently predict that the character is experiencing sadness.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, the tone is low and conveys a sense of helplessness. In the text, the subtitle reads, "<vis_desc>In the video, the screen shows a male character, and the scene should be indoors. In the opening shot, we see a close-up of a male character's face. His facial expression appears somewhat distressed, with a furrowed brow and tightly pursed lips. The corners of his mouth are slightly open, as if speaking or responding to someone.</vis_desc>
<aud_desc>In the audio, the tone is low and conveys a sense of helplessness. In the text, the subtitle reads, "<aud_desc>In the audio, the tone is low and carries a sense of helplessness. Combined with the text content, it seems to express sympathy and worry towards the other person. In the text, the subtitle says, "<vis_desc>In the video, the screen shows a male character, and the scene should be indoors. In the opening shot, we see a close-up of the male character's face. His facial expression appears somewhat distressed, with a furrowed brow and tightly pursed lips. The corners of his mouth are slightly open, as if speaking or responding to someone. Overall, it appears that the character is going through a sad situation. As time passes, he seems to feel even worse, which indicates that he may be experiencing prolonged sadness.</vis_desc>
<think>The male character's expressive facial cues—frown, purse lips, slightly open mouth—suggest distress and possibly a want to cry, which are strong indicators of sadness. Despite these physical signs of pain, he is characterized as "sad, passed away a friend," a statement
[2025-11-07 03:21:54,756] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 81/2088 [1:12:22<30:45:53, 55.18s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.092564707771989, 'learning_rate': 9.61206896551724e-07, 'completion_length': 226.85546875, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.4896925389766693, 'reward': 1.8256300687789917, 'reward_std': 0.5798330008983612, 'kl': 0.01031494140625, 'epoch': 0.08}
  4%|▍         | 81/2088 [1:12:22<30:45:53, 55.18s/it][2025-11-07 03:22:45,502] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 82/2088 [1:13:13<30:00:28, 53.85s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.0415030208780105, 'learning_rate': 9.60727969348659e-07, 'completion_length': 224.54296875, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.40289968252182007, 'reward': 1.60993093252182, 'reward_std': 0.5405716300010681, 'kl': 0.011566162109375, 'epoch': 0.08}
  4%|▍         | 82/2088 [1:13:13<30:00:28, 53.85s/it][2025-11-07 03:23:39,470] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 83/2088 [1:14:07<30:00:42, 53.89s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.0775672079735106, 'learning_rate': 9.60249042145594e-07, 'completion_length': 223.30859375, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.4650418758392334, 'reward': 1.9337918758392334, 'reward_std': 0.5924845933914185, 'kl': 0.010650634765625, 'epoch': 0.08}
  4%|▍         | 83/2088 [1:14:07<30:00:42, 53.89s/it][2025-11-07 03:24:33,851] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 84/2088 [1:15:01<30:04:46, 54.03s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.20855978009877, 'learning_rate': 9.597701149425286e-07, 'completion_length': 224.89453125, 'rewards/av_format_reward': 0.65625, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.39066796004772186, 'reward': 1.5312930345535278, 'reward_std': 0.5495821535587311, 'kl': 0.011444091796875, 'epoch': 0.08}
  4%|▍         | 84/2088 [1:15:01<30:04:46, 54.03s/it][2025-11-07 03:25:27,242] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 85/2088 [1:15:54<29:57:25, 53.84s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.124178341049941, 'learning_rate': 9.592911877394635e-07, 'completion_length': 225.31640625, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.46632955968379974, 'reward': 1.704610824584961, 'reward_std': 0.5465422868728638, 'kl': 0.010955810546875, 'epoch': 0.08}
  4%|▍         | 85/2088 [1:15:54<29:57:25, 53.84s/it][2025-11-07 03:26:19,283] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 86/2088 [1:16:46<29:38:30, 53.30s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.1840629265283433, 'learning_rate': 9.588122605363985e-07, 'completion_length': 224.3046875, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.4523557871580124, 'reward': 1.7257933020591736, 'reward_std': 0.6714709401130676, 'kl': 0.011444091796875, 'epoch': 0.08}
  4%|▍         | 86/2088 [1:16:46<29:38:30, 53.30s/it][2025-11-07 03:27:10,652] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 87/2088 [1:17:38<29:18:19, 52.72s/it]                                                      {'loss': 0.0005, 'grad_norm': 1.9960385237683813, 'learning_rate': 9.583333333333334e-07, 'completion_length': 227.0234375, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.5053835213184357, 'reward': 1.685071051120758, 'reward_std': 0.6380689144134521, 'kl': 0.011260986328125, 'epoch': 0.08}
  4%|▍         | 87/2088 [1:17:38<29:18:19, 52.72s/it][2025-11-07 03:28:01,608] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 88/2088 [1:18:29<28:59:42, 52.19s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.087867878333649, 'learning_rate': 9.578544061302681e-07, 'completion_length': 221.76171875, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.4176647812128067, 'reward': 1.6715710163116455, 'reward_std': 0.5914658159017563, 'kl': 0.011138916015625, 'epoch': 0.08}
  4%|▍         | 88/2088 [1:18:29<28:59:42, 52.19s/it][2025-11-07 03:28:57,918] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 89/2088 [1:19:25<29:40:00, 53.43s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.0192682443150543, 'learning_rate': 9.57375478927203e-07, 'completion_length': 223.0234375, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.45318683981895447, 'reward': 1.6445931196212769, 'reward_std': 0.47739899158477783, 'kl': 0.01129150390625, 'epoch': 0.09}
  4%|▍         | 89/2088 [1:19:25<29:40:00, 53.43s/it][2025-11-07 03:29:48,648] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 90/2088 [1:20:16<29:12:10, 52.62s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.083708495578735, 'learning_rate': 9.568965517241378e-07, 'completion_length': 231.67578125, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.4417031705379486, 'reward': 1.7659218907356262, 'reward_std': 0.6303372979164124, 'kl': 0.01171875, 'epoch': 0.09}
  4%|▍         | 90/2088 [1:20:16<29:12:10, 52.62s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01758.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07961.mp4
[DEBUG] Step 90
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=170): <aud_desc>In the audio, there are obvious sighs during the character's breathing, and the tone is very low and weak, suggesting that the character is feeling very sad and painlessly.</aud_desc>
<vis_desc>In the video, we see a woman with a furrowed brow and slightly downturned mouth, which may indicate that she is feeling down or frustrated. Her gaze seems somewhat vacant, as if she is contemplating something.</vis_desc>
<think>Based on the visual clue of the woman's furrowed brow and downturned mouth, as well as the audio clue of the character's sighing and low and weak tone, we can infer that the woman may be feeling down and very sad. Her gaze may be vacant, indicating that she is contemplating something troubling her.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see a woman with a furrowed brow and a slightly downturned mouth, tears streaming down her face. Her gaze is fixed on the distance, and her body posture appears somewhat heavy. Overall, we can infer that the character in the video is experiencing emotional pain.</vis_desc>
<aud_desc>In the audio, there is obvious sobbing in the character's voice.combined with the text, it can be felt that the character is weeping due to emotional pain. The specific emotional reason needs to be analyzed in conjunction with other clues. in the text, the character's voice sounds very low and sobbing, as if she is crying loudly. it can be inferred from the description that she is experiencing emotional pain and sobbing that she is crying.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the character is experiencing emotional turmoil and pain. Visually, the woman’s furrowed brow and slightly downturned mouth, paired with tears streaming down her face and her fixed gaze towards the distance, unmistakably display feelings of sorrow and distress. Her weighty body posture aligns with sadness or grief, reinforcing a deeply emotionally charged state. Audibly, the presence of obvious sobbing in her voice further accentuates her vulnerable condition, while the mention of the character weeping due to emotional pain implies a clear emotional burden or distress that underscores her pain. Thinkers historically, the combination of somber facial expressions, body language, and vocal tone have been strongly suggestive of sadness or grief. Visually, the tight-cropped facial appearance and attentive gaze convey a deeply affected and real-life moment rather than a staged or emotive attempt. Audibly, the vocal quality and emotional
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08943.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05052.mp4
[DEBUG] Step 90
[DEBUG] prompt_completion_ids shape: torch.Size([32, 305])
[DEBUG] Shortest output (len=188): <vis_desc>In the video, we see a female character with a furrowed brow and slightly downturned mouth, seemingly experiencing an unpleasant emotional state. Her gaze seems to be fixed on someone off-screen, who is not visible to the camera.</vis_desc>
<aud_desc>In the audio, the character's tone is urgent and they express the word "Bah." This phrase may be a response from the female character to someone off-screen.</aud_desc>
<think>Based on the video clues of the female character's furrowed brow and downturned mouth, as well as the description of the character's urgent tone in the audio, we can infer that the female character's emotional state is one of unease or nervousness. The mention of "Bah" in the video clue may imply that something is happening unexpectedly or suddenly, which triggered her unease or nervousness.</think>
<answer>anxiety</answer>
[DEBUG] Longest output (len=305): <vis_desc>In the video, we see a female character with slightly furrowed brows and an open mouth, seemingly crying. Her gaze is downward, seemingly lost or searching for answers.</vis_desc>
<aud_desc>In the audio, the tone is low and the intonation is steady. In the text, the subtitle reads, "She's come all through it, but look at herself! Memes."</aud_desc>
<think>The combined visual and auditory cues in this scenario strongly suggest that the female character is experiencing emotional distress or confusion. Visually, her furrowed brows, partially open mouth, and downward gaze convey a state of internal turmoil or seeking meaning can be indicative of someone who is百足，suffering or searching for answers. The low tone and steady intonation in her voice further reinforce the sense of inner struggle or unease. Audibly, the tone suggests a dull, helpless rhythm, while the mention of "mersons" at the end adds a humorous or mocking context, intensifying the sense of disappointment or sneeze the character is feeling. Even though her subtitle content doesn't convey explicit happiness, the visual indicators clearly depict her having rung out all available opportunities yet still finding it "magical," which can be interpreted as a constant struggle or the unexpected allure of one's condition. Therefore, integrating these multimodal signals, it is reasonable to conclude that the emotional state conveyed by the female character is emotional disappointment.</think>
<answer>disappointed</answer>
[2025-11-07 03:30:42,803] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 91/2088 [1:21:10<29:26:39, 53.08s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.146961303396006, 'learning_rate': 9.564176245210727e-07, 'completion_length': 235.375, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.456607386469841, 'reward': 1.6870761513710022, 'reward_std': 0.6372356414794922, 'kl': 0.0113525390625, 'epoch': 0.09}
  4%|▍         | 91/2088 [1:21:10<29:26:39, 53.08s/it][2025-11-07 03:31:36,119] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 92/2088 [1:22:03<29:28:08, 53.15s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.1304151157891633, 'learning_rate': 9.559386973180076e-07, 'completion_length': 230.578125, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4624090939760208, 'reward': 1.8178777694702148, 'reward_std': 0.5613881796598434, 'kl': 0.0118408203125, 'epoch': 0.09}
  4%|▍         | 92/2088 [1:22:03<29:28:08, 53.15s/it][2025-11-07 03:32:29,479] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4%|▍         | 93/2088 [1:22:57<29:29:20, 53.21s/it]                                                      {'loss': 0.0004, 'grad_norm': 2.104596775819228, 'learning_rate': 9.554597701149425e-07, 'completion_length': 240.42578125, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.4184970557689667, 'reward': 1.719278335571289, 'reward_std': 0.613196074962616, 'kl': 0.011138916015625, 'epoch': 0.09}
  4%|▍         | 93/2088 [1:22:57<29:29:20, 53.21s/it][2025-11-07 03:33:19,154] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 94/2088 [1:23:46<28:53:10, 52.15s/it]                                                      {'loss': 0.0014, 'grad_norm': 2.559528902332566, 'learning_rate': 9.549808429118773e-07, 'completion_length': 225.73828125, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.543702095746994, 'reward': 1.7390146255493164, 'reward_std': 0.47442564368247986, 'kl': 0.03594970703125, 'epoch': 0.09}
  5%|▍         | 94/2088 [1:23:46<28:53:10, 52.15s/it][2025-11-07 03:34:11,791] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 95/2088 [1:24:39<28:57:08, 52.30s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.1590186201208894, 'learning_rate': 9.545019157088122e-07, 'completion_length': 224.92578125, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.4618082046508789, 'reward': 1.676651954650879, 'reward_std': 0.4537225067615509, 'kl': 0.012176513671875, 'epoch': 0.09}
  5%|▍         | 95/2088 [1:24:39<28:57:08, 52.30s/it][2025-11-07 03:35:04,669] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 96/2088 [1:25:32<29:02:02, 52.47s/it]                                                      {'loss': 0.0005, 'grad_norm': 1.9118321628308863, 'learning_rate': 9.540229885057471e-07, 'completion_length': 232.3515625, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.4475148320198059, 'reward': 1.791264832019806, 'reward_std': 0.5034113079309464, 'kl': 0.0118408203125, 'epoch': 0.09}
  5%|▍         | 96/2088 [1:25:32<29:02:02, 52.47s/it][2025-11-07 03:35:50,793] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 97/2088 [1:26:18<27:57:59, 50.57s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.12833338125038, 'learning_rate': 9.535440613026819e-07, 'completion_length': 219.39453125, 'rewards/av_format_reward': 0.67578125, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4297060966491699, 'reward': 1.6523624062538147, 'reward_std': 0.6001403331756592, 'kl': 0.012664794921875, 'epoch': 0.09}
  5%|▍         | 97/2088 [1:26:18<27:57:59, 50.57s/it][2025-11-07 03:36:41,431] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 98/2088 [1:27:09<27:57:51, 50.59s/it]                                                      {'loss': 0.0005, 'grad_norm': 2.1343113862402197, 'learning_rate': 9.530651340996169e-07, 'completion_length': 218.0625, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.39725761115550995, 'reward': 1.5886638164520264, 'reward_std': 0.5540060997009277, 'kl': 0.012603759765625, 'epoch': 0.09}
  5%|▍         | 98/2088 [1:27:09<27:57:51, 50.59s/it][2025-11-07 03:37:35,590] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 99/2088 [1:28:03<28:32:30, 51.66s/it]                                                      {'loss': 0.0005, 'grad_norm': 1.993803365178859, 'learning_rate': 9.525862068965517e-07, 'completion_length': 230.40625, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.4447144567966461, 'reward': 1.6790894269943237, 'reward_std': 0.5433880090713501, 'kl': 0.01263427734375, 'epoch': 0.09}
  5%|▍         | 99/2088 [1:28:03<28:32:30, 51.66s/it][2025-11-07 03:38:25,310] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 100/2088 [1:28:52<28:12:22, 51.08s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.125457134098907, 'learning_rate': 9.521072796934866e-07, 'completion_length': 224.984375, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.60546875, 'rewards/think_step_av_with_neutral_reward': 0.537627786397934, 'reward': 1.8618466258049011, 'reward_std': 0.5321063101291656, 'kl': 0.012176513671875, 'epoch': 0.1}
  5%|▍         | 100/2088 [1:28:52<28:12:22, 51.08s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2588.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9787.mp4
[DEBUG] Step 100
[DEBUG] prompt_completion_ids shape: torch.Size([32, 294])
[DEBUG] Shortest output (len=172): <aud_desc>In the audio, the character's voice sounds very scared, with a tense and nervous tone. In the text, the subtitle says, " came here." This sentence may be a scream or shout from the man in response to the other person.</aud_desc>
<vis_desc>In the video, we see a man wearing a red and white helmet, white gloves, and glasses, standing next to a gun. He is holding a rifle and looking at the other person with a threatening facial expression.</vis_desc>
<think>Based on the man's threatening facial expression and the description of the character's tense and nervous tone in the audio, we can infer that this sentence carries a sense of anger or excitement. The man may be uncomfortable with the other person's presence, hence expressing this word.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=294): <vis_desc>In the video, the screen shows a man sitting in the car. In the video, his eyes are wide open, his mouth is wide open, and his facial expression appears very scared. He stares directly at the other person, with his body slightly tilted, seemingly hoping for a response or defending himself. Overall, he is experiencing a highly aroused and hostile emotional state.</vis_desc>
<aud_desc>In the audio, the character's volume is very loud and the tone is aggressive, giving the impression that the character's emotions are one of excitement. In the text, the subtitle reads: "What's the fuki?Isn't it a bad idea?" This sentence may be the man's response to a suggestion or request.</aud_desc>
<think>Based on the man's wide-open eyes, mouth, and facial expression in the video, as well as his posture of looking directly at the other person and slightly tilting his head, it can be inferred that he is experiencing a highly aroused and hostile emotion. Additionally, the word "fuki" in the subtitle may be the man's reaction to a certain idea or request.Considering the audio clue of an aggressive tone, one can speculate that the man's emotional state may be anger or excitement. Therefore, this sentence may be the man's refusal or questioning of a certain viewpoint or action, expressing a strong dissatisfaction and anger.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11069.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8842.mp4
[DEBUG] Step 100
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=167): <aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to infer the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, we see a female character with a partially shrouded head, leaning against a wire fence in the snow. Her eyes are wide open and she seems to be experiencing an emotional shock or receiving strange information.</vis_desc>
<think>Due to the absence of the female character's facial features, as well as the non-informative subtitle content, we are unable to infer her emotional state directly. However, the visual cues suggest that the character's eyes are wide open and she may be experiencing an emotional shock or receiving strange information from the environment.</think>
<answer>anxiety</answer>
[DEBUG] Longest output (len=302): <vis_desc>In the video, we see a woman wearing a blue headscarf and a blue outfit. She has a red damage on her forehead and some black marks on her nose and mouth. In the video, she slightly furrows her brows, lifts the corners of her mouth, and her eyes are slightly closed, as if she is crying or sobbing. Overall, she seems to be experiencing an inner struggle or pain.</vis_desc>
<aud_desc>In the audio, the voice is low and the tone is heavy, conveying a sense of sorrow and despair. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman’s visual cues strongly indicate an emotional state of sorrow. Her slight furrowing of the brows, lifted corners of the mouth, and the closed eyes suggest a deep emotional distress or pain, typical of someone experiencing grief or emotional pain. The low vocal tone and heavy tone of voice further convey a heavy and sorrowful emotional weight. Additionally, the audio description of a low voice coupled with a heavy, sobbering tone reinforces this interpretation. Even though the subtitle content does not explicitly reveal her emotions, the combined facial expressions and vocal characteristics together strongly support the conclusion that the predominant emotion being experienced is sorrow. This integration of multimodal evidence—primarily the visual cues of distress and pain paired with the vocal tone of sorrow—facilitates a coherent emotional portrayal.</think>
<answer>sad</answer>
[2025-11-07 03:39:30,054] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 101/2088 [1:29:57<30:27:18, 55.18s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.078718088736882, 'learning_rate': 9.516283524904214e-07, 'completion_length': 231.74609375, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.72265625, 'rewards/think_step_av_with_neutral_reward': 0.5948660671710968, 'reward': 2.0167410373687744, 'reward_std': 0.609222024679184, 'kl': 0.0120849609375, 'epoch': 0.1}
  5%|▍         | 101/2088 [1:29:57<30:27:18, 55.18s/it][2025-11-07 03:40:24,256] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 102/2088 [1:30:51<30:16:41, 54.88s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.1127397898285, 'learning_rate': 9.511494252873563e-07, 'completion_length': 234.53125, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.40618447959423065, 'reward': 1.4725908041000366, 'reward_std': 0.5148671269416809, 'kl': 0.011932373046875, 'epoch': 0.1}
  5%|▍         | 102/2088 [1:30:51<30:16:41, 54.88s/it][2025-11-07 03:41:17,383] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 103/2088 [1:31:44<29:58:19, 54.36s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.179402942484961, 'learning_rate': 9.506704980842911e-07, 'completion_length': 229.59375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.41796875, 'rewards/think_step_av_with_neutral_reward': 0.32437436282634735, 'reward': 1.4571868777275085, 'reward_std': 0.5645756721496582, 'kl': 0.01177978515625, 'epoch': 0.1}
  5%|▍         | 103/2088 [1:31:44<29:58:19, 54.36s/it][2025-11-07 03:42:09,051] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 104/2088 [1:32:36<29:30:44, 53.55s/it]                                                       {'loss': 0.0005, 'grad_norm': 1.9562770815425679, 'learning_rate': 9.50191570881226e-07, 'completion_length': 236.09375, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.45895493030548096, 'reward': 1.755829930305481, 'reward_std': 0.5202673226594925, 'kl': 0.011383056640625, 'epoch': 0.1}
  5%|▍         | 104/2088 [1:32:36<29:30:44, 53.55s/it][2025-11-07 03:42:58,864] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 105/2088 [1:33:26<28:52:47, 52.43s/it]                                                       {'loss': 0.0005, 'grad_norm': 1.9301175480204178, 'learning_rate': 9.497126436781608e-07, 'completion_length': 231.4140625, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4381541311740875, 'reward': 1.8014354705810547, 'reward_std': 0.5618270933628082, 'kl': 0.012451171875, 'epoch': 0.1}
  5%|▌         | 105/2088 [1:33:26<28:52:47, 52.43s/it][2025-11-07 03:43:50,962] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 106/2088 [1:34:18<28:48:37, 52.33s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.0753670301610234, 'learning_rate': 9.492337164750958e-07, 'completion_length': 235.4296875, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.46279221773147583, 'reward': 1.7206047177314758, 'reward_std': 0.524310290813446, 'kl': 0.012481689453125, 'epoch': 0.1}
  5%|▌         | 106/2088 [1:34:18<28:48:37, 52.33s/it][2025-11-07 03:44:39,754] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 107/2088 [1:35:07<28:12:43, 51.27s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.039433047166461, 'learning_rate': 9.487547892720306e-07, 'completion_length': 227.71875, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.4447433352470398, 'reward': 1.8158370852470398, 'reward_std': 0.4576207250356674, 'kl': 0.01263427734375, 'epoch': 0.1}
  5%|▌         | 107/2088 [1:35:07<28:12:43, 51.27s/it][2025-11-07 03:45:30,268] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 108/2088 [1:35:57<28:04:23, 51.04s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.0101198809384524, 'learning_rate': 9.482758620689655e-07, 'completion_length': 229.5703125, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.4459649473428726, 'reward': 1.7272148728370667, 'reward_std': 0.46435868740081787, 'kl': 0.0135498046875, 'epoch': 0.1}
  5%|▌         | 108/2088 [1:35:57<28:04:23, 51.04s/it][2025-11-07 03:46:22,647] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 109/2088 [1:36:50<28:16:45, 51.44s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.1678432832926835, 'learning_rate': 9.477969348659003e-07, 'completion_length': 223.31640625, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.46627889573574066, 'reward': 1.813935101032257, 'reward_std': 0.5923413038253784, 'kl': 0.012725830078125, 'epoch': 0.1}
  5%|▌         | 109/2088 [1:36:50<28:16:45, 51.44s/it][2025-11-07 03:47:14,480] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 110/2088 [1:37:42<28:19:45, 51.56s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0798557654675562, 'learning_rate': 9.473180076628352e-07, 'completion_length': 231.21484375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.39441007375717163, 'reward': 1.6600351333618164, 'reward_std': 0.5645314157009125, 'kl': 0.013885498046875, 'epoch': 0.11}
  5%|▌         | 110/2088 [1:37:42<28:19:45, 51.56s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3157.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02617.mp4
[DEBUG] Step 110
[DEBUG] prompt_completion_ids shape: torch.Size([32, 301])
[DEBUG] Shortest output (len=162): <vis_desc>In the video, we see a woman with a slight upward curve at the corners of her mouth, wearing a smile. Her eyes glint, indicating a positive and friendly emotional state.</vis_desc>
<aud_desc>In the audio, the character's tone is neutral. In the text, the subtitle reads: "Did Rendal ever mention Serum shorter than you?"</aud_desc>
<think>Based on the woman's smile and positive emotional state in the video clues, as well as the neutral tone described in the audio clue, we can infer that this sentence is expressed in a teasing or joking tone. The woman may be teasing Rendal by asking a question about his medication, which may make Rendal surprised and confused.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=301): <vis_desc>In the video, the screen shows a male character. In the video, his facial expression is relatively calm, with a slight upward curve at the corners of his mouth, and his gaze is directly facing the other person. This indicates that he is engaged in a serious conversation. His mouth moves slightly, confirming that he is actively speaking. Overall, it can be inferred that the character is in a somewhat relaxed and pleasant emotional state, possibly engaged in communication or interaction.</vis_desc>
<aud_desc>In the audio, there is a sense of laughter in the character's voice, suggesting that he is teasing himself with a slap. In the text, the subtitle says, "Just good guys hug." This sentence may be a character's way of making funny gestures or expressions to indicate his pleasant emotional state.</aud_desc>
<think>Based on the man's calm facial expression with a slight upward curve at the corners of his mouth and direct gaze towards the other person in the video, as well as the lively and humorous emotion he displays while laughing, we can infer that he holds this phrase in a positive and relaxed emotions. Additionally, the laughter in the character's voice further supports this inference. Therefore, this sentence may be a humorous way of expressing the character's optimistic attitude towards guys, indicating that he believes guys can get through difficult times and even embrace them, which aligns with the overall relaxed and pleasant emotional state portrayed by the character.</think>
<answer>happy</answer>
[mpeg4 @ 0x9b22b180] slice end not reached but screenspace end (652 left 11A086, score= -51)
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09345.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/13192.mp4
[DEBUG] Step 110
[DEBUG] prompt_completion_ids shape: torch.Size([32, 264])
[DEBUG] Shortest output (len=167): <vis_desc>In the video, we see a female character with wide-open eyes and slightly parted lips, seemingly expressing surprise or fear.</vis_desc>
<aud_desc>In the audio, there is a scream in the voice, expressing the character's extreme fear. In the text, the subtitle says, "Oh." This sentence may be the female character's reaction to a sudden situation or event.</aud_desc>
<think>Based on the video clue of the female character's wide-open eyes and slightly parted lips, as well as the audio clue of the scream, we can infer that this sentence expresses the female character's fear and surprise. The female character may have encountered a sudden situation or event that shocked her, and this situation led to her death, causing her fear and surprise.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=264): <vis_desc>In the video, we see a woman standing next to a door. Her eyes are wide open and she slowly moves her head to the right side. She seems to see something or someone on the right side of the door that has scared her.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The woman's wide-open eyes and slow head movement to the right indicate a sudden, heightened state of alertness or fear, as if she is reacting to something unexpected or dangerous on the right side of the door. Although the auditory cues and subtitle content do not provide any clear emotional signal, the visual expression of her facial muscles and posture strongly conveys a fear response. Her ears are pricked forward and her eyes are wide open, reflecting heightened alertness and vulnerability. Together, these visual signals—particularly the alignment of her facial muscles with alertness and the forward-looking ears and eyes—suggest that she is experiencing fear as she observes or prepares to act on something threatening. Thus, the combination of auditory fear indicators and visual signs reinforces the conclusion that the target emotional state is indeed fear.</think>
<answer>fear</answer>
[2025-11-07 03:48:03,435] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 111/2088 [1:38:31<27:53:10, 50.78s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.062863084849036, 'learning_rate': 9.468390804597701e-07, 'completion_length': 223.12109375, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.5047620534896851, 'reward': 1.719605803489685, 'reward_std': 0.5297046899795532, 'kl': 0.012908935546875, 'epoch': 0.11}
  5%|▌         | 111/2088 [1:38:31<27:53:10, 50.78s/it][2025-11-07 03:48:56,965] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 112/2088 [1:39:24<28:19:29, 51.60s/it]                                                       {'loss': 0.0005, 'grad_norm': 1.9456366810366728, 'learning_rate': 9.463601532567049e-07, 'completion_length': 228.03515625, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.38546372950077057, 'reward': 1.5456199645996094, 'reward_std': 0.4475513696670532, 'kl': 0.012664794921875, 'epoch': 0.11}
  5%|▌         | 112/2088 [1:39:24<28:19:29, 51.60s/it][2025-11-07 03:49:48,634] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 113/2088 [1:40:16<28:19:17, 51.62s/it]                                                       {'loss': 0.0005, 'grad_norm': 1.865584645317522, 'learning_rate': 9.458812260536398e-07, 'completion_length': 228.48046875, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.37890625, 'rewards/think_step_av_with_neutral_reward': 0.3606433868408203, 'reward': 1.567674696445465, 'reward_std': 0.4277533292770386, 'kl': 0.013031005859375, 'epoch': 0.11}
  5%|▌         | 113/2088 [1:40:16<28:19:17, 51.62s/it][2025-11-07 03:50:41,235] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▌         | 114/2088 [1:41:08<28:28:03, 51.92s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.1208288551658705, 'learning_rate': 9.454022988505747e-07, 'completion_length': 224.11328125, 'rewards/av_format_reward': 0.94140625, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5031891763210297, 'reward': 2.0461578369140625, 'reward_std': 0.6447585225105286, 'kl': 0.01336669921875, 'epoch': 0.11}
  5%|▌         | 114/2088 [1:41:08<28:28:03, 51.92s/it][2025-11-07 03:51:33,392] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 115/2088 [1:42:00<28:29:33, 51.99s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.090851223328482, 'learning_rate': 9.449233716475096e-07, 'completion_length': 219.953125, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.41796875, 'rewards/think_step_av_with_neutral_reward': 0.39080649614334106, 'reward': 1.461118996143341, 'reward_std': 0.45358806848526, 'kl': 0.01336669921875, 'epoch': 0.11}
  6%|▌         | 115/2088 [1:42:00<28:29:33, 51.99s/it][2025-11-07 03:52:23,356] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 116/2088 [1:42:50<28:08:45, 51.38s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.147978021264145, 'learning_rate': 9.444444444444444e-07, 'completion_length': 220.875, 'rewards/av_format_reward': 0.9140625, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.42611634731292725, 'reward': 1.8636162877082825, 'reward_std': 0.5524614155292511, 'kl': 0.0140380859375, 'epoch': 0.11}
  6%|▌         | 116/2088 [1:42:50<28:08:45, 51.38s/it][2025-11-07 03:53:12,658] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 117/2088 [1:43:40<27:47:22, 50.76s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.030567419091921, 'learning_rate': 9.439655172413793e-07, 'completion_length': 222.83984375, 'rewards/av_format_reward': 0.66796875, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.2981063649058342, 'reward': 1.35279381275177, 'reward_std': 0.3643009066581726, 'kl': 0.01422119140625, 'epoch': 0.11}
  6%|▌         | 117/2088 [1:43:40<27:47:22, 50.76s/it][2025-11-07 03:54:05,663] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 118/2088 [1:44:33<28:08:41, 51.43s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.0093418963056133, 'learning_rate': 9.434865900383141e-07, 'completion_length': 227.5390625, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.4886390417814255, 'reward': 1.6527015566825867, 'reward_std': 0.5941761136054993, 'kl': 0.013336181640625, 'epoch': 0.11}
  6%|▌         | 118/2088 [1:44:33<28:08:41, 51.43s/it][2025-11-07 03:54:59,364] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 119/2088 [1:45:26<28:30:08, 52.11s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.195501071806883, 'learning_rate': 9.43007662835249e-07, 'completion_length': 223.59375, 'rewards/av_format_reward': 0.6796875, 'rewards/accuracy_reward': 0.671875, 'rewards/think_step_av_with_neutral_reward': 0.5353325605392456, 'reward': 1.8868950009346008, 'reward_std': 0.48729220032691956, 'kl': 0.01416015625, 'epoch': 0.11}
  6%|▌         | 119/2088 [1:45:26<28:30:08, 52.11s/it][2025-11-07 03:55:52,677] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 120/2088 [1:46:20<28:41:06, 52.47s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.055887944854057, 'learning_rate': 9.425287356321838e-07, 'completion_length': 227.8515625, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.38801969587802887, 'reward': 1.6341134309768677, 'reward_std': 0.49563586711883545, 'kl': 0.01336669921875, 'epoch': 0.11}
  6%|▌         | 120/2088 [1:46:20<28:41:06, 52.47s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/06846.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03939.mp4
[DEBUG] Step 120
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=186): <vis_desc>In the video, we see a man wearing a black suit standing on stage. His facial expression appears relaxed and confident, with his mouth slightly open as if speaking. He appears to be engaged in a conversation or explanation.</vis_desc>
<aud_desc>In the audio, the character's tone is relatively calm. In the text, the subtitle reads: "I think I should be able to recognize it." This sentence may be the man's response to a question or statement.</aud_desc>
<think>Based on the video clues of the man's relaxed and confident facial expression, with his mouth slightly open as if speaking, as well as the audio clue of the character's calm tone, we can infer that this sentence may be the man's confident expression of his awareness or willingness to recognize something. The specific emotion needs to be analyzed in conjunction with additional context.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, in the opening scene, we see a character wearing a black and white patterned suit with wide-open hands, as if performing some kind of gesture or expressing dissatisfaction. The background looks like windows and a door, which may indicate that the character is in a social setting. In the following scene, the character puts down the object in his hand and the action is no longer visible. In the very end of the video, an audio is provided, and the character's voice is excited and quick. Overall, we can infer that the character in the video is likely in a state of anger or excitement, and the specific emotion needs to be analyzed in conjunction with additional context.</vis_desc>
<aud_desc>In the audio, the character speaks quickly, with an excited tone, and a high pitch, suggesting that this sentence may be spoken after the character has finally been accepted. Overall, we can infer that the character in the video is likely in a state of anger or excitement, and the specific emotion needs to be analyzed in conjunction with additional context.</aud_desc>
<think>The character's wide-open hands, shown in the video, visually convey dissatisfaction and frustration. His performance of a body gesture, such as making multiple hand shapes and placing his hands over his chest, suggests inner anger and anxiety. The context, including the windows and door, implies that the character is in a social setting, likely in an argument or dispute. The audio further reinforces this inference, where he speaks quickly, with an excited tone and a high pitch, indicating heightened emotion even before being accepted. Although the subtitle text does not provide emotional information, the fast speech with excited and urgent sounds strongly indicate anger or excitement. Even though the visual display
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04173.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1445.mp4
[DEBUG] Step 120
[DEBUG] prompt_completion_ids shape: torch.Size([32, 271])
[DEBUG] Shortest output (len=176): <vis_desc>In the video, we see a woman with a furrowed brow and slightly downturned mouth, her facial expression conveying sadness and confusion.</vis_desc>
<aud_desc>In the audio, the speech is fast-paced, with a stern and aggressive tone. The emphasis is on "my son," suggesting that the woman is angry at her son. In the text, the subtitle reads, "My son." This sentence is likely spoken by the woman during the intense argument.</aud_desc>
<think>Based on the facial expressions of the woman in the video and the fast-paced, stern, and aggressive tone in the audio, we can infer that this sentence carries a sense of anger and dissatisfaction. The woman's facial expression and tone both indicate that she is unhappy with something and is ready to express her anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=271): <vis_desc>In the video, the screen shows a female character. In the video, her facial expression appears relaxed and at ease, with a slight upward curve at the corners of her mouth, indicating a friendly and confident smile. Her gaze looks slightly upward, possibly indicating that she is engaged in a pleasant conversation or receiving some kind of evaluation or reminder from a stimulus outside the frame. Overall, her tone and expression suggest a positive and stable emotional state.</vis_desc>
<aud_desc>In the audio, the tone is relatively calm and the pace is steady. Combined with the text content, it seems to carry a sense of encouragement towards the other person. In the text, the subtitle reads: "I correctioned the car and we overcharged by an overpass." This sentence may be the female character's evaluation or commentary about the car or an external situation.</aud_desc>
<think>Based on the female character's relaxed and confident smile in the video clues, as well as the calm tone and steady pace in the audio clues, we can infer that this sentence may have a positive and humorous tone. The female character may be satisfied with the car correction and acknowledges some kindness or recognition from someone or something outside the frame. Therefore, this sentence expresses the female character's friendly and confident emotional state.</think>
<answer>happy</answer>
[2025-11-07 03:56:43,362] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 121/2088 [1:47:10<28:22:39, 51.94s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.095102710473434, 'learning_rate': 9.420498084291187e-07, 'completion_length': 222.92578125, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.361552894115448, 'reward': 1.6115529537200928, 'reward_std': 0.49607646465301514, 'kl': 0.013671875, 'epoch': 0.12}
  6%|▌         | 121/2088 [1:47:10<28:22:39, 51.94s/it][2025-11-07 03:57:35,794] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 122/2088 [1:48:03<28:26:38, 52.08s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.041867531811426, 'learning_rate': 9.415708812260536e-07, 'completion_length': 222.1171875, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.45415960252285004, 'reward': 1.9190033078193665, 'reward_std': 0.5233983099460602, 'kl': 0.0140380859375, 'epoch': 0.12}
  6%|▌         | 122/2088 [1:48:03<28:26:38, 52.08s/it][2025-11-07 03:58:28,560] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 123/2088 [1:48:56<28:32:28, 52.29s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.077960082814985, 'learning_rate': 9.410919540229885e-07, 'completion_length': 226.26953125, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.43306438624858856, 'reward': 1.733845591545105, 'reward_std': 0.5665106177330017, 'kl': 0.015899658203125, 'epoch': 0.12}
  6%|▌         | 123/2088 [1:48:56<28:32:28, 52.29s/it][2025-11-07 03:59:20,472] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 124/2088 [1:49:48<28:27:53, 52.18s/it]                                                       {'loss': 0.0005, 'grad_norm': 2.0281556286309375, 'learning_rate': 9.406130268199234e-07, 'completion_length': 227.58203125, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.4312763512134552, 'reward': 1.5640888214111328, 'reward_std': 0.5207845568656921, 'kl': 0.012908935546875, 'epoch': 0.12}
  6%|▌         | 124/2088 [1:49:48<28:27:53, 52.18s/it][2025-11-07 04:00:08,295] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 125/2088 [1:50:35<27:44:17, 50.87s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.18943382086853, 'learning_rate': 9.401340996168582e-07, 'completion_length': 221.30859375, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.40496961772441864, 'reward': 1.623719573020935, 'reward_std': 0.5859535932540894, 'kl': 0.0137939453125, 'epoch': 0.12}
  6%|▌         | 125/2088 [1:50:35<27:44:17, 50.87s/it][2025-11-07 04:00:58,494] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 126/2088 [1:51:26<27:36:52, 50.67s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0575496883381845, 'learning_rate': 9.396551724137931e-07, 'completion_length': 223.90234375, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.34375, 'rewards/think_step_av_with_neutral_reward': 0.3700619488954544, 'reward': 1.4091244339942932, 'reward_std': 0.5889256298542023, 'kl': 0.0157470703125, 'epoch': 0.12}
  6%|▌         | 126/2088 [1:51:26<27:36:52, 50.67s/it][2025-11-07 04:01:52,337] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 127/2088 [1:52:19<28:07:09, 51.62s/it]                                                       {'loss': 0.0006, 'grad_norm': 1.9684797352098826, 'learning_rate': 9.391762452107279e-07, 'completion_length': 232.74609375, 'rewards/av_format_reward': 0.89453125, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.3273559957742691, 'reward': 1.6554810404777527, 'reward_std': 0.38397158682346344, 'kl': 0.01446533203125, 'epoch': 0.12}
  6%|▌         | 127/2088 [1:52:19<28:07:09, 51.62s/it][2025-11-07 04:02:46,823] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 128/2088 [1:53:14<28:34:21, 52.48s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0587578142672003, 'learning_rate': 9.386973180076628e-07, 'completion_length': 227.60546875, 'rewards/av_format_reward': 0.88671875, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.41438841819763184, 'reward': 1.7932946681976318, 'reward_std': 0.5542452037334442, 'kl': 0.014373779296875, 'epoch': 0.12}
  6%|▌         | 128/2088 [1:53:14<28:34:21, 52.48s/it][2025-11-07 04:03:34,479] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 129/2088 [1:54:02<27:46:13, 51.03s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0505110368545867, 'learning_rate': 9.382183908045976e-07, 'completion_length': 218.8515625, 'rewards/av_format_reward': 0.55078125, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.4409105181694031, 'reward': 1.448723018169403, 'reward_std': 0.6282270550727844, 'kl': 0.0147705078125, 'epoch': 0.12}
  6%|▌         | 129/2088 [1:54:02<27:46:13, 51.03s/it][2025-11-07 04:04:28,332] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 130/2088 [1:54:55<28:12:59, 51.88s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0363288364356205, 'learning_rate': 9.377394636015326e-07, 'completion_length': 232.81640625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4954555332660675, 'reward': 1.8938931226730347, 'reward_std': 0.50201316177845, 'kl': 0.014617919921875, 'epoch': 0.12}
  6%|▌         | 130/2088 [1:54:55<28:12:59, 51.88s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05773.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07860.mp4
[DEBUG] Step 130
[DEBUG] prompt_completion_ids shape: torch.Size([32, 316])
[DEBUG] Shortest output (len=183): <vis_desc>In the video, the screen shows a woman sitting in an indoor environment. In the video, her mouth is open and her facial expression appears surprised, indicating that she may have heard something unexpected.</vis_desc>
<aud_desc>In the audio, the tone of the character is heavy and emotional. In the text, the subtitle reads: "Everyone else was dressed, and I was talking about my lucky number." This sentence may be the woman expressing her surprise and confusion about something.</aud_desc>
<think>Based on the woman's surprised facial expression and open mouth in the video, as well as the description of the character's heavy and emotional tone in the audio, we can infer that this sentence expresses the woman's feelings of surprise and confusion. The woman may have heard about the other person's lucky number and found it surprising and peculiar.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=316): <vis_desc>In the video, in the opening scene, we see a woman with a furrowed brow and slightly downturned mouth, her facial expression showing some anger and negativity. Her gaze is directly at the other person, with her mouth open, seemingly emphasizing a point or expressing dissatisfaction. In the final scene, the other person's expression becomes more agitated, with their facial muscles tense, especially prominent forehead lines, and we can sense the woman's body tremors, indicating their heated argument. Taking all these visuals into account, one can feel the woman's anger and negative emotions.</vis_desc>
<aud_desc>In the audio, the voice is sharp and loud, with a stern and aggressive tone. Combined with the text content, the character expresses dissatisfaction and anger. The subtitle in the text says, "Children are so timid, what will happen when they encounter difficulty? This way, the coat on the dog has been transferred to someone else." This sentence expresses the woman's discontent and anger towards children encountering difficulties.</aud_desc>
<think>Based on the angry and negative facial expression of the woman in the video clues, as well as the direct gaze at the other person and the action of opening her mouth, it can be inferred that her emotions are heightened. In the audio, her voice is sharp and loud, with a stern and aggressive tone, and the character expresses dissatisfaction and anger. Overall, the visual and auditory cues strongly indicate that the woman is dissatisfied and angry with a negative emotional state.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/06995.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07583.mp4
[DEBUG] Step 130
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=175): <vis_desc>In the video, we see a male character with a focused and somewhat angry expression. His mouth moves a lot, and his gaze constantly scans the other person, indicating a tense conversation.</vis_desc>
<aud_desc>In the audio, the speech is fast-paced and carries a sense of emphasis. In the text, the subtitle says, "What's going on? Why are you knowing me?" This sentence may be the male character's questioning or complaint to the other person.</aud_desc>
<think>Based on the focused and angry expression of the male character in the video, as well as the fast-paced and emphasis on words in the audio, we can infer that this sentence carries a sense of anger and dissatisfaction. The male character may feel ignored or unappreciated, hence expressing his emotions.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=295): <vis_desc>In the video, the screen shows a male character. In the video, he has a frown, drooping eyes, and slightly downturned corners of the mouth, which are obvious signs of feeling upset or painful. His gaze is not focused on the camera, suggesting that he may be contemplating something. At the end of the video, he opens his mouth, seemingly speaking loudly, which could indicate that he is expressing his emotions or has received sad news.</vis_desc>
<aud_desc>In the audio, the volume is stronger, the tone carries a sense of scream, and the speech is very short and excited. Combined with the text content, it seems to carry a tone of complaining and expressing one's feelings. In the text, the subtitle reads: "Oh my, the water is fresh, the water is dewy, the water is crystal-clear." This sentence may be the male character's reaction to environmental conditions or an unexpected event.</aud_desc>
<think>Based on the visual clues of the male character's frown, drooping eyes, and downturned corners of the mouth, as well as the audio clue of the strong volume, scream tone, and excited speech, we can infer that this sentence may carry a sense of sadness and sorrow. The male character may be complaining about the environment or expressing his feelings when encountering something unpleasant, which aligns with his overall display of emotions.</think>
<answer>sad</answer>
[2025-11-07 04:05:17,887] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 131/2088 [1:55:45<27:49:23, 51.18s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0990639941322122, 'learning_rate': 9.372605363984674e-07, 'completion_length': 223.08984375, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.4645135998725891, 'reward': 1.8238885998725891, 'reward_std': 0.49249720573425293, 'kl': 0.01544189453125, 'epoch': 0.13}
  6%|▋         | 131/2088 [1:55:45<27:49:23, 51.18s/it][2025-11-07 04:06:08,539] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 132/2088 [1:56:36<27:43:20, 51.02s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.1292643756306524, 'learning_rate': 9.367816091954023e-07, 'completion_length': 233.63671875, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.39394980669021606, 'reward': 1.6244186162948608, 'reward_std': 0.3913283348083496, 'kl': 0.014404296875, 'epoch': 0.13}
  6%|▋         | 132/2088 [1:56:36<27:43:20, 51.02s/it][2025-11-07 04:07:00,422] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 133/2088 [1:57:28<27:50:57, 51.28s/it]                                                       {'loss': 0.0006, 'grad_norm': 1.9590401213600495, 'learning_rate': 9.363026819923371e-07, 'completion_length': 225.3515625, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.37478862702846527, 'reward': 1.4841636419296265, 'reward_std': 0.5278185307979584, 'kl': 0.0145263671875, 'epoch': 0.13}
  6%|▋         | 133/2088 [1:57:28<27:50:57, 51.28s/it][2025-11-07 04:07:52,502] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 134/2088 [1:58:20<27:57:50, 51.52s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.140133731855305, 'learning_rate': 9.35823754789272e-07, 'completion_length': 227.5703125, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.4222432076931, 'reward': 1.5628682374954224, 'reward_std': 0.46549467742443085, 'kl': 0.01519775390625, 'epoch': 0.13}
  6%|▋         | 134/2088 [1:58:20<27:57:50, 51.52s/it][2025-11-07 04:08:43,552] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 135/2088 [1:59:11<27:52:23, 51.38s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0922380875353848, 'learning_rate': 9.353448275862068e-07, 'completion_length': 234.53515625, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.64453125, 'rewards/think_step_av_with_neutral_reward': 0.5209574401378632, 'reward': 1.7904887795448303, 'reward_std': 0.5378565192222595, 'kl': 0.014892578125, 'epoch': 0.13}
  6%|▋         | 135/2088 [1:59:11<27:52:23, 51.38s/it][2025-11-07 04:09:35,005] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 136/2088 [2:00:02<27:52:14, 51.40s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.111036344049491, 'learning_rate': 9.348659003831417e-07, 'completion_length': 225.5625, 'rewards/av_format_reward': 0.83203125, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.44146035611629486, 'reward': 1.851616621017456, 'reward_std': 0.565987229347229, 'kl': 0.0157470703125, 'epoch': 0.13}
  7%|▋         | 136/2088 [2:00:02<27:52:14, 51.40s/it][2025-11-07 04:10:23,410] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 137/2088 [2:00:50<27:22:10, 50.50s/it]                                                       {'loss': 0.0006, 'grad_norm': 1.917138283053987, 'learning_rate': 9.343869731800765e-07, 'completion_length': 227.1640625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4378501772880554, 'reward': 1.7816002368927002, 'reward_std': 0.37165413796901703, 'kl': 0.014495849609375, 'epoch': 0.13}
  7%|▋         | 137/2088 [2:00:50<27:22:10, 50.50s/it][2025-11-07 04:11:12,580] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 138/2088 [2:01:40<27:08:19, 50.10s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.156465184671387, 'learning_rate': 9.339080459770115e-07, 'completion_length': 224.33984375, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4431881457567215, 'reward': 1.740063190460205, 'reward_std': 0.5580701529979706, 'kl': 0.01507568359375, 'epoch': 0.13}
  7%|▋         | 138/2088 [2:01:40<27:08:19, 50.10s/it][2025-11-07 04:12:06,896] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 139/2088 [2:02:34<27:48:35, 51.37s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.099704875297556, 'learning_rate': 9.334291187739464e-07, 'completion_length': 232.98828125, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.48994800448417664, 'reward': 1.767291784286499, 'reward_std': 0.5989204049110413, 'kl': 0.0155029296875, 'epoch': 0.13}
  7%|▋         | 139/2088 [2:02:34<27:48:35, 51.37s/it][2025-11-07 04:12:58,891] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 140/2088 [2:03:26<27:53:48, 51.55s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0342447616792025, 'learning_rate': 9.329501915708812e-07, 'completion_length': 222.2578125, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.43946580588817596, 'reward': 1.7871220707893372, 'reward_std': 0.4446461498737335, 'kl': 0.01617431640625, 'epoch': 0.13}
  7%|▋         | 140/2088 [2:03:26<27:53:48, 51.55s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12675.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01036.mp4
[DEBUG] Step 140
[DEBUG] prompt_completion_ids shape: torch.Size([32, 280])
[DEBUG] Shortest output (len=155): <vis_desc>In the video, we see a male character with a neutral facial expression, his eyes looking directly at the other person, and his overall expression appearing calm.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The character's direct eye contact and calm expression suggest a neutral emotional state, without strong positive or negative indicators. The auditory cues of ambiguous vocal tone and subtitle content do not provide additional emotional context. Together, the visual openness, combined with the absence of emotive vocal signals, supports the conclusion that the character's emotional state is neutral.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=280): <vis_desc>In the video, we see a character with furrowed brows and slightly downturned mouth, his facial expression being fierce. He stares directly at someone, with his body angled upwards, seemingly shouting or shouting loudly. His eyes are filled with sweat, indicating that he may be extremely angry or emotionally excited.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The character’s facial expression, marked by furrowed brows and downturned mouth, conveys a strong sense of anger or heightened emotion, which are key behavioral indicators of this emotional state. His fierce gaze and direct eye contact suggests that he is engaged in a heated argument or is determined to have an important conversation with someone. The fact that he is shouting or shouting loudly adds an intense vocal expression that further emphasizes or expresses a particularly charged emotional state. Although the auditory cues and subtitle content do not provide clear visual indicators of anger, the powerful visual signals—his facial expression, body language, and facial coloring—combined suggest that he is truly experiencing anger. Together, these multimodal observations support the conclusion that the character is effectively projecting anger through his facial expression, even in the absence of supportive auditory evidence.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/180.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09839.mp4
[DEBUG] Step 140
[DEBUG] prompt_completion_ids shape: torch.Size([32, 287])
[DEBUG] Shortest output (len=171): <vis_desc>In the video, we see a woman with a slightly widened eye opening, wide-open mouth, and slightly tilted head, which may indicate that she is experiencing surprise or confusion.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The visual cues strongly suggest that the woman is experiencing surprise: her wide-open eyes, mouth, and tilted head indicate an unexpected or sudden reaction. Although the auditory information and subtitle content do not provide clear emotional indicators, the clear and recognizable visual signals play a crucial role in determining the target emotion. Together, these multimodal observations lead to a confident inference that the woman is feeling surprise.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=287): <vis_desc>In the video, the screen shows a male character in a city environment at night. In the video, his facial expression is somewhat fierce, with an apparent increase in Blood Pressure Index, suggesting that he is experiencing an intense emotional state. His mouth is open, and his gaze is fixed on someone or something outside the frame, indicating that there is a conflict or an argument taking place. The specific emotions of the character need to be analyzed in conjunction with additional context.</vis_desc>
<aud_desc>In the audio, the volume is very loud and the tone is intense, with noticeable squints of the eyes and a stern facial expression, indicating that the character's emotions are extremely excited. In the text, the subtitle says, "A good spirit of Christmas!" This sentence may be the male character's evaluation or reaction to something.</aud_desc>
<think>Based on the male character's fierce facial expression, increased Blood Pressure Index, and open mouth and gaze towards someone or something outside the frame, it can be inferred that his emotions are extremely excited. Additionally, the description of the audio volume, loudness, and intense tone with pronounced squints of the eyes and a stern facial expression further supports this inference.Therefore, this sentence may carry a sense of anger, dissatisfaction, or stern warning, aligning with the overall display of excitement by the male character.</think>
<answer>angry</answer>
[2025-11-07 04:13:51,381] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 141/2088 [2:04:18<28:02:04, 51.84s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.020687904415166, 'learning_rate': 9.324712643678161e-07, 'completion_length': 228.9765625, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.421207070350647, 'reward': 1.772769570350647, 'reward_std': 0.4433775991201401, 'kl': 0.01556396484375, 'epoch': 0.14}
  7%|▋         | 141/2088 [2:04:18<28:02:04, 51.84s/it][2025-11-07 04:14:40,211] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 142/2088 [2:05:07<27:31:57, 50.93s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.1037159904525433, 'learning_rate': 9.319923371647508e-07, 'completion_length': 228.80078125, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.47496210038661957, 'reward': 1.7562121152877808, 'reward_std': 0.5096988677978516, 'kl': 0.0152587890625, 'epoch': 0.14}
  7%|▋         | 142/2088 [2:05:07<27:31:57, 50.93s/it][2025-11-07 04:15:36,154] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 143/2088 [2:06:03<28:19:49, 52.44s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.1411227159021577, 'learning_rate': 9.315134099616858e-07, 'completion_length': 230.81640625, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.40614134073257446, 'reward': 1.7264539003372192, 'reward_std': 0.4259498119354248, 'kl': 0.01617431640625, 'epoch': 0.14}
  7%|▋         | 143/2088 [2:06:03<28:19:49, 52.44s/it][2025-11-07 04:16:28,291] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 144/2088 [2:06:55<28:16:02, 52.35s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0073142104219963, 'learning_rate': 9.310344827586206e-07, 'completion_length': 230.421875, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.47998273372650146, 'reward': 1.9018577933311462, 'reward_std': 0.5158168226480484, 'kl': 0.016021728515625, 'epoch': 0.14}
  7%|▋         | 144/2088 [2:06:55<28:16:02, 52.35s/it][2025-11-07 04:17:17,705] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 145/2088 [2:07:45<27:46:40, 51.47s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0144094125230376, 'learning_rate': 9.305555555555555e-07, 'completion_length': 227.87890625, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.44425690174102783, 'reward': 1.7294131517410278, 'reward_std': 0.509996086359024, 'kl': 0.015899658203125, 'epoch': 0.14}
  7%|▋         | 145/2088 [2:07:45<27:46:40, 51.47s/it][2025-11-07 04:18:10,112] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 146/2088 [2:08:37<27:54:56, 51.75s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.060744292423622, 'learning_rate': 9.300766283524904e-07, 'completion_length': 230.5703125, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.47563859820365906, 'reward': 1.7412636876106262, 'reward_std': 0.6244021654129028, 'kl': 0.01580810546875, 'epoch': 0.14}
  7%|▋         | 146/2088 [2:08:37<27:54:56, 51.75s/it][2025-11-07 04:19:03,244] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 147/2088 [2:09:30<28:07:30, 52.16s/it]                                                       {'loss': 0.0006, 'grad_norm': 1.9506472423367534, 'learning_rate': 9.295977011494253e-07, 'completion_length': 220.47265625, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.3984375, 'rewards/think_step_av_with_neutral_reward': 0.35320859402418137, 'reward': 1.513364851474762, 'reward_std': 0.36851951479911804, 'kl': 0.01611328125, 'epoch': 0.14}
  7%|▋         | 147/2088 [2:09:30<28:07:30, 52.16s/it][2025-11-07 04:19:55,140] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 148/2088 [2:10:22<28:04:01, 52.08s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0552976306983095, 'learning_rate': 9.291187739463601e-07, 'completion_length': 226.2109375, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.3809199631214142, 'reward': 1.4785761833190918, 'reward_std': 0.5709177851676941, 'kl': 0.01568603515625, 'epoch': 0.14}
  7%|▋         | 148/2088 [2:10:22<28:04:01, 52.08s/it][2025-11-07 04:20:46,003] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 149/2088 [2:11:13<27:51:20, 51.72s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0895151096846263, 'learning_rate': 9.28639846743295e-07, 'completion_length': 225.45703125, 'rewards/av_format_reward': 0.52734375, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.4775276482105255, 'reward': 1.512683928012848, 'reward_std': 0.6623648107051849, 'kl': 0.01556396484375, 'epoch': 0.14}
  7%|▋         | 149/2088 [2:11:13<27:51:20, 51.72s/it][2025-11-07 04:21:38,708] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 150/2088 [2:12:06<28:00:01, 52.01s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9930710543913284, 'learning_rate': 9.281609195402297e-07, 'completion_length': 229.07421875, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.4660547971725464, 'reward': 1.9113672971725464, 'reward_std': 0.46490177512168884, 'kl': 0.0164794921875, 'epoch': 0.14}
  7%|▋         | 150/2088 [2:12:06<28:00:01, 52.01s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07600.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00687.mp4
[DEBUG] Step 150
[DEBUG] prompt_completion_ids shape: torch.Size([32, 269])
[DEBUG] Shortest output (len=149): <vis_desc>In the video, we see a young male with a furrowed brow and slightly widened eyes, seemingly expressing surprise or anger. His gestures appear confident, indicating that he may be emphasizing or defending a point.</vis_desc>
<aud_desc>Although the language is difficult to determine, the tone and intonation are quite strong, suggesting that the character is quite excited. When expressing the phrase "Have you promised?", the volume in volume and intonation are quite loud, emphasizing the character's voice.</aud_desc>
<think>The overall emotional state is one of anger and excitement, as the character expresses a state of surprise and disbelief through his facial expression and tone of voice.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=269): <vis_desc>In the video, the screen shows a female character seated at a table with a brick wall in the background. She is wearing a white cardigan and is having a conversation with two male characters on either side of the frame. In the video, her mouth is moving, seemingly emphasizing a certain point or story. Over time, her expression becomes more angry, with a furrowed brow and outstretched hands, indicating a strong emotional reaction.</vis_desc>
<aud_desc>In the audio, the volume is loud and the tone is excited and joyful, suggesting that the character's mood is very positive and joyful. In the text, the subtitle says, "What I want is future I want, really amazing, it can be learned from the recommendations." This sentence may express the female character's hopes or satisfaction.</aud_desc>
<think>Based on the audio clue of the character's excited and joyful tone, as well as the video clue of the female character's angry expression and strong emotional reaction, we can infer that this sentence is expressed in a positive and joyful tone. The female character may highlight the "strong recommendation" she has for the item, which made her feel great. Therefore, this sentence expresses the female character's happiness and satisfaction with her current situation.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01943.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7380.mp4
[DEBUG] Step 150
[DEBUG] prompt_completion_ids shape: torch.Size([32, 291])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned corners of the mouth, tears streaming down her face. Her gaze is fixed on a certain place, appearing somewhat surprised.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very surprised when saying "He wasn't wearing a mask." It can be inferred that she is surprised by this information.</aud_desc>
<think>Given the visual cues of the female character's furrowed brows, downturned corners of the mouth, and tears streaming down her face, as well as the audio clue of the character's surprised tone, we can deduce that this information may be unexpected and disturbing to the character. The overall emotional state of the female character is one of surprise and confusion.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=291): <vis_desc>In the video, the screen shows a female character in a dim environment, possibly a room or a corridor. In the video, her eyebrows are slightly raised, with a furrowed brow, and her facial expression appears somewhat scared. Her gaze is fixed on a certain place, as if something or someone is threatening her. Her mouth moves slightly, indicating that she is speaking, possibly expressing her fear or asking questions.</vis_desc>
<aud_desc>In the audio, the character's tone is low and there are obvious sighs during conversation. It can be inferred that the character is in a state of discomfort or fear. In the text, the subtitle reads: "He wasn't wearing a mask, it didn'texist, he wasn't infected, so why did it happen?" This sentence may indicate the female character's confusion and suspicion.</aud_desc>
<think>This sentence may be the female character's questioning or suspicion towards a certain situation or event. Based on the video clues of the female character's raised eyebrows, furrowed brow, and scared facial expression, as well as the audio clue of the character's low tone and sighs, we can infer that this sentence may carry a state of discomfort or fear. The female character may be questioning or suspicion towards a certain situation or event, while expressing "why did it happen?" This sentence may reflect her uncertainty and dissatisfaction.</think>
<answer>fear</answer>
[2025-11-07 04:22:29,532] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 151/2088 [2:12:57<27:47:39, 51.66s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0497386826569173, 'learning_rate': 9.276819923371647e-07, 'completion_length': 224.21875, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.38608306646347046, 'reward': 1.6673331260681152, 'reward_std': 0.5191669762134552, 'kl': 0.01763916015625, 'epoch': 0.14}
  7%|▋         | 151/2088 [2:12:57<27:47:39, 51.66s/it][2025-11-07 04:23:20,668] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 152/2088 [2:13:48<27:41:45, 51.50s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.082009054520336, 'learning_rate': 9.272030651340996e-07, 'completion_length': 224.40234375, 'rewards/av_format_reward': 0.6484375, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.42370130121707916, 'reward': 1.568232536315918, 'reward_std': 0.6075595617294312, 'kl': 0.01629638671875, 'epoch': 0.15}
  7%|▋         | 152/2088 [2:13:48<27:41:45, 51.50s/it][2025-11-07 04:24:15,847] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 153/2088 [2:14:43<28:16:29, 52.60s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.001898170925346, 'learning_rate': 9.267241379310344e-07, 'completion_length': 228.29296875, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.5295812487602234, 'reward': 1.9905188083648682, 'reward_std': 0.5346010476350784, 'kl': 0.01641845703125, 'epoch': 0.15}
  7%|▋         | 153/2088 [2:14:43<28:16:29, 52.60s/it][2025-11-07 04:25:08,862] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 154/2088 [2:15:36<28:19:33, 52.73s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.132003063301069, 'learning_rate': 9.262452107279694e-07, 'completion_length': 234.28125, 'rewards/av_format_reward': 0.58203125, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.41236844658851624, 'reward': 1.4670559763908386, 'reward_std': 0.6444506645202637, 'kl': 0.01702880859375, 'epoch': 0.15}
  7%|▋         | 154/2088 [2:15:36<28:19:33, 52.73s/it][2025-11-07 04:26:01,873] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 155/2088 [2:16:29<28:21:26, 52.81s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.105494475083431, 'learning_rate': 9.257662835249042e-07, 'completion_length': 230.49609375, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.46484375, 'rewards/think_step_av_with_neutral_reward': 0.4245835840702057, 'reward': 1.701927363872528, 'reward_std': 0.5764710605144501, 'kl': 0.015716552734375, 'epoch': 0.15}
  7%|▋         | 155/2088 [2:16:29<28:21:26, 52.81s/it][2025-11-07 04:26:52,485] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 156/2088 [2:17:20<27:59:17, 52.15s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.11443542179621, 'learning_rate': 9.252873563218391e-07, 'completion_length': 227.37890625, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.48443712294101715, 'reward': 1.7891247272491455, 'reward_std': 0.5707069635391235, 'kl': 0.01678466796875, 'epoch': 0.15}
  7%|▋         | 156/2088 [2:17:20<27:59:17, 52.15s/it][2025-11-07 04:27:41,388] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 157/2088 [2:18:08<27:27:03, 51.18s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1191140156103234, 'learning_rate': 9.248084291187739e-07, 'completion_length': 223.41796875, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5181253254413605, 'reward': 2.045469105243683, 'reward_std': 0.4975837171077728, 'kl': 0.01654052734375, 'epoch': 0.15}
  8%|▊         | 157/2088 [2:18:08<27:27:03, 51.18s/it][2025-11-07 04:28:34,233] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 158/2088 [2:19:01<27:42:18, 51.68s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.967087823226915, 'learning_rate': 9.243295019157087e-07, 'completion_length': 231.91015625, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.431394025683403, 'reward': 1.7087377905845642, 'reward_std': 0.42384470999240875, 'kl': 0.0233154296875, 'epoch': 0.15}
  8%|▊         | 158/2088 [2:19:01<27:42:18, 51.68s/it][2025-11-07 04:29:24,959] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 159/2088 [2:19:52<27:32:15, 51.39s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9457118956271038, 'learning_rate': 9.238505747126436e-07, 'completion_length': 228.390625, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.4140625, 'rewards/think_step_av_with_neutral_reward': 0.3383492976427078, 'reward': 1.4750680923461914, 'reward_std': 0.5246672630310059, 'kl': 0.01727294921875, 'epoch': 0.15}
  8%|▊         | 159/2088 [2:19:52<27:32:15, 51.39s/it][aac @ 0x88935080] channel element 1.12 is not allocated
[04:29:52] /github/workspace/src/audio/audio_reader.cc:189: ERROR Fail to send packet.
[2025-11-07 04:30:18,567] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 160/2088 [2:20:46<27:52:46, 52.06s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9569662643360568, 'learning_rate': 9.233716475095785e-07, 'completion_length': 225.28125, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.5510444641113281, 'reward': 1.8010445833206177, 'reward_std': 0.5350656807422638, 'kl': 0.01739501953125, 'epoch': 0.15}
  8%|▊         | 160/2088 [2:20:46<27:52:46, 52.06s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03636.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/10509.mp4
[DEBUG] Step 160
[DEBUG] prompt_completion_ids shape: torch.Size([32, 319])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a woman with a furrowed brow and an angry facial expression. Her mouth is open and her gaze is fixed on the person she is talking to.</vis_desc>
<aud_desc>In the audio, the tone is aggressive and the character's emotions are more excited. Combined with the text content, the character is expressing dissatisfaction and anger. The subtitle in the text says, "He would still be illegally reading our texts." This sentence expresses the woman's dissatisfaction and anger toward someone.</aud_desc>
<think>Based on the aggressive tone and excited emotions of the character in the audio, as well as the angry facial expression and open mouth in the video, we can infer that this sentence is the woman expressing her dissatisfaction and anger towards a certain behavior or individual.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=319): <vis_desc>In the video, the scene shows a male lying on a bed in a dimly lit bedroom. At the beginning of the video, he leans back his head and slowly turns over on the bed, while moving his body and supporting his head with his hands. Through these movements, he is likely trying to get comfortable or prepare for sleep. At the same time, the slight increase in blood pressure indicates that he is likely to be excited or surprised.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the speaker's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The male's visual cues strongly suggest a state of surprise. His initial action of leaning back his head and turning over on the bed indicates a transition from a calm to an unexpected posture, while his body movements and support for his head conveys a heightened level of involvement and alertness. The slow but steady pace of head turns combined with subtle increases in blood pressure reinforces the impression that his body is responding to an unexpected stimulus, collectively supporting the interpretation that he is experiencing surprise. Although the auditory information and subtitle content do not provide additional emotional context, the consistent and clear visual signals of surprise reliably convey that emotion in the face and body. Thus, integrating the multimodal evidence, the confident upward movement of the male's blood pressure during turns, and the focused expression on the target emotion, all converge to confidently support the prediction of surprise as the target emotion.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03114.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02087.mp4
[DEBUG] Step 160
[DEBUG] prompt_completion_ids shape: torch.Size([32, 277])
[DEBUG] Shortest output (len=165): <vis_desc>In the video, we see a man in an indoor setting. His facial expression is neutral, his eyes are looking directly at the other person, and his mouth is moving slightly.</vis_desc>
<aud_desc>In the audio, the tone is calm and the intonation is steady. In the text, the subtitle reads: "Hello, everyone. It seems like I've been delayed." This sentence is likely the man introducing or responding to the people around him.</aud_desc>
<think>Based on these scenes, it can be predicted that the man in this video feels delayed or uneasy, but he is willing to accept it. He is willing to look responsible to the people around him, so he shows a calm and sincere emotional state throughout the video.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=277): <vis_desc>In the video, we see a woman in a bathroom. Her facial expression appears relaxed and happy, with a slight upward curve at the corners of her mouth. Her eyes seem to reflect comfort and ease, as if what she is about to see or hear interests her or makes her feel happy. She seems to be enjoying this moment or getting ready for a pleasant conversation.</vis_desc>
<aud_desc>In the audio, the tone is calm and conveys a sense of happiness and anticipation. In the text, the subtitle reads, "It's almost noon." This sentence may be the woman's expression of time.</aud_desc>
<think>Based on the relaxed and happy facial expression of the woman in the video clues, as well as the calm tone in the audio clues, we can infer that this sentence may have a relaxed and happy tone. The woman may be enjoying a pleasant conversation or anticipation for a pleasant event, so she expressed this sentence. Due to the low video quality and dialogue speed, it is difficult to determine the specific emotions behind this sentence. However, based on the overall relaxed and happy expression of the woman, as well as the calm tone in the audio clues, we can infer that this sentence is likely a moment of happiness or anticipation, and the woman's overall mood is relaxed and happy.</think>
<answer>happy</answer>
[2025-11-07 04:31:04,852] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 161/2088 [2:21:32<26:56:17, 50.33s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.039208272860442, 'learning_rate': 9.228927203065133e-07, 'completion_length': 215.34375, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.64453125, 'rewards/think_step_av_with_neutral_reward': 0.5293045938014984, 'reward': 2.017585813999176, 'reward_std': 0.49006474018096924, 'kl': 0.0174560546875, 'epoch': 0.15}
  8%|▊         | 161/2088 [2:21:32<26:56:17, 50.33s/it][2025-11-07 04:31:58,309] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 162/2088 [2:22:25<27:25:36, 51.26s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0274612808922883, 'learning_rate': 9.224137931034482e-07, 'completion_length': 222.625, 'rewards/av_format_reward': 0.63671875, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.4665883183479309, 'reward': 1.579869568347931, 'reward_std': 0.6935807168483734, 'kl': 0.01702880859375, 'epoch': 0.16}
  8%|▊         | 162/2088 [2:22:25<27:25:36, 51.26s/it][2025-11-07 04:32:50,681] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 163/2088 [2:23:18<27:35:24, 51.60s/it]                                                       {'loss': 0.0006, 'grad_norm': 2.0278554856593707, 'learning_rate': 9.219348659003831e-07, 'completion_length': 229.4765625, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.5494808256626129, 'reward': 1.78385591506958, 'reward_std': 0.4853277951478958, 'kl': 0.0162353515625, 'epoch': 0.16}
  8%|▊         | 163/2088 [2:23:18<27:35:24, 51.60s/it][2025-11-07 04:33:42,834] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 164/2088 [2:24:10<27:39:53, 51.76s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9624498855458776, 'learning_rate': 9.21455938697318e-07, 'completion_length': 227.7265625, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4964626729488373, 'reward': 1.8284938335418701, 'reward_std': 0.5618415176868439, 'kl': 0.0177001953125, 'epoch': 0.16}
  8%|▊         | 164/2088 [2:24:10<27:39:53, 51.76s/it][2025-11-07 04:34:31,942] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 165/2088 [2:24:59<27:13:29, 50.97s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1481822243509345, 'learning_rate': 9.209770114942529e-07, 'completion_length': 224.015625, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.395244300365448, 'reward': 1.574931800365448, 'reward_std': 0.5221997201442719, 'kl': 0.01873779296875, 'epoch': 0.16}
  8%|▊         | 165/2088 [2:24:59<27:13:29, 50.97s/it][2025-11-07 04:35:22,394] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 166/2088 [2:25:49<27:07:43, 50.81s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0507709461476047, 'learning_rate': 9.204980842911876e-07, 'completion_length': 223.5390625, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.4828789532184601, 'reward': 1.7289727330207825, 'reward_std': 0.580791711807251, 'kl': 0.01812744140625, 'epoch': 0.16}
  8%|▊         | 166/2088 [2:25:49<27:07:43, 50.81s/it][2025-11-07 04:36:12,825] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 167/2088 [2:26:40<27:03:11, 50.70s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.024408247973401, 'learning_rate': 9.200191570881226e-07, 'completion_length': 231.796875, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.4836898744106293, 'reward': 1.7961899042129517, 'reward_std': 0.542150616645813, 'kl': 0.0181884765625, 'epoch': 0.16}
  8%|▊         | 167/2088 [2:26:40<27:03:11, 50.70s/it][2025-11-07 04:37:01,856] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 168/2088 [2:27:29<26:46:20, 50.20s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0243960773179768, 'learning_rate': 9.195402298850574e-07, 'completion_length': 223.85546875, 'rewards/av_format_reward': 0.91796875, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5119560807943344, 'reward': 2.0041435956954956, 'reward_std': 0.5013185888528824, 'kl': 0.01806640625, 'epoch': 0.16}
  8%|▊         | 168/2088 [2:27:29<26:46:20, 50.20s/it][2025-11-07 04:37:54,866] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 169/2088 [2:28:22<27:12:28, 51.04s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.088298042002998, 'learning_rate': 9.190613026819923e-07, 'completion_length': 234.640625, 'rewards/av_format_reward': 0.5546875, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4772326350212097, 'reward': 1.5905138850212097, 'reward_std': 0.5555526614189148, 'kl': 0.01800537109375, 'epoch': 0.16}
  8%|▊         | 169/2088 [2:28:22<27:12:28, 51.04s/it][mpeg4 @ 0xaadbd1c0] slice end not reached but screenspace end (3 left C00000, score= -30)
[2025-11-07 04:38:43,661] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 170/2088 [2:29:11<26:50:04, 50.37s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.8991216419194672, 'learning_rate': 9.185823754789271e-07, 'completion_length': 220.51953125, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.359375, 'rewards/think_step_av_with_neutral_reward': 0.32069607079029083, 'reward': 1.5902272462844849, 'reward_std': 0.3842937499284744, 'kl': 0.018310546875, 'epoch': 0.16}
  8%|▊         | 170/2088 [2:29:11<26:50:04, 50.37s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/6357.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12157.mp4
[DEBUG] Step 170
[DEBUG] prompt_completion_ids shape: torch.Size([32, 299])
[DEBUG] Shortest output (len=172): <vis_desc>In the video, we see a male character using a cellphone to make a call outdoors. He is holding the phone in both hands and seems to be concentrating on the conversation.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle says, "I have no time." This sentence may be spoken by the male character while making the phone call.</aud_desc>
<think>Based on the video clue of the male character using a cellphone to make a call outdoors and concentrating on the conversation, as well as the audio clue of a neutral tone, we can infer that this sentence may carry a serious or important tone. Therefore, this sentence may indicate that the male character is busy with some work or other responsibilities and does not have time to talk.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=299): <vis_desc>In the video, we see a man wearing a police uniform. His eyes are wide open and his facial expression appears somewhat surprised. He seems to be looking at something or someone on the ground, which could imply that he is seeing something bad or strange happening.</vis_desc>
<aud_desc>In the audio, there is a sense of fear in the character's voice. Even though the character's vocal tone is low, there is a noticeable increase in breathing frequency, which is usually associated with feeling scared or experiencing an intense emotional state. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Combining the visual and auditory cues gives a clear and cohesive emotional picture. The man's wide-open eyes and expression of surprise suggest an immediate, involuntary reaction to something unexpected or alarming, consistent with fear. His focused gaze on something off-screen further indicates that a threatening or distressing event has caught his attention. Complementing this, the auditory tone of fear strengthens the impression that he is feeling genuine fear rather than just responding cautiously. Even though the vocal tone is subdued and the speech slows down noticeably, the increased breathing frequency and heightened overall fear levels convey a physiological response that aligns with fear rather than calm neutrality. Together, these multimodal signals—visual surprise combined with vocal fearful tone and increased alertness—jointly support the prediction that the man is experiencing fear.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03609.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01465.mp4
[DEBUG] Step 170
[DEBUG] prompt_completion_ids shape: torch.Size([32, 313])
[DEBUG] Shortest output (len=175): <vis_desc>In the video, we see a female character with a neutral facial expression, looking attentively at the other person.</vis_desc>
<aud_desc>In the audio, the tone and intonation are relatively steady. In the text, the subtitle says, "When do I start?" This sentence may be the female character asking about the start of a certain event or task.</aud_desc>
<think>Based on the video clues of the female character's neutral facial expression and attentive gaze, as well as the audio clue of the steady tone, intonation, and the question the female character is asking about the start of a certain event, we can infer that this sentence may carry a sense of anticipation or curiosity. Therefore, this sentence expresses the female character's expectation and tentative planning for a certain action or scenario.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=313): <vis_desc>In the video, the screen shows a female character standing in a room with a historical and cultural theme. There is a United States flag and a British flag hanging on the wall. In the video, her mouth is downright, which is not proper formality. She also turns her head to speak, which is not considered formally proper either. Her eyes are looking directly at the other person, appearing serious and focused, as if she is actively listening or preparing to speak.</vis_desc>
<aud_desc>In the audio, there are consecutive laughter sounds, which are not natural speech, so it can be inferred that the character's emotions are upset and is about to slap the other person. In the text, the subtitle says, "Go on." This sentence may be a request or command from the female character to the other person.</aud_desc>
<think>Based on the facial expression of the female character in the video, with her mouth downright and her head turning to speak, which is not considered formally proper either, we can infer that she is somewhat formalistic in her behavior and might have a certain degree of impatience for modern customs. At the same time, the appearance of consecutive laughter sounds in the audio also indicates that the character's emotions are upset and is about to slap the other person. Therefore, this sentence may be a demonstration of the female character's determination and persistence, as she requests the other person to accept a proposal or decision that goes against her personal feelings.</think>
<answer>neutral</answer>
[2025-11-07 04:39:36,234] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 171/2088 [2:30:03<27:10:22, 51.03s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0573420488511185, 'learning_rate': 9.181034482758621e-07, 'completion_length': 226.8515625, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.590220719575882, 'reward': 1.9886583089828491, 'reward_std': 0.47328366339206696, 'kl': 0.017822265625, 'epoch': 0.16}
  8%|▊         | 171/2088 [2:30:03<27:10:22, 51.03s/it][2025-11-07 04:40:25,058] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 172/2088 [2:30:52<26:48:24, 50.37s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.129026244841069, 'learning_rate': 9.176245210727969e-07, 'completion_length': 231.8515625, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.35608328878879547, 'reward': 1.5904582738876343, 'reward_std': 0.5643820762634277, 'kl': 0.01666259765625, 'epoch': 0.16}
  8%|▊         | 172/2088 [2:30:52<26:48:24, 50.37s/it][2025-11-07 04:41:19,475] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 173/2088 [2:31:47<27:26:20, 51.58s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.8637499096476147, 'learning_rate': 9.171455938697318e-07, 'completion_length': 225.29296875, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.48547178506851196, 'reward': 1.7198467254638672, 'reward_std': 0.48506250977516174, 'kl': 0.018798828125, 'epoch': 0.17}
  8%|▊         | 173/2088 [2:31:47<27:26:20, 51.58s/it][2025-11-07 04:42:10,525] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 174/2088 [2:32:38<27:20:22, 51.42s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9510638256556174, 'learning_rate': 9.166666666666665e-07, 'completion_length': 236.5859375, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5893071293830872, 'reward': 2.034619629383087, 'reward_std': 0.5428325235843658, 'kl': 0.0162353515625, 'epoch': 0.17}
  8%|▊         | 174/2088 [2:32:38<27:20:22, 51.42s/it][2025-11-07 04:43:03,155] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 175/2088 [2:33:30<27:31:04, 51.78s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1013607952311055, 'learning_rate': 9.161877394636015e-07, 'completion_length': 228.625, 'rewards/av_format_reward': 0.94140625, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5265743732452393, 'reward': 2.1164180040359497, 'reward_std': 0.5511176288127899, 'kl': 0.01763916015625, 'epoch': 0.17}
  8%|▊         | 175/2088 [2:33:30<27:31:04, 51.78s/it][2025-11-07 04:43:54,018] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 176/2088 [2:34:21<27:21:26, 51.51s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0217371525410535, 'learning_rate': 9.157088122605363e-07, 'completion_length': 223.44921875, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.72265625, 'rewards/think_step_av_with_neutral_reward': 0.5964105725288391, 'reward': 2.080785572528839, 'reward_std': 0.5293715000152588, 'kl': 0.01947021484375, 'epoch': 0.17}
  8%|▊         | 176/2088 [2:34:21<27:21:26, 51.51s/it][2025-11-07 04:44:47,640] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 177/2088 [2:35:15<27:40:43, 52.14s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0863432436770935, 'learning_rate': 9.152298850574712e-07, 'completion_length': 223.0859375, 'rewards/av_format_reward': 0.5625, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.39352835714817047, 'reward': 1.3857158422470093, 'reward_std': 0.6723035275936127, 'kl': 0.027099609375, 'epoch': 0.17}
  8%|▊         | 177/2088 [2:35:15<27:40:43, 52.14s/it][2025-11-07 04:45:38,465] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 178/2088 [2:36:06<27:27:17, 51.75s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0816635385661617, 'learning_rate': 9.14750957854406e-07, 'completion_length': 222.8359375, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.3671875, 'rewards/think_step_av_with_neutral_reward': 0.3801804184913635, 'reward': 1.4739303588867188, 'reward_std': 0.5063687115907669, 'kl': 0.01849365234375, 'epoch': 0.17}
  9%|▊         | 178/2088 [2:36:06<27:27:17, 51.75s/it][2025-11-07 04:46:35,431] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 179/2088 [2:37:03<28:16:16, 53.31s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.128203491990975, 'learning_rate': 9.14272030651341e-07, 'completion_length': 236.58984375, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.46484375, 'rewards/think_step_av_with_neutral_reward': 0.43065954744815826, 'reward': 1.708003282546997, 'reward_std': 0.5943715423345566, 'kl': 0.0191650390625, 'epoch': 0.17}
  9%|▊         | 179/2088 [2:37:03<28:16:16, 53.31s/it][2025-11-07 04:47:26,483] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 180/2088 [2:37:54<27:53:45, 52.63s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1257687512321017, 'learning_rate': 9.137931034482759e-07, 'completion_length': 229.3125, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.4486766457557678, 'reward': 1.643989086151123, 'reward_std': 0.5562244355678558, 'kl': 0.01708984375, 'epoch': 0.17}
  9%|▊         | 180/2088 [2:37:54<27:53:45, 52.63s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09094.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01953.mp4
[DEBUG] Step 180
[DEBUG] prompt_completion_ids shape: torch.Size([32, 270])
[DEBUG] Shortest output (len=164): <vis_desc>In the video, we see a woman with a slightly furrowed brow and a pursed mouth, seemingly engaged in a tense conversation indoors.</vis_desc>
<aud_desc>In the audio, the tone is calm. In the text, the subtitle reads: "I have information about a closure, can you close your account?" This sentence may be a question or request from the woman to the other person.</aud_desc>
<think>Based on the woman's furrowed brow and pursed mouth in the video, as well as the calm tone in the audio, we can infer that this sentence may carry a serious or questioning tone. The woman may be asking the other person if they canclose their account because of some information about a closure.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=270): <vis_desc>In the video, the screen shows a female character. In the video, she has a smile on her face and her eyes are gently squinted, indicating that she may be experiencing an emotional joy or feeling good. Her mouth moves slightly, suggesting that she may be speaking or laughing, which further supports the idea of joy or pleasant emotion.</vis_desc>
<aud_desc>In the audio, the voice is gentle and the tone is relatively steady. Combined with the text content, it seems to carry a sense of comfort and intimacy. The subtitle in the text says, "She's too shy to admit it." This sentence may be the female character's evaluation or reaction to someone.</aud_desc>
<think>Based on the video clues of the female character smiling, her eyes gently squinted, and her mouth moving slightly, it can be inferred that she may be experiencing an emotional joy or feeling good. Additionally, based on the audio clue describing a gentle voice and steady tone, as well as the subtitle content, it can be further confirmed that the female character may have a feeling of companionship and intimacy with the person they are discussing. Therefore, this sentence expresses the female character's concern and sympathy for someone's shyness, aligning with her displayed emotional joy.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07251.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11476.mp4
[DEBUG] Step 180
[DEBUG] prompt_completion_ids shape: torch.Size([32, 326])
[DEBUG] Shortest output (len=174): <vis_desc>In the video, we see a man with a furrowed brow and slightly downturned mouth, which may indicate that he is experiencing an unpleasant emotional state.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The visual cues strongly suggest that the man is experiencing frustration or anxiety: his furrowed brow and slightly downturned mouth suggest stress or sadness, while the lack of valuable emotional clues in the audio and subtitle content means that neither supportive nor contradicting signals are available. Together, the visual indicators provide a coherent basis to infer that the man is feeling frustrated, even though the vocal information does not offer additional insight into his emotions.</think>
<answer>frustrated</answer>
[DEBUG] Longest output (len=326): <vis_desc>In the video, the screen shows a woman standing in a narrow corridor. At the beginning of the video, her facial expression is relatively relaxed. In the following scenes, she slowly moves forward while chewing her lips, which is a common facial expression for people who love to eat or need to move frequently. At the same time, she tilts her head back, furrows her brow, and slightly opens her mouth, which often indicates that she is thinking, listening, or waiting.</vis_desc>
<aud_desc>In the audio, the tone is calm and heartfelt, with a rising intonation. Overall, it can be felt that the character is trying her best to console and comfort the other person. In the text, the subtitle says, "I just have something really sad and scary." This sentence is likely what the woman is thinking and saying to herself.</aud_desc>
<think>Based on the woman's facial expressions in the video clues, she slowly moves forward while chewing her lips, which often indicates that she is thinking, listening, or waiting. At the same time, she tilts her head back, furrows her brow, and slightly opens her mouth, which often indicates that she is thinking, listening, or waiting. In the audio clues, the tone is calm and heartfelt, with a rising intonation. Overall, it can be felt that the character is trying her best to console and comfort the other person. Therefore, this sentence expresses the woman's helplessness and care for the other person, conveying a sense of responsibility and anxiety.</think>
<answer>sad</answer>
[2025-11-07 04:48:14,378] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 181/2088 [2:38:41<27:07:44, 51.21s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0558691431125458, 'learning_rate': 9.133141762452107e-07, 'completion_length': 222.46875, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.4961691200733185, 'reward': 2.003981649875641, 'reward_std': 0.585094541311264, 'kl': 0.018798828125, 'epoch': 0.17}
  9%|▊         | 181/2088 [2:38:41<27:07:44, 51.21s/it][2025-11-07 04:49:07,772] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▊         | 182/2088 [2:39:35<27:27:37, 51.87s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.1175942264865375, 'learning_rate': 9.128352490421456e-07, 'completion_length': 228.453125, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4701598882675171, 'reward': 1.8100035786628723, 'reward_std': 0.5575845539569855, 'kl': 0.01898193359375, 'epoch': 0.17}
  9%|▊         | 182/2088 [2:39:35<27:27:37, 51.87s/it][2025-11-07 04:49:58,554] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 183/2088 [2:40:26<27:16:25, 51.54s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0630344064757815, 'learning_rate': 9.123563218390804e-07, 'completion_length': 222.26171875, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.70703125, 'rewards/think_step_av_with_neutral_reward': 0.5244705528020859, 'reward': 2.0947831869125366, 'reward_std': 0.5713746547698975, 'kl': 0.01788330078125, 'epoch': 0.18}
  9%|▉         | 183/2088 [2:40:26<27:16:25, 51.54s/it][2025-11-07 04:50:46,833] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 184/2088 [2:41:14<26:44:30, 50.56s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.103690928901566, 'learning_rate': 9.118773946360153e-07, 'completion_length': 217.8828125, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.517684668302536, 'reward': 1.8809658885002136, 'reward_std': 0.607265293598175, 'kl': 0.0179443359375, 'epoch': 0.18}
  9%|▉         | 184/2088 [2:41:14<26:44:30, 50.56s/it][2025-11-07 04:51:36,244] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 185/2088 [2:42:03<26:32:42, 50.22s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0972907043465976, 'learning_rate': 9.113984674329501e-07, 'completion_length': 223.28125, 'rewards/av_format_reward': 0.62109375, 'rewards/accuracy_reward': 0.35546875, 'rewards/think_step_av_with_neutral_reward': 0.36353787779808044, 'reward': 1.3401002883911133, 'reward_std': 0.5051665902137756, 'kl': 0.0185546875, 'epoch': 0.18}
  9%|▉         | 185/2088 [2:42:03<26:32:42, 50.22s/it][2025-11-07 04:52:35,062] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 186/2088 [2:43:02<27:53:40, 52.80s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.081220955683031, 'learning_rate': 9.10919540229885e-07, 'completion_length': 237.71484375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.5213937312364578, 'reward': 1.8495187163352966, 'reward_std': 0.4582655280828476, 'kl': 0.0174560546875, 'epoch': 0.18}
  9%|▉         | 186/2088 [2:43:02<27:53:40, 52.80s/it][2025-11-07 04:53:27,588] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 187/2088 [2:43:55<27:50:13, 52.72s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1302320343345627, 'learning_rate': 9.104406130268199e-07, 'completion_length': 238.4609375, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.4397567957639694, 'reward': 1.7210068106651306, 'reward_std': 0.6123822629451752, 'kl': 0.01708984375, 'epoch': 0.18}
  9%|▉         | 187/2088 [2:43:55<27:50:13, 52.72s/it][2025-11-07 04:54:20,290] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 188/2088 [2:44:47<27:49:12, 52.71s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0616058090231397, 'learning_rate': 9.099616858237548e-07, 'completion_length': 231.484375, 'rewards/av_format_reward': 0.4765625, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.3490544408559799, 'reward': 1.2201482057571411, 'reward_std': 0.6416732966899872, 'kl': 0.01800537109375, 'epoch': 0.18}
  9%|▉         | 188/2088 [2:44:47<27:49:12, 52.71s/it][2025-11-07 04:55:11,039] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 189/2088 [2:45:38<27:29:40, 52.12s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.046522510237359, 'learning_rate': 9.094827586206896e-07, 'completion_length': 225.65625, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.40234375, 'rewards/think_step_av_with_neutral_reward': 0.41822606325149536, 'reward': 1.6760385632514954, 'reward_std': 0.5032316893339157, 'kl': 0.01812744140625, 'epoch': 0.18}
  9%|▉         | 189/2088 [2:45:38<27:29:40, 52.12s/it][2025-11-07 04:56:03,005] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 190/2088 [2:46:30<27:27:20, 52.08s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0271939629352604, 'learning_rate': 9.090038314176245e-07, 'completion_length': 227.8515625, 'rewards/av_format_reward': 0.88671875, 'rewards/accuracy_reward': 0.7421875, 'rewards/think_step_av_with_neutral_reward': 0.6383132636547089, 'reward': 2.2672195434570312, 'reward_std': 0.45572908222675323, 'kl': 0.01812744140625, 'epoch': 0.18}
  9%|▉         | 190/2088 [2:46:30<27:27:20, 52.08s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1564.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10564.mp4
[DEBUG] Step 190
[DEBUG] prompt_completion_ids shape: torch.Size([32, 289])
[DEBUG] Shortest output (len=191): <vis_desc>In the video, we see a male character with a slight upward curve at the corners of his mouth, wearing a smile, and his eyes appear to be shining, giving him a friendly and joyful impression. He appears to be engaged in a friendly conversation.</vis_desc>
<aud_desc>In the audio, the tone is positive and the emotion conveyed is satisfaction and warmth. In the text, the subtitle reads: "Is not meat? Is not tasty?" This sentence may be the male character posing a question or responding to another person.</aud_desc>
<think>Based on the friendly and joyful impression displayed by the male character in the video, as well as the positive tone described in the audio, we can infer that this sentence carries a relaxed and humorous tone. Therefore, this sentence may be an invited question or a friendly challenge, emphasizing the character's willingness to engage in the conversation.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=289): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned corners of the mouth, some bloodstains visible on her face, indicating that she may have just taken a big breath or finished something unpleasant. In the following scene, she sits still and looks directly at the camera, with a facial expression that appears somewhat surprised.</vis_desc>
<aud_desc>In the audio, there is obvious sobbing in her voice. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Although the vocal tone and subtitle content do not provide clear emotional clues, the visual cues of the female character’s furrowed brows, downturned corners of the mouth, and focused expression on the face suggest a sense of discomfort or unease that might associate with surprise or unexpectedness. The presence of bloodstains on her face further reinforces this interpretation, indicating that she may have just taken a unpleasant action, perhaps inhaling strongly or doing something frightening. Together, these visual signs—especially the tense facial expression and the presence of bloodmarks—contrindiate the typically associated emotions of surprise and suggest a more restrained or unsettling reaction. Therefore, integrating these observations, it is reasonable to conclude that the target emotional state is indeed surprise,interpreting the visual expressions as subtle but compelling indicators of this emotion.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3167.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10651.mp4
[DEBUG] Step 190
[DEBUG] prompt_completion_ids shape: torch.Size([32, 326])
[DEBUG] Shortest output (len=158): <vis_desc>In the video, we see a character wearing traditional Japanese clothing, holding a brush and a jar of oil. The character is carefully applying the oil to the brush tip, presumably to improve its performance or prevent it from drying too quickly. The environment looks like a garden or a similar natural setting, which may indicate that the character is taking care of something important or resting here.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>Combining the visual and auditory cues gives a clear and coherent picture of the character experiencing a typical traditional Japanese emotion, relaxation.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=326): <vis_desc>In the video, we see two characters sitting on a sofa, with the character on the left holding a pen. The left character is looking at the right character and seemingly expressing an opinion or suggestion to her. In the following scene, the character whose hands are no longer in the frame sits up, shows a surprised expression, and seems to be experiencing an unexpected event or conversation that happened outside the frame. At the same time, his eyes are wide open, hands are raised, palms facing outwards, and a raised hand is usually interpreted as a sign of protest, anger, or aggression.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The character on the left’s behavior, from looking serious and attentive to holding up the second character, reveals a range of emotions from concern and concern about the potential outcome to surprise and surprise about what has happened outside the frame. His wide-open eyes and the expression of palms facing outwards suggests heightened emotional distress, likely related to confusion, surprise, or uncertainty about what has happened, which could trigger unexpected feelings in the other person. Although the auditory cues and subtitle content do not provide clear emotional indicators, the visual signals—particularly the way he displays signs of anxiety, surprise, and open-handed gestures—strongly point towards an emotionally reacting state consistent with surprise. Together, these multimodal observations support the conclusion that the target emotion expressed is surprise.</think>
<answer>surprise</answer>
[2025-11-07 04:56:52,357] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 191/2088 [2:47:19<27:00:38, 51.26s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.063836372785508, 'learning_rate': 9.085249042145593e-07, 'completion_length': 222.73828125, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.3983364701271057, 'reward': 1.7069302201271057, 'reward_std': 0.5048530399799347, 'kl': 0.01934814453125, 'epoch': 0.18}
  9%|▉         | 191/2088 [2:47:19<27:00:38, 51.26s/it][2025-11-07 04:57:45,362] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 192/2088 [2:48:12<27:16:19, 51.78s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0342946263341073, 'learning_rate': 9.080459770114942e-07, 'completion_length': 230.19921875, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.422146275639534, 'reward': 1.785427451133728, 'reward_std': 0.5219680815935135, 'kl': 0.01971435546875, 'epoch': 0.18}
  9%|▉         | 192/2088 [2:48:12<27:16:19, 51.78s/it][2025-11-07 04:58:38,606] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 193/2088 [2:49:06<27:29:18, 52.22s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.00185947958636, 'learning_rate': 9.075670498084291e-07, 'completion_length': 224.75390625, 'rewards/av_format_reward': 0.8984375, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.40770864486694336, 'reward': 1.7827085852622986, 'reward_std': 0.5054937899112701, 'kl': 0.0164794921875, 'epoch': 0.18}
  9%|▉         | 193/2088 [2:49:06<27:29:18, 52.22s/it][2025-11-07 04:59:31,699] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 194/2088 [2:49:59<27:36:43, 52.48s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.043453693103176, 'learning_rate': 9.070881226053639e-07, 'completion_length': 233.93359375, 'rewards/av_format_reward': 0.90625, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4709324836730957, 'reward': 1.8889012932777405, 'reward_std': 0.630224198102951, 'kl': 0.0172119140625, 'epoch': 0.19}
  9%|▉         | 194/2088 [2:49:59<27:36:43, 52.48s/it][2025-11-07 05:00:21,760] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 195/2088 [2:50:49<27:12:56, 51.76s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.006442369905046, 'learning_rate': 9.066091954022989e-07, 'completion_length': 234.66796875, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.4636344015598297, 'reward': 1.7917594909667969, 'reward_std': 0.48502974212169647, 'kl': 0.01800537109375, 'epoch': 0.19}
  9%|▉         | 195/2088 [2:50:49<27:12:56, 51.76s/it][2025-11-07 05:01:16,755] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 196/2088 [2:51:44<27:42:39, 52.73s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0165530557000992, 'learning_rate': 9.061302681992337e-07, 'completion_length': 234.61328125, 'rewards/av_format_reward': 0.61328125, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.40365178883075714, 'reward': 1.4427143335342407, 'reward_std': 0.5578383803367615, 'kl': 0.01837158203125, 'epoch': 0.19}
  9%|▉         | 196/2088 [2:51:44<27:42:39, 52.73s/it][2025-11-07 05:02:06,775] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 197/2088 [2:52:34<27:16:13, 51.92s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.124161511100297, 'learning_rate': 9.056513409961686e-07, 'completion_length': 231.44921875, 'rewards/av_format_reward': 0.9296875, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.48302966356277466, 'reward': 1.9439671635627747, 'reward_std': 0.6082354784011841, 'kl': 0.019775390625, 'epoch': 0.19}
  9%|▉         | 197/2088 [2:52:34<27:16:13, 51.92s/it][2025-11-07 05:02:57,686] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  9%|▉         | 198/2088 [2:53:25<27:05:51, 51.61s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9730421656727535, 'learning_rate': 9.051724137931034e-07, 'completion_length': 230.6171875, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5444707572460175, 'reward': 1.8843145966529846, 'reward_std': 0.4405646175146103, 'kl': 0.018798828125, 'epoch': 0.19}
  9%|▉         | 198/2088 [2:53:25<27:05:51, 51.61s/it][2025-11-07 05:03:47,860] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 199/2088 [2:54:15<26:51:22, 51.18s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.02728986914371, 'learning_rate': 9.046934865900383e-07, 'completion_length': 228.43359375, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.41015625, 'rewards/think_step_av_with_neutral_reward': 0.3861216902732849, 'reward': 1.5228403210639954, 'reward_std': 0.5479404628276825, 'kl': 0.0198974609375, 'epoch': 0.19}
 10%|▉         | 199/2088 [2:54:15<26:51:22, 51.18s/it][2025-11-07 05:04:43,864] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 200/2088 [2:55:11<27:36:03, 52.63s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1923760275249213, 'learning_rate': 9.042145593869731e-07, 'completion_length': 243.24609375, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.4112313240766525, 'reward': 1.6065438389778137, 'reward_std': 0.48659056425094604, 'kl': 0.0181884765625, 'epoch': 0.19}
 10%|▉         | 200/2088 [2:55:11<27:36:03, 52.63s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1802.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8415.mp4
[DEBUG] Step 200
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=182): <vis_desc>In the video, we see a character with a furrowed brow and an angry facial expression. His gaze is direct, indicating that he is engaged in a tense conversation.</vis_desc>
<aud_desc>In the audio, the tone is aggressive, with a fast pace and a sense of warning. In the text, the subtitle says, "You won't need Gift01, okay?" This sentence is likely an attack or remark from the male character towards the other person.</aud_desc>
<think>Based on the angry facial expression and direct gaze of the male character in the video, as well as the aggressive tone, fast pace, and warning feeling in the audio, we can infer that this sentence carries a sense of anger and aggression. The male character may be expressing his discontent and threatening the other person with an unnecessary solution.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the screen shows a woman next to a car, and the scene can be inferred to be in the car because of the fabric in the video. In the video, her facial expression shows obvious sadness and anxiety, with furrowed browns, slightly downturned corners of the mouth, and shallow blinking, suggesting that she is crying and feeling sorrowful. Her gaze seems to be fixed on the distance, perhaps contemplating something sad or contemplating whether she will make it through the journey. Overall, she is likely having a difficult conversation while driving, and the natural lighting does not provide enough warmth, contributing to her sad and anxious mood.</vis_desc>
<aud_desc>In the audio, the character's sobbing and crying sounds can be felt, indicating that the character's emotional state is very heavy and painful. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly suggest that the woman is experiencing sadness and anxiety. Visually, her facial expression reveals deep sadness and anxiety, with furrowed browns, downturned corners of the mouth, and shallow blinking consistent with distress and sorrow. Her distant gaze reinforces a sense of introspection and turmoil. Audibly, the sobbing and crying soundscape supports this interpretation, with the character's heavy emotion supported by both physical appearance and vocal tone. Even though the subtitle text does not convey emotional content, the congruence between her visual appearance and vocal expression provides sufficient evidence to confidently infer that her sad mood is conveyed through the vocal channels. Together, these multimodal signals create a coherent picture of someone who is deeply affected by a difficult conversation while driving, displaying profound sadness and
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1344.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08761.mp4
[DEBUG] Step 200
[DEBUG] prompt_completion_ids shape: torch.Size([32, 305])
[DEBUG] Shortest output (len=171): <vis_desc>In the video, we see a woman with wide eyes and slightly open mouth, seemingly expressing surprise or confusion.</vis_desc>
<aud_desc>In the audio, the tone is calm. In the text, the subtitle says, "What has caused the children to grow up that way?" This sentence may be the woman's questioning or complaint towards someone or something.</aud_desc>
<think>Based on the woman's facial expression in the video clues and the description of a calm tone in the audio clues, we can infer that this sentence may carry a tone of anger, dissatisfaction, or sadness. The woman's wide eyes and slightly open mouth indicate that she is expressing surprise or confusion, while the calm tone of the subtitle suggests that she may be unhappy with a certain situation or figure.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=305): <vis_desc>In the video, in the opening scene, we see a woman with a furrowed brow and slightly downturned mouth, which may indicate that she is experiencing an unpleasant emotional state. Her gaze seems to be focused on the other person, indicating that they are engaged in a serious conversation. As time passes, in the middle scene, the woman's expression and posture do not significantly change, suggesting that the conversation may be in progress for a while. Overall, we can infer that the emotional state of the woman in this video is one of displeasure or frustration, as she is engaged in a serious conversation with a serious expression and notable forehead wrinkles.</vis_desc>
<aud_desc>In the audio, the tone is relatively neutral. In the text, the subtitle says, "What experience have you been without children? How does that describe you?" This sentence is likely the woman asking the other person about their experience without children.</aud_desc>
<think>Based on the video clues of the woman's furrowed brow and downturned mouth, as well as the serious expression and forehead wrinkles, we can infer that the woman is expressing a negative emotion when saying this sentence, namely displeasure or frustration. The woman's tone is relatively neutral, but her focused gaze on the other person indicates that they are engaged in a serious conversation. Therefore, this sentence may be the woman requesting an explanation or insight from the other person regarding her experience without children.</think>
<answer>angry</answer>
[2025-11-07 05:05:43,894] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 201/2088 [2:56:11<28:44:59, 54.85s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0713352651785994, 'learning_rate': 9.03735632183908e-07, 'completion_length': 224.44921875, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4790791869163513, 'reward': 1.5884541869163513, 'reward_std': 0.5660965442657471, 'kl': 0.0186767578125, 'epoch': 0.19}
 10%|▉         | 201/2088 [2:56:11<28:44:59, 54.85s/it][2025-11-07 05:06:35,892] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 202/2088 [2:57:03<28:17:12, 53.99s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9190815875968272, 'learning_rate': 9.032567049808428e-07, 'completion_length': 230.140625, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.4019714444875717, 'reward': 1.534783959388733, 'reward_std': 0.4495885372161865, 'kl': 0.01861572265625, 'epoch': 0.19}
 10%|▉         | 202/2088 [2:57:03<28:17:12, 53.99s/it][2025-11-07 05:07:27,782] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 203/2088 [2:57:55<27:56:31, 53.36s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0206373285235113, 'learning_rate': 9.027777777777778e-07, 'completion_length': 228.48046875, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5184028148651123, 'reward': 1.8582465648651123, 'reward_std': 0.6137366890907288, 'kl': 0.017822265625, 'epoch': 0.19}
 10%|▉         | 203/2088 [2:57:55<27:56:31, 53.36s/it][2025-11-07 05:08:16,896] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 204/2088 [2:58:44<27:15:32, 52.09s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.021080333334778, 'learning_rate': 9.022988505747126e-07, 'completion_length': 222.96875, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.45107269287109375, 'reward': 1.6268539428710938, 'reward_std': 0.47816962003707886, 'kl': 0.01690673828125, 'epoch': 0.2}
 10%|▉         | 204/2088 [2:58:44<27:15:32, 52.09s/it][2025-11-07 05:09:05,300] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 205/2088 [2:59:32<26:39:59, 50.98s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9281567552115244, 'learning_rate': 9.018199233716475e-07, 'completion_length': 223.35546875, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.41796875, 'rewards/think_step_av_with_neutral_reward': 0.4018470048904419, 'reward': 1.663565754890442, 'reward_std': 0.5727888941764832, 'kl': 0.018798828125, 'epoch': 0.2}
 10%|▉         | 205/2088 [2:59:32<26:39:59, 50.98s/it][2025-11-07 05:09:57,404] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 206/2088 [3:00:24<26:49:45, 51.32s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.090113303151413, 'learning_rate': 9.013409961685823e-07, 'completion_length': 229.97265625, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.66796875, 'rewards/think_step_av_with_neutral_reward': 0.5670992732048035, 'reward': 2.0358493328094482, 'reward_std': 0.4828159809112549, 'kl': 0.01776123046875, 'epoch': 0.2}
 10%|▉         | 206/2088 [3:00:24<26:49:45, 51.32s/it][2025-11-07 05:10:52,363] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 207/2088 [3:01:19<27:23:04, 52.41s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0566547246124807, 'learning_rate': 9.008620689655172e-07, 'completion_length': 241.83203125, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.5179689228534698, 'reward': 1.885156512260437, 'reward_std': 0.6008996069431305, 'kl': 0.019287109375, 'epoch': 0.2}
 10%|▉         | 207/2088 [3:01:19<27:23:04, 52.41s/it][2025-11-07 05:11:43,397] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|▉         | 208/2088 [3:02:10<27:09:15, 52.00s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.105156865950391, 'learning_rate': 9.003831417624521e-07, 'completion_length': 227.73046875, 'rewards/av_format_reward': 0.921875, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.4465293735265732, 'reward': 1.9309043288230896, 'reward_std': 0.46925094723701477, 'kl': 0.0177001953125, 'epoch': 0.2}
 10%|▉         | 208/2088 [3:02:10<27:09:15, 52.00s/it][2025-11-07 05:12:35,807] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 209/2088 [3:03:03<27:12:15, 52.12s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.2595067248071956, 'learning_rate': 8.999042145593869e-07, 'completion_length': 224.8203125, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.500726968050003, 'reward': 1.6804144978523254, 'reward_std': 0.6012053191661835, 'kl': 0.0185546875, 'epoch': 0.2}
 10%|█         | 209/2088 [3:03:03<27:12:15, 52.12s/it][2025-11-07 05:13:26,737] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 210/2088 [3:03:54<27:00:12, 51.76s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.084097073600442, 'learning_rate': 8.994252873563218e-07, 'completion_length': 226.4296875, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.6640625, 'rewards/think_step_av_with_neutral_reward': 0.5494769215583801, 'reward': 1.8736956715583801, 'reward_std': 0.48280663788318634, 'kl': 0.0172119140625, 'epoch': 0.2}
 10%|█         | 210/2088 [3:03:54<27:00:12, 51.76s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02524.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05867.mp4
[DEBUG] Step 210
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=199): <vis_desc>In the video, we see a man standing in an indoor environment. His facial expression appears relaxed and happy, with a slight upward curve at the corners of his mouth. He seems to be smiling, which often implies contentment or a sense of humor.</vis_desc>
<aud_desc>In the audio, the character's voice is laughing and humorous, expressing the character's dissatisfaction and anger. In the text, the subtitle reads, "I'm so bad! I'll make it better, let's do it together." This sentence expresses the man's frustration and determination.</aud_desc>
<think>Based on the man's facial expressions of relaxation and happiness in the video clues, as well as the laughter and humorous tone in the audio clues, we can infer that this sentence is the man expressing his dissatisfaction and determination. The content of this sentence aligns with the emotional states displayed by the man in the video and audio clues.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the screen shows a woman walking in a dimly lit corridor. At the beginning of the video, she is facing the camera and looks downwards, with a somewhat melancholic expression in her eye sockets. As she walks on, she turns her head and seems a bit nervous, with slightly widened eyes and slightly open mouth, showing more visible teeth. In the following scenes, she stops walking, widens her eyes, and looks directly at the other person, with an expression that may be confusion or surprise.</vis_desc>
<aud_desc>In the audio, the character's breath is short and fast, with noticeable panting sounds, indicating deep breathing. The tone is low and monotonous, lacking a heroic or dramatic element. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues clearly convey a sense of surprise. Visually, the woman’s initial melancholic expression transitions sharply as she looks downwards, accompanied by a visual cue of teeth showing, which typically signals surprise. Her weary or nervous expression as she turns and looks directly at the other person further reinforce this interpretation, as these body language signals often accompany sudden or unexpected reactions. Audibly, her rapid, shortbreathed tone with noticeable panting suggests an immediate and spontaneous emotional reaction rather than a staged or coached display. When combining these multimodal signals, the surprise she feels becomes unmistakable: the suddenness of her eyes showing teeth aligns closely with typical surprise, and the auditory quality reinforces this emotional state as the interlocutor and environment are devoid of other strong emotional markers. Together, the somber visual expressions combined with the low and constrained vocal tone from
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5301.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01334.mp4
[DEBUG] Step 210
[DEBUG] prompt_completion_ids shape: torch.Size([32, 298])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a male wearing blue medical scrub. His facial expression appears neutral, with his gaze directly focused on the other person, seemingly engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads: "Yes, she tried twice. She did try again." This sentence may be the male's response to a prior statement or question.</aud_desc>
<think>Based on the neutral facial expression and direct gaze of the male in the video clues, as well as the neutral tone in the audio clue, we can infer that this sentence may carry an objective or factual tone. The male may be recounting or emphasizing a certain experience or statement, rather than expressing strong emotions. Therefore, this sentence may be described as neutral?</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=298): <vis_desc>In the video, we see a woman sobbing gently in a indoors setting. Her tears drop down slowly, and her face is covered with embarrassment. She has one hand pressing against her nose, and the other hand supports it, seemingly trying to control herself from crying even more. The specific emotions of the character need to be analyzed in conjunction with additional context.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>Despite the lack of emotional clarity in the auditory information and the neutral subtitle content, the visual cues provide the strongest indication of the character’s emotional state. The woman’s gentle sobbing and cover of the nose implies an internal emotional distress or embarrassment, which is inherently emotional and often reflected in gentle, subdued behaviors. The presence of one hand pressing against her nose and the other supporting her indicates a tried-and-true strategy for self-control or soothing oneself, further reinforcing the meaning of these actions as deliberate and emotionally resonant. Although specific emotions are not discernible, the combined presence of these visual signals— gentle silence paired with body language reflecting effort and comfort— convincingly conveys a sense of sadness and vulnerability, common emotional states in such contexts. Thus, integrating all the information, it is reasonable to conclude that the character is experiencing sadness.</think>
<answer>sad</answer>
[2025-11-07 05:14:20,291] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 211/2088 [3:04:47<27:16:07, 52.30s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.015343243628544, 'learning_rate': 8.989463601532567e-07, 'completion_length': 238.8203125, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4759583920240402, 'reward': 1.6126770377159119, 'reward_std': 0.5060326308012009, 'kl': 0.0179443359375, 'epoch': 0.2}
 10%|█         | 211/2088 [3:04:47<27:16:07, 52.30s/it][2025-11-07 05:15:12,036] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 212/2088 [3:05:39<27:10:03, 52.13s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.123480348806956, 'learning_rate': 8.984674329501916e-07, 'completion_length': 237.01171875, 'rewards/av_format_reward': 0.89453125, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4213130474090576, 'reward': 1.8666255474090576, 'reward_std': 0.46322789788246155, 'kl': 0.0177001953125, 'epoch': 0.2}
 10%|█         | 212/2088 [3:05:39<27:10:03, 52.13s/it][2025-11-07 05:16:05,769] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 213/2088 [3:06:33<27:24:12, 52.61s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0635143518909076, 'learning_rate': 8.979885057471264e-07, 'completion_length': 230.26171875, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.4246072322130203, 'reward': 1.690232276916504, 'reward_std': 0.5328640937805176, 'kl': 0.01849365234375, 'epoch': 0.2}
 10%|█         | 213/2088 [3:06:33<27:24:12, 52.61s/it][2025-11-07 05:16:57,410] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 214/2088 [3:07:24<27:14:13, 52.32s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0700428839786165, 'learning_rate': 8.975095785440613e-07, 'completion_length': 227.0625, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5555885285139084, 'reward': 1.8993384838104248, 'reward_std': 0.4735299348831177, 'kl': 0.01739501953125, 'epoch': 0.2}
 10%|█         | 214/2088 [3:07:24<27:14:13, 52.32s/it][2025-11-07 05:17:51,259] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 215/2088 [3:08:18<27:27:36, 52.78s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9893489561759954, 'learning_rate': 8.97030651340996e-07, 'completion_length': 240.921875, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.3828125, 'rewards/think_step_av_with_neutral_reward': 0.35676872730255127, 'reward': 1.4466124773025513, 'reward_std': 0.5077509880065918, 'kl': 0.01898193359375, 'epoch': 0.21}
 10%|█         | 215/2088 [3:08:18<27:27:36, 52.78s/it][2025-11-07 05:18:45,310] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 216/2088 [3:09:12<27:38:37, 53.16s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0118347480712826, 'learning_rate': 8.96551724137931e-07, 'completion_length': 224.3984375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5117449760437012, 'reward': 1.933620035648346, 'reward_std': 0.5183893591165543, 'kl': 0.02056884765625, 'epoch': 0.21}
 10%|█         | 216/2088 [3:09:12<27:38:37, 53.16s/it][2025-11-07 05:19:39,899] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 217/2088 [3:10:07<27:51:05, 53.59s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1175705384034194, 'learning_rate': 8.960727969348658e-07, 'completion_length': 231.1875, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4323273301124573, 'reward': 1.7643585801124573, 'reward_std': 0.5404331386089325, 'kl': 0.0185546875, 'epoch': 0.21}
 10%|█         | 217/2088 [3:10:07<27:51:05, 53.59s/it][2025-11-07 05:20:32,989] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 218/2088 [3:11:00<27:45:34, 53.44s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1062652815179774, 'learning_rate': 8.955938697318007e-07, 'completion_length': 240.40234375, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.38826629519462585, 'reward': 1.5093600749969482, 'reward_std': 0.5394953191280365, 'kl': 0.01812744140625, 'epoch': 0.21}
 10%|█         | 218/2088 [3:11:00<27:45:34, 53.44s/it][2025-11-07 05:21:26,420] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 10%|█         | 219/2088 [3:11:53<27:44:32, 53.44s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.122433058264832, 'learning_rate': 8.951149425287355e-07, 'completion_length': 232.5, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.47373513877391815, 'reward': 1.7588913440704346, 'reward_std': 0.5555083453655243, 'kl': 0.019775390625, 'epoch': 0.21}
 10%|█         | 219/2088 [3:11:54<27:44:32, 53.44s/it][2025-11-07 05:22:16,001] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 220/2088 [3:12:43<27:07:38, 52.28s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0177784789940305, 'learning_rate': 8.946360153256705e-07, 'completion_length': 230.4296875, 'rewards/av_format_reward': 0.6484375, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5454659759998322, 'reward': 1.8189035058021545, 'reward_std': 0.5566169321537018, 'kl': 0.01788330078125, 'epoch': 0.21}
 11%|█         | 220/2088 [3:12:43<27:07:38, 52.28s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09148.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07433.mp4
[DEBUG] Step 220
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=184): <vis_desc>In the video, we see a woman with wide eyes and slightly open mouth, standing indoors. Her facial expression shows surprise and excitement. She seems to be talking to someone and appears enthusiastic.</vis_desc>
<aud_desc>In the audio, we can sense the character's excited emotions through her volume and tone. In the text, the subtitle says, "We can take care of ourselves." This sentence may be a phrase said by the female character while speaking. Based on the surprised and excited facial expressions of the female character in the video clues, as well as the description of the excited emotions in the audio clues, we can deduce that this sentence may carry a sense of relaxation and happiness.</aud_desc>
<think>Therefore, this sentence may be the female character's way of expressing her confidence and relaxation, aligning with her enthusiastic mood.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, in the opening scene, we see a woman with wide-open eyes and an outstretched arm, which may indicate that she is trying to stop the other person. Her facial expression appears surprised and excited, suggesting that she may have heard some unexpected news or amazing information. Her eyes show a certain inclination towards certain things and actions, further reinforcing the impression of surprise. As time passes, the woman's expression and posture remain consistent, indicating that she may still be processing whatever she has heard or seen. Taking these scenes together, we can infer that the woman in the video is likely in a state of surprise.</vis_desc>
<aud_desc>In the audio, the character's voice is excited and cheerful, as if the character is going on an adventure or discovery. Towards the end of the speech, the character emphasizes the words "no way" and "you have to go", implying that the woman expresses her surprise and excitement at the moment using a stop-motion sentence, signaling her unwillingness to go despite the previous statement. In the text, the subtitle reads: "No way! I have to go and check it out." This sentence expresses the woman's surprise and excitement.</aud_desc>
<think>Based on the woman's facial expression and body language in the video clues, as well as the character's excited and cheerful voice in the audio clues, we can infer that this sentence is the woman expressing her surprise and willingness to go even more even though the previous statement said "No way!" The woman may have heard or seen something that surprised and caught her attention, and she believes that going is necessary or right, even if previous statements seem to have contradicted her desires or expectations.</think>
<answer
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07002.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2492.mp4
[DEBUG] Step 220
[DEBUG] prompt_completion_ids shape: torch.Size([32, 292])
[DEBUG] Shortest output (len=204): <vis_desc>In the video, we see a woman standing in an indoor environment with ornate decorations. Her facial expression appears serious and her mouth is moving, indicating that she is engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone and intonation of the character suggest that her emotional state is not positive. In the text, the subtitle reads: "She feels happy again, and her position will be stronger after the treaty, her letters will come." This sentence may be the woman's evaluation or reaction to a treaty.</aud_desc>
<think>Based on the serious facial expression and mouth movement of the woman in the video clues, as well as the description of a negative emotional state in the character's intonation and tone in the audio clues, we can infer that this sentence may have a neutral or negative emotional tone. The woman may be satisfied and happy with the results of the treaty, thus expressing her opinion or reflection.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=292): <vis_desc>In the video, the scene shows a woman in a beautifully decorated room, with a background that appears to be an exhibition or a similar gathering place. In the video, her facial expression is serious, with a furrowed brow and downturned corners of the mouth, which may indicate that she is engaged in a tense conversation or debate. Her mouth is open, seemingly emphasizing a point or responding to an opponent. Overall, she is engaged in a serious and negative conversation.</vis_desc>
<aud_desc>In the audio, the tone is serious. Combined with the text content, it seems that the character is accusing another person, potentially accusing them of a crime. In the text, the subtitle reads: "She feels her strength has been put in stronger positions for a treaty, her letters also say so." This sentence may be the woman's accusation or questioning towards another person.</aud_desc>
<think>Based on the serious facial expression and furrowed brow of the woman in the video, as well as the serious tone in the audio, we can infer that this sentence carries a sense of anger or dissatisfaction. The woman may be accusing another person for taking advantage of her strength or using her power for a treaty, or despite her best efforts, the other person has not fulfilled their obligations. such actions are often associated with criminal charges, hence displaying a sense of anger or dissatisfaction.</think>
<answer>angry</answer>
[2025-11-07 05:23:09,761] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 221/2088 [3:13:37<27:20:34, 52.72s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.027900109819116, 'learning_rate': 8.941570881226054e-07, 'completion_length': 231.22265625, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5299849808216095, 'reward': 1.9401412606239319, 'reward_std': 0.47473347187042236, 'kl': 0.01739501953125, 'epoch': 0.21}
 11%|█         | 221/2088 [3:13:37<27:20:34, 52.72s/it][2025-11-07 05:24:02,465] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 222/2088 [3:14:30<27:19:31, 52.72s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.1446693963844576, 'learning_rate': 8.936781609195402e-07, 'completion_length': 235.0234375, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4485370069742203, 'reward': 1.796193242073059, 'reward_std': 0.5430253446102142, 'kl': 0.0184326171875, 'epoch': 0.21}
 11%|█         | 222/2088 [3:14:30<27:19:31, 52.72s/it][2025-11-07 05:24:57,942] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 223/2088 [3:15:25<27:44:22, 53.55s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9376255552575834, 'learning_rate': 8.93199233716475e-07, 'completion_length': 241.86328125, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.48078227043151855, 'reward': 1.8401572108268738, 'reward_std': 0.49465224146842957, 'kl': 0.01788330078125, 'epoch': 0.21}
 11%|█         | 223/2088 [3:15:25<27:44:22, 53.55s/it][2025-11-07 05:25:48,930] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 224/2088 [3:16:16<27:19:39, 52.78s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9141797010023742, 'learning_rate': 8.927203065134099e-07, 'completion_length': 241.3671875, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.5339174419641495, 'reward': 1.7292298674583435, 'reward_std': 0.534044161438942, 'kl': 0.01751708984375, 'epoch': 0.21}
 11%|█         | 224/2088 [3:16:16<27:19:39, 52.78s/it][2025-11-07 05:26:40,366] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 225/2088 [3:17:07<27:06:15, 52.38s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.075525257717534, 'learning_rate': 8.922413793103448e-07, 'completion_length': 232.921875, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5117339789867401, 'reward': 1.7343902587890625, 'reward_std': 0.4831582009792328, 'kl': 0.0181884765625, 'epoch': 0.22}
 11%|█         | 225/2088 [3:17:07<27:06:15, 52.38s/it][2025-11-07 05:27:34,003] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 226/2088 [3:18:01<27:17:10, 52.76s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.089731235447152, 'learning_rate': 8.917624521072796e-07, 'completion_length': 236.3359375, 'rewards/av_format_reward': 0.640625, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.49492184817790985, 'reward': 1.6785156726837158, 'reward_std': 0.4540109932422638, 'kl': 0.01953125, 'epoch': 0.22}
 11%|█         | 226/2088 [3:18:01<27:17:10, 52.76s/it][2025-11-07 05:28:24,240] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 227/2088 [3:18:51<26:52:48, 52.00s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.048687308198666, 'learning_rate': 8.912835249042145e-07, 'completion_length': 233.55859375, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.5111628472805023, 'reward': 1.82366281747818, 'reward_std': 0.4930383861064911, 'kl': 0.01922607421875, 'epoch': 0.22}
 11%|█         | 227/2088 [3:18:51<26:52:48, 52.00s/it][2025-11-07 05:29:18,626] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 228/2088 [3:19:46<27:14:10, 52.72s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.0266768034957017, 'learning_rate': 8.908045977011494e-07, 'completion_length': 235.08984375, 'rewards/av_format_reward': 0.6640625, 'rewards/accuracy_reward': 0.41015625, 'rewards/think_step_av_with_neutral_reward': 0.3748668283224106, 'reward': 1.4490856528282166, 'reward_std': 0.4298599511384964, 'kl': 0.01776123046875, 'epoch': 0.22}
 11%|█         | 228/2088 [3:19:46<27:14:10, 52.72s/it][2025-11-07 05:30:12,898] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 229/2088 [3:20:40<27:27:45, 53.18s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0170238224043464, 'learning_rate': 8.903256704980843e-07, 'completion_length': 230.6875, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.37695157527923584, 'reward': 1.5175765752792358, 'reward_std': 0.4954816699028015, 'kl': 0.01934814453125, 'epoch': 0.22}
 11%|█         | 229/2088 [3:20:40<27:27:45, 53.18s/it][2025-11-07 05:31:08,080] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 230/2088 [3:21:35<27:45:26, 53.78s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.117113380888193, 'learning_rate': 8.898467432950191e-07, 'completion_length': 240.18359375, 'rewards/av_format_reward': 0.59765625, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5387395322322845, 'reward': 1.7262394428253174, 'reward_std': 0.5898786187171936, 'kl': 0.017578125, 'epoch': 0.22}
 11%|█         | 230/2088 [3:21:35<27:45:26, 53.78s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10280.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7232.mp4
[DEBUG] Step 230
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=199): <vis_desc>In the video, we see a male character with a focused expression, slightly close eyes, and looking downward. His attention is fixed on some blurry things on the ground.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The character’s focused gaze and subdued facial expression—with slightly closed eyes and downward staring—suggest a state of quiet observation or contemplation rather than strong emotion. The lack of distinct auditory cues (neither vocal nor verbal) nor textual information provides conflicting or reinforcing emotional signals, conicides with the visual calm to indicate a neutral emotional state. Taken together, the isolated visual focus and subdued vocal tone reinforce the conclusion that the character’s emotion is best categorized as measured—the balance of calm and absence—neutralizing the influence of ambiguous auditory information.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=295): <aud_desc>In the audio, the tone is stern and aggressive. Combined with the text content, it can be inferred that the character's emotional state is anger. The subtitle in the text says, "He who lacks learning is despised by the people. Seizing power without learning is a threat to a regime." This sentence carries a sense of anger and accusation.</aud_desc>
<vis_desc>In the video, the screen shows a man wearing a judge's uniform and a baseball cap, standing in a traditional-style room with calligraphy paintings and a potted plant. In the video, his facial expression is serious, his eyes are wide open, and he has noticeable movement of his head. This indicates that he may be engaged in a heated argument with someone. His mouth is open, and his gaze does not stop staring, suggesting that he is passionately expressing his viewpoint or complaining about a problem.</vis_desc>
<think>Based on the serious facial expression, wide-open eyes, and staring posture of the judge in the video, it can be inferred that he is engaged in a heated argument with someone. Additionally, based on the description of the audio as stern and aggressive, as well as the analysis of anger, it can be inferred that this sentence carries a sense of anger and accusation. Therefore, this sentence expresses the judge's dissatisfaction and accusation towards someone and emphasizes the importance of learning while accusing powerlessness.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02710.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02456.mp4
[DEBUG] Step 230
[DEBUG] prompt_completion_ids shape: torch.Size([32, 307])
[DEBUG] Shortest output (len=185): <vis_desc>In the video, we see a male character with a furrowed brow and a negative facial expression. His mouth is open and his gaze is direct, seemingly engaged in a conversation with someone.</vis_desc>
<aud_desc>In the audio, the tone is low and monotonous. In the text, the subtitle reads: "Dr. Zhang is in his home outside the house, always walking around, always in a state of excitement." This sentence may be the male character's evaluation or reaction to someone.</aud_desc>
<think>Based on the negative facial expression and open mouth in the video clues, as well as the low and monotonous tone in the audio clue, we can infer that this sentence may carry an excited or serious tone. The male character may express his excitement when saying this sentence, which aligns with his overall performance.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=307): <vis_desc>In the video, the screen shows a male character in an indoor setting. In the video, his eyebrows are slightly furrowed, and his facial expression appears serious and focused. His gaze is directly focused on the other person, giving off an overall sense of weightlessness. The corners of his mouth are slightly lifted, indicating a pleasant or satisfied emotion. His mouth moves slightly, suggesting that he is engaged in a calm conversation.</vis_desc>
<aud_desc>In the audio, the tone and intonation of the character are relatively steady. In the text, the subtitle reads: "No, you're not in your house outside of the door, always always, it feels warmer." This sentence may be the male character emphasizing his absence or unusual behavior.</aud_desc>
<think>Based on the male character's serious and focused facial expression in the video, as well as his direct gaze toward the other person, it can be inferred that he is engaged in a calm conversation and possibly expressing a sense of happiness or satisfaction about his absence. The overall sense of weightlessness also supports this inference. However, the quiet tone and intonation of the character in the audio also imply that he may be discussing a topic that does not require words, which may further emphasize his absence or unusual behavior. Therefore, this sentence may be the male character emphasizing his absence or discussing a warm feeling outside the door, indicating that he is not in his house but still feels comfortable or relaxed.</think>
<answer>neutral</answer>
[2025-11-07 05:32:03,511] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 231/2088 [3:22:31<27:59:52, 54.28s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9696956638275305, 'learning_rate': 8.893678160919539e-07, 'completion_length': 244.2109375, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5519753694534302, 'reward': 2.05588161945343, 'reward_std': 0.49406807124614716, 'kl': 0.01800537109375, 'epoch': 0.22}
 11%|█         | 231/2088 [3:22:31<27:59:52, 54.28s/it][2025-11-07 05:32:57,790] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 232/2088 [3:23:25<27:59:01, 54.28s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.934608301619099, 'learning_rate': 8.888888888888888e-07, 'completion_length': 249.79296875, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.4262348711490631, 'reward': 1.6215473413467407, 'reward_std': 0.4785132110118866, 'kl': 0.018310546875, 'epoch': 0.22}
 11%|█         | 232/2088 [3:23:25<27:59:01, 54.28s/it][2025-11-07 05:33:49,628] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 233/2088 [3:24:17<27:35:26, 53.55s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.8362111745626417, 'learning_rate': 8.884099616858237e-07, 'completion_length': 237.9140625, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.454190656542778, 'reward': 1.6729406714439392, 'reward_std': 0.522306352853775, 'kl': 0.0181884765625, 'epoch': 0.22}
 11%|█         | 233/2088 [3:24:17<27:35:26, 53.55s/it][2025-11-07 05:34:42,586] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█         | 234/2088 [3:25:10<27:29:05, 53.37s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.102689535158408, 'learning_rate': 8.879310344827586e-07, 'completion_length': 235.13671875, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.4658668041229248, 'reward': 1.7002418041229248, 'reward_std': 0.5177145749330521, 'kl': 0.02105712890625, 'epoch': 0.22}
 11%|█         | 234/2088 [3:25:10<27:29:05, 53.37s/it][2025-11-07 05:35:35,761] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 235/2088 [3:26:03<27:26:25, 53.31s/it]                                                       {'loss': 0.0007, 'grad_norm': 2.062651923053496, 'learning_rate': 8.874521072796934e-07, 'completion_length': 239.95703125, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.4665467292070389, 'reward': 1.6071717739105225, 'reward_std': 0.5586491823196411, 'kl': 0.0186767578125, 'epoch': 0.23}
 11%|█▏        | 235/2088 [3:26:03<27:26:25, 53.31s/it][2025-11-07 05:36:27,484] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 236/2088 [3:26:55<27:10:49, 52.83s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.019597032373161, 'learning_rate': 8.869731800766284e-07, 'completion_length': 237.109375, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.64453125, 'rewards/think_step_av_with_neutral_reward': 0.5585440248250961, 'reward': 1.9257314801216125, 'reward_std': 0.6132409870624542, 'kl': 0.01885986328125, 'epoch': 0.23}
 11%|█▏        | 236/2088 [3:26:55<27:10:49, 52.83s/it][2025-11-07 05:37:25,292] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 237/2088 [3:27:52<27:55:58, 54.33s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.992836406245738, 'learning_rate': 8.864942528735632e-07, 'completion_length': 233.21484375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.5488599538803101, 'reward': 1.9433910846710205, 'reward_std': 0.5565768182277679, 'kl': 0.0189208984375, 'epoch': 0.23}
 11%|█▏        | 237/2088 [3:27:52<27:55:58, 54.33s/it][2025-11-07 05:38:16,114] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 238/2088 [3:28:43<27:22:38, 53.27s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.046671302722934, 'learning_rate': 8.860153256704981e-07, 'completion_length': 224.58203125, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.5229692161083221, 'reward': 1.8159379959106445, 'reward_std': 0.5831089615821838, 'kl': 0.0194091796875, 'epoch': 0.23}
 11%|█▏        | 238/2088 [3:28:43<27:22:38, 53.27s/it][2025-11-07 05:39:05,932] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 239/2088 [3:29:33<26:49:47, 52.24s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0060572355245765, 'learning_rate': 8.855363984674328e-07, 'completion_length': 236.71875, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.4573604315519333, 'reward': 1.8714230060577393, 'reward_std': 0.5085153728723526, 'kl': 0.0206298828125, 'epoch': 0.23}
 11%|█▏        | 239/2088 [3:29:33<26:49:47, 52.24s/it][mpeg4 @ 0xae674700] Error at MB: 3416
[mpeg4 @ 0xae674700] slice end not reached but screenspace end (10 left 798000, score= -69)
[2025-11-07 05:39:59,414] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 11%|█▏        | 240/2088 [3:30:26<27:00:25, 52.61s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9389146630583616, 'learning_rate': 8.850574712643678e-07, 'completion_length': 240.44921875, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.45953184366226196, 'reward': 1.815000593662262, 'reward_std': 0.5766755044460297, 'kl': 0.0189208984375, 'epoch': 0.23}
 11%|█▏        | 240/2088 [3:30:26<27:00:25, 52.61s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08604.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08600.mp4
[DEBUG] Step 240
[DEBUG] prompt_completion_ids shape: torch.Size([32, 287])
[DEBUG] Shortest output (len=185): <vis_desc>In the video, we see a female character with a furrowed brow and slightly downturned mouth, seemingly expressing some dissatisfaction or disappointment.</vis_desc>
<aud_desc>In the audio, the character's tone is low and the character's voice sounds very scared. In the text, the subtitle says, "Seiicha is begging for me." This sentence may be the female character begging for someone to help or care for.</aud_desc>
<think>Based on the video clues of the female character's furrowed brow and downturned mouth, as well as the audio clue of the character's low tone and scared voice, we can infer that this sentence expresses the female character's frustration and sadness. She may be begging for someone to help her or care for her, but the person she is begging from does not seem to want to help her.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=287): <vis_desc>In the video, we see a man and a woman facing each other. The man's gaze is directed towards the woman on the right side, while his gaze is not focused on the other person when looking at the other. His hands are crossed, indicating that he is attentively listening or waiting for the woman to speak. Subsequently, the man's mouth moves, seemingly responding to the woman's statement or question, with his gaze still focused on the other person. Overall, the man's body language suggests that he is possibly listening attentively and waiting for an opportunity to speak.</vis_desc>
<aud_desc>In the audio, the voice is urgent and excited. In the text, the subtitle reads: "When I said this was wrong, suddenly, I had to explain it again to a girl." This sentence may express the man's dissatisfaction or confusion with a certain topic.</aud_desc>
<think>Based on the visual clues of the man's body language, including his serious gaze and the movement of his mouth, as well as the audio clue of the man's urgent and excited tone, we can infer that this sentence expresses the man's dissatisfaction or confusion with a certain topic. The man's expression and tone indicate that he may feel angry or annoyed, possibly because he thinks the woman is expressing an incorrect view or because she is arguing with him.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07601.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5389.mp4
[DEBUG] Step 240
[DEBUG] prompt_completion_ids shape: torch.Size([32, 340])
[DEBUG] Shortest output (len=182): <vis_desc>In the video, we see a woman with a surprised facial expression. Her eyes are wide open and her mouth is slightly open, as if she is speaking or reacting to something.</vis_desc>
<aud_desc>In the audio, the character's tone is surprised when saying "Was I suddenly dropped?" In the text, the subtitle reads "Was I suddenly dropped?" This sentence may be spoken by the woman when she rec remembers something.</aud_desc>
<think>Based on the surprised facial expression of the woman in the video, the character's surprised tone when saying "Was I suddenly dropped?", and the description in the subtitle that the character recalls being suddenly dropped, we can infer that this sentence may carry a sense of surprise or disbelief. The woman may feel surprised and amazed by the memory of being suddenly dropped, hence expressing this word.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=340): <aud_desc>In the audio, the character's voice is loud and excited, with a stern and aggressive tone. Combining the video clues of the character's wide-open eyes, upturned corners of the mouth, and exaggerated hand gestures, as well as the audio clue of the character's excited and stern voice, it can be inferred that the character is expressing anger and excitement in this sentence. Overall, the video and audio clues provide a strong emotional indication that the character is expressing anger.</aud_desc>
<vis_desc>In the video, the screen shows a man in an indoor setting. In the video, his eyes are wide open, his mouth is upturned, and his palms are unfolded, indicating that he is speaking or expressing an opinion. His facial expression is fierce and aggressive, suggesting that he is very angry. His body posture is forward-leaning, with his arms crossed, leaning heavily forward, and forcefully touching his legs, further indicating that he is very angry.</vis_desc>
<think>The combined visual and auditory cues strongly support the inference that the man is experiencing intense anger. Visually, his wide-open eyes, upturned corners of the mouth, and exaggerated, exaggeratedalized hand gestures clearly convey a shouting or expressing anger emotion. The audible audio reinforces this impression, as his voice is loud and exuberant, with a stern and aggressive tone. Even though the subtitle content does not convey emotion, the combined visual expressions and vocal qualities provide compelling evidence that the character is entirely angry. Together, the expressive aggression and aggressive vocal style offer a clear and cohesive message of anger, making this the most supported prediction among the options.</think>
<answer>angry</answer>
[2025-11-07 05:40:50,739] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 241/2088 [3:31:18<26:47:40, 52.23s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.131647576357104, 'learning_rate': 8.845785440613026e-07, 'completion_length': 230.1484375, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.4498331695795059, 'reward': 1.6021769642829895, 'reward_std': 0.5907493382692337, 'kl': 0.020263671875, 'epoch': 0.23}
 12%|█▏        | 241/2088 [3:31:18<26:47:40, 52.23s/it][2025-11-07 05:41:44,912] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 242/2088 [3:32:12<27:04:47, 52.81s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9324371863636853, 'learning_rate': 8.840996168582375e-07, 'completion_length': 230.58984375, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.4055244028568268, 'reward': 1.6750556826591492, 'reward_std': 0.555555522441864, 'kl': 0.0194091796875, 'epoch': 0.23}
 12%|█▏        | 242/2088 [3:32:12<27:04:47, 52.81s/it][2025-11-07 05:42:37,210] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 243/2088 [3:33:04<26:59:10, 52.66s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9787254896839244, 'learning_rate': 8.836206896551723e-07, 'completion_length': 239.65625, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.36328125, 'rewards/think_step_av_with_neutral_reward': 0.3448607176542282, 'reward': 1.4464231729507446, 'reward_std': 0.511289969086647, 'kl': 0.01959228515625, 'epoch': 0.23}
 12%|█▏        | 243/2088 [3:33:04<26:59:10, 52.66s/it][2025-11-07 05:43:28,670] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 244/2088 [3:33:56<26:47:16, 52.30s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.099736258951005, 'learning_rate': 8.831417624521073e-07, 'completion_length': 239.50390625, 'rewards/av_format_reward': 0.5234375, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.42716842889785767, 'reward': 1.4193559288978577, 'reward_std': 0.5800046622753143, 'kl': 0.01953125, 'epoch': 0.23}
 12%|█▏        | 244/2088 [3:33:56<26:47:16, 52.30s/it][2025-11-07 05:44:21,158] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 245/2088 [3:34:48<26:48:09, 52.35s/it]                                                       {'loss': 0.0007, 'grad_norm': 1.9597165177540894, 'learning_rate': 8.826628352490421e-07, 'completion_length': 241.24609375, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.4481579214334488, 'reward': 1.6864391565322876, 'reward_std': 0.590190589427948, 'kl': 0.0174560546875, 'epoch': 0.23}
 12%|█▏        | 245/2088 [3:34:48<26:48:09, 52.35s/it][2025-11-07 05:45:15,272] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 246/2088 [3:35:42<27:03:29, 52.88s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.017430723770618, 'learning_rate': 8.82183908045977e-07, 'completion_length': 239.9296875, 'rewards/av_format_reward': 0.62109375, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.48943524062633514, 'reward': 1.6769352555274963, 'reward_std': 0.5607866942882538, 'kl': 0.0201416015625, 'epoch': 0.24}
 12%|█▏        | 246/2088 [3:35:42<27:03:29, 52.88s/it][2025-11-07 05:46:08,830] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 247/2088 [3:36:36<27:08:49, 53.08s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0544963868042685, 'learning_rate': 8.817049808429117e-07, 'completion_length': 243.57421875, 'rewards/av_format_reward': 0.609375, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.4115319401025772, 'reward': 1.489656925201416, 'reward_std': 0.5173960626125336, 'kl': 0.0203857421875, 'epoch': 0.24}
 12%|█▏        | 247/2088 [3:36:36<27:08:49, 53.08s/it][2025-11-07 05:47:02,044] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 248/2088 [3:37:29<27:09:08, 53.12s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9351588066184062, 'learning_rate': 8.812260536398467e-07, 'completion_length': 233.37109375, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.4792984127998352, 'reward': 1.86211097240448, 'reward_std': 0.47698645293712616, 'kl': 0.0205078125, 'epoch': 0.24}
 12%|█▏        | 248/2088 [3:37:29<27:09:08, 53.12s/it][2025-11-07 05:47:56,304] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 249/2088 [3:38:23<27:18:44, 53.47s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9607914768811794, 'learning_rate': 8.807471264367816e-07, 'completion_length': 236.85546875, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.44140625, 'rewards/think_step_av_with_neutral_reward': 0.3848646879196167, 'reward': 1.5528334975242615, 'reward_std': 0.5713682770729065, 'kl': 0.0191650390625, 'epoch': 0.24}
 12%|█▏        | 249/2088 [3:38:23<27:18:44, 53.47s/it][2025-11-07 05:48:45,269] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 250/2088 [3:39:12<26:36:25, 52.11s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0801737361969357, 'learning_rate': 8.802681992337164e-07, 'completion_length': 228.6875, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4141896069049835, 'reward': 1.6993458271026611, 'reward_std': 0.6098127067089081, 'kl': 0.02008056640625, 'epoch': 0.24}
 12%|█▏        | 250/2088 [3:39:12<26:36:25, 52.11s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09468.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00308.mp4
[DEBUG] Step 250
[DEBUG] prompt_completion_ids shape: torch.Size([32, 283])
[DEBUG] Shortest output (len=179): <vis_desc>In the video, we see a male character standing next to a door in a dimly lit corridor. His facial expression is very scared and vividly shows the typical signs of a panic attack.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very scared and gives a feeling of being eaten by a monster. In the text, the subtitle says, "Let's panic." This sentence may be the male character's response to a sudden situation or danger.</aud_desc>
<think>Based on the video clues of the male character's scared facial expression and the audio clue of the character's voice giving a feeling of being eaten by a monster, we can infer that this sentence expresses the male character's desire to panic and prepare for a dangerous situation. The overall emotions of the character are fear and anxiety.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=283): <vis_desc>In the video, it is impossible to determine the emotional state of the character. In the opening scene, we see a female character with a furrowed brow and slightly downturned mouth, as if she is experiencing an unpleasant emotional state. As time goes on, the female character's expression becomes more intense, with a downward and squinted mouth, indicating a sudden shift in her emotional state to agony. Throughout the subsequent scenes, she slowly moves closer to the camera, with a furrowed brow and tightly screwed lips, reflecting an intense and painful emotion.</vis_desc>
<aud_desc>In the audio, based on the emotional clues of the character's furrowed brow and tightly screwed lips, as well as the crying sound, we can infer that the character is experiencing intense pain or grief. In the text, the subtitle says, "I'm not doing that for her." This sentence may be the female character's response to the other person.</aud_desc>
<think>Based on the facial expressions and movements of the female character in the video clues, with a furrowed brow and tightly screwed lips, as well as the crying sound, we can infer that this sentence may carry a sense of pain or sadness. The female character may be expressing this sentence because the other person is wrong or unfair, triggering her distress and pain.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8343.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4958.mp4
[DEBUG] Step 250
[DEBUG] prompt_completion_ids shape: torch.Size([32, 310])
[DEBUG] Shortest output (len=131): <vis_desc>In the video, the screen shows a male character in a dimly lit indoor environment. In the video, his eyes are wide open and his facial expression appears very surprised. He looks directly at the camera, with his mouth slightly open, seemingly speaking or responding to someone.</vis_desc>
<aud_desc>In the audio, there are obvious sobbing sounds and the character's voice is shaking. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the character is experiencing surprise.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=310): <aud_desc>In the audio, the tone is aggressive and intimidating. Combined with the text content, it seems to carry a sense of warning. The subtitle in the text says, "The Federal Blunder After Another!" This sentence may be a threat or warning from the middle character to the other person.</aud_desc>
<vis_desc>In the video, we see a confrontational scene between two characters in a dim environment, possibly a fighting gym. The character on the left has his back to the camera, while the character on the right is facing the camera. We mainly analyze the emotional state of the character on the right. In the video, he furrows his brow and slightly opens his mouth, indicating that he is engaged in a heated argument with the other person on the right side. His facial expression is fierce, suggesting that some intense emotions have overwhelmed him. Although the specific actions of the character on left are not visible, it can be inferred that he may be engaged in a forceful attack or threat to the character on the right.</vis_desc>
<think>Based on the facial expressions and body language of the character on the right in the video, he is exhibiting fierce emotions and a heated argument. At the same time, the audio clue describes the tone as aggressive and threatening, with a sense of warning. Therefore, this sentence may carry a sense of threat or warning, indicating that the character on right may be threatening the other person on the right side with his existence.</think>
<answer>angry</answer>
[2025-11-07 05:49:35,394] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 251/2088 [3:40:02<26:17:17, 51.52s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9841035666335445, 'learning_rate': 8.797892720306513e-07, 'completion_length': 230.8046875, 'rewards/av_format_reward': 0.61328125, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.4450417757034302, 'reward': 1.5348855257034302, 'reward_std': 0.49318352341651917, 'kl': 0.018798828125, 'epoch': 0.24}
 12%|█▏        | 251/2088 [3:40:02<26:17:17, 51.52s/it][2025-11-07 05:50:30,994] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 252/2088 [3:40:58<26:53:54, 52.74s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.985789410051302, 'learning_rate': 8.793103448275862e-07, 'completion_length': 234.35546875, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5102399438619614, 'reward': 1.7719587683677673, 'reward_std': 0.6066785156726837, 'kl': 0.0201416015625, 'epoch': 0.24}
 12%|█▏        | 252/2088 [3:40:58<26:53:54, 52.74s/it][2025-11-07 05:51:20,854] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 253/2088 [3:41:48<26:26:34, 51.88s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.936477273659145, 'learning_rate': 8.788314176245211e-07, 'completion_length': 239.94921875, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.42763713002204895, 'reward': 1.8065434098243713, 'reward_std': 0.453012615442276, 'kl': 0.019287109375, 'epoch': 0.24}
 12%|█▏        | 253/2088 [3:41:48<26:26:34, 51.88s/it][2025-11-07 05:52:12,691] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 254/2088 [3:42:40<26:25:21, 51.87s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0520966800343947, 'learning_rate': 8.783524904214559e-07, 'completion_length': 236.8203125, 'rewards/av_format_reward': 0.88671875, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.48091521859169006, 'reward': 1.8715401887893677, 'reward_std': 0.5936176776885986, 'kl': 0.01995849609375, 'epoch': 0.24}
 12%|█▏        | 254/2088 [3:42:40<26:25:21, 51.87s/it][2025-11-07 05:53:03,477] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 255/2088 [3:43:31<26:14:35, 51.54s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0928816288619654, 'learning_rate': 8.778735632183908e-07, 'completion_length': 229.71875, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5278151333332062, 'reward': 1.9770339131355286, 'reward_std': 0.4826001822948456, 'kl': 0.021240234375, 'epoch': 0.24}
 12%|█▏        | 255/2088 [3:43:31<26:14:35, 51.54s/it][2025-11-07 05:53:56,014] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 256/2088 [3:44:23<26:22:50, 51.84s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0299030021014155, 'learning_rate': 8.773946360153256e-07, 'completion_length': 230.16015625, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.4728383421897888, 'reward': 1.777525782585144, 'reward_std': 0.4068418890237808, 'kl': 0.0198974609375, 'epoch': 0.25}
 12%|█▏        | 256/2088 [3:44:23<26:22:50, 51.84s/it][2025-11-07 05:54:47,496] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 257/2088 [3:45:15<26:18:42, 51.73s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0115710886434, 'learning_rate': 8.769157088122605e-07, 'completion_length': 232.203125, 'rewards/av_format_reward': 0.921875, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.4989195317029953, 'reward': 2.0223571062088013, 'reward_std': 0.5540156066417694, 'kl': 0.0203857421875, 'epoch': 0.25}
 12%|█▏        | 257/2088 [3:45:15<26:18:42, 51.73s/it][2025-11-07 05:55:38,421] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 258/2088 [3:46:06<26:10:28, 51.49s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.095809077116484, 'learning_rate': 8.764367816091953e-07, 'completion_length': 232.546875, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.543177604675293, 'reward': 1.9494275450706482, 'reward_std': 0.5757183730602264, 'kl': 0.0208740234375, 'epoch': 0.25}
 12%|█▏        | 258/2088 [3:46:06<26:10:28, 51.49s/it][2025-11-07 05:56:31,443] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 259/2088 [3:46:59<26:23:36, 51.95s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0477966134511734, 'learning_rate': 8.759578544061302e-07, 'completion_length': 227.72265625, 'rewards/av_format_reward': 0.5078125, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.6340216398239136, 'reward': 1.7746465802192688, 'reward_std': 0.6607517898082733, 'kl': 0.0189208984375, 'epoch': 0.25}
 12%|█▏        | 259/2088 [3:46:59<26:23:36, 51.95s/it][2025-11-07 05:57:21,593] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▏        | 260/2088 [3:47:49<26:06:17, 51.41s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.034988362327969, 'learning_rate': 8.754789272030651e-07, 'completion_length': 229.85546875, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.45158883929252625, 'reward': 1.7992451190948486, 'reward_std': 0.6667588949203491, 'kl': 0.0206298828125, 'epoch': 0.25}
 12%|█▏        | 260/2088 [3:47:49<26:06:17, 51.41s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04026.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7754.mp4
[DEBUG] Step 260
[DEBUG] prompt_completion_ids shape: torch.Size([32, 303])
[DEBUG] Shortest output (len=204): <vis_desc>In the video, the screen shows a male character in a dimly lit bar. In the video, his facial expression is calm, with a neutral gaze, and his mouth is moving, seemingly engaged in a serious and earnest conversation with someone.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads: "I couldn't attend to your rally because it's not part of the current strategy." This sentence may be the male character's response to someone or something outside the frame.</aud_desc>
<think>Based on the neutral gaze and serious expression of the male character in the video clues, as well as the neutral tone described in the audio clue, we can infer that this sentence may carry a serious or firm tone. Therefore, the male character may be explaining or warning the other person about something that caused his inability to go to the rally, while also emphasizing the importance of a certain strategy.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=303): <vis_desc>In the video, the screen shows a male character in what appears to be a bar setting. At the beginning of the video, his facial expression is calm and serious. His eyes are looking down and his mouth is moving, seemingly engaged in a serious conversation with someone off-screen. As the video progresses to the middle part, he continues to look down, with no obvious change in facial expression or mouth movement, indicating that he is attentively listening to the other person.</vis_desc>
<aud_desc>In the audio, the tone is neutral. Combined with the text content, it seems that the character is discussing something that is not welcome or beneficial to the company at this moment. This sentence may carry a sense of blame or discontent. In the text, the subtitle reads: "I cannot have had my own race dealt to me because it is not part of the current strategy." This sentence may be the male character's response to something or someone.</aud_desc>
<think>Based on the video clues of the male character's calm and serious facial expressions, as well as his downward eye contact and movement of the mouth, it can be inferred that he is attentively listening to the other person and expressing his dissatisfaction or blame towards them. At the same time, the neutral tone described in the audio also indicates a sense of blame or discontent. Therefore, this sentence can be understood as the male character expressing his dissatisfaction and blame towards someone or something.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11828.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8382.mp4
[DEBUG] Step 260
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=183): <vis_desc>In the video, we see a woman in a military uniform with large eyes and a smile, seemingly expressing happiness.</vis_desc>
<aud_desc>In the audio, the voice is gentle. Combined with the text content, it feels like the character is apologizing to someone. In the text, the subtitle says, "For my forgetting, my dad is guilty. My dad is guilty." This sentence may be the woman expressing her remorse and guilt.</aud_desc>
<think>Based on the joy displayed by the woman in the video and the description of the character's gentle voice and apology tone in the audio, we can infer that this sentence is the woman expressing her apologies to the person next to her. The woman's joyful emotions and the description of the character's gentle voice and apology tone together indicate that she truly feels guilty and remorseful.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see a lady wearing a Japanese-style hat and military uniform, standing in an indoor environment with windows and flowers in the background. There is a man standing next to her, carrying a gift box for the lady. We mainly analyze the emotional state of the lady. At the beginning of the video, her facial expression is smiling, with the corner of her mouth upturned and her eyes slightly squinted, which usually indicates a pleasant or positive emotional state. At the same time, as time goes on, she accompanies her speech with head movements, which often indicate that a conversation is active, and usually accompanies a smiling emotional state. Based on these, we can infer that the lady in the video is in a positive emotional state throughout the process, possibly discussing a pleasant topic with the man next to her. Overall, this video is likely discussing a positive topic, and the lady in the video generally maintains a positive and smiling emotional state.</vis_desc>
<aud_desc>In the audio, the character's voice is cheerful and sounds very pleased to see the lady again. In the text, the subtitle says, "You, huh? My dad is crazy." This sentence may be a comment or reaction from the lady towards the man next to her.</aud_desc>
<think>Based on the lady's smiling and happy emotional state in the video clues, as well as the overall positive mood described in the audio clues, we can infer that this sentence is expressed in a teasing or funny tone, possibly referring to the lady's dad as "my dad is crazy." This sentence may be a playful or sarcastic remark, aligned with the overall positive mood displayed by the lady.</think>
<answer>happy</answer
[2025-11-07 05:58:13,213] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 12%|█▎        | 261/2088 [3:48:40<26:07:21, 51.47s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0274208730141767, 'learning_rate': 8.75e-07, 'completion_length': 233.31640625, 'rewards/av_format_reward': 0.63671875, 'rewards/accuracy_reward': 0.3984375, 'rewards/think_step_av_with_neutral_reward': 0.3551655262708664, 'reward': 1.3903217315673828, 'reward_std': 0.5342144966125488, 'kl': 0.02154541015625, 'epoch': 0.25}
 12%|█▎        | 261/2088 [3:48:40<26:07:21, 51.47s/it][2025-11-07 05:59:07,721] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 262/2088 [3:49:35<26:34:12, 52.38s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0022986749896785, 'learning_rate': 8.745210727969349e-07, 'completion_length': 223.59765625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5136420726776123, 'reward': 1.943329632282257, 'reward_std': 0.6165568679571152, 'kl': 0.0213623046875, 'epoch': 0.25}
 13%|█▎        | 262/2088 [3:49:35<26:34:12, 52.38s/it][mpeg4 @ 0xad18e100] ac-tex damaged at 55 15
[mpeg4 @ 0xad18e100] Error at MB: 1270
[aac @ 0xad18e100] Number of bands (65) exceeds limit (43).
[05:59:08] /github/workspace/src/audio/audio_reader.cc:189: ERROR Fail to send packet.
[2025-11-07 06:00:01,140] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 263/2088 [3:50:28<26:42:46, 52.69s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0768816823160443, 'learning_rate': 8.740421455938696e-07, 'completion_length': 227.93359375, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.4896016865968704, 'reward': 1.8567891716957092, 'reward_std': 0.5779400765895844, 'kl': 0.02093505859375, 'epoch': 0.25}
 13%|█▎        | 263/2088 [3:50:28<26:42:46, 52.69s/it][2025-11-07 06:00:55,824] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 264/2088 [3:51:23<27:00:02, 53.29s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.011769359301039, 'learning_rate': 8.735632183908046e-07, 'completion_length': 241.95703125, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.4140625, 'rewards/think_step_av_with_neutral_reward': 0.3557898998260498, 'reward': 1.5120400190353394, 'reward_std': 0.6318081021308899, 'kl': 0.02069091796875, 'epoch': 0.25}
 13%|█▎        | 264/2088 [3:51:23<27:00:02, 53.29s/it][2025-11-07 06:01:51,505] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 265/2088 [3:52:19<27:20:56, 54.01s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9319038520294993, 'learning_rate': 8.730842911877394e-07, 'completion_length': 243.3046875, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.66015625, 'rewards/think_step_av_with_neutral_reward': 0.5443154573440552, 'reward': 1.9193154573440552, 'reward_std': 0.5911389291286469, 'kl': 0.019287109375, 'epoch': 0.25}
 13%|█▎        | 265/2088 [3:52:19<27:20:56, 54.01s/it][2025-11-07 06:02:45,854] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 266/2088 [3:53:13<27:23:09, 54.11s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9771872185725992, 'learning_rate': 8.726053639846743e-07, 'completion_length': 235.6875, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5723906457424164, 'reward': 2.045046806335449, 'reward_std': 0.5012611895799637, 'kl': 0.02020263671875, 'epoch': 0.25}
 13%|█▎        | 266/2088 [3:53:13<27:23:09, 54.11s/it][2025-11-07 06:03:36,712] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 267/2088 [3:54:04<26:52:38, 53.13s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9072458189059995, 'learning_rate': 8.721264367816091e-07, 'completion_length': 225.43359375, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.4082636088132858, 'reward': 1.6738885641098022, 'reward_std': 0.5013922899961472, 'kl': 0.021240234375, 'epoch': 0.26}
 13%|█▎        | 267/2088 [3:54:04<26:52:38, 53.13s/it][2025-11-07 06:04:28,429] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 268/2088 [3:54:56<26:38:51, 52.71s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9580391328871924, 'learning_rate': 8.716475095785441e-07, 'completion_length': 236.34375, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4942491352558136, 'reward': 1.658311665058136, 'reward_std': 0.45698246359825134, 'kl': 0.0203857421875, 'epoch': 0.26}
 13%|█▎        | 268/2088 [3:54:56<26:38:51, 52.71s/it][2025-11-07 06:05:21,613] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 269/2088 [3:55:49<26:42:17, 52.85s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.017779005359706, 'learning_rate': 8.711685823754789e-07, 'completion_length': 237.46875, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.5183695256710052, 'reward': 1.9246194958686829, 'reward_std': 0.5229551047086716, 'kl': 0.0223388671875, 'epoch': 0.26}
 13%|█▎        | 269/2088 [3:55:49<26:42:17, 52.85s/it][2025-11-07 06:06:15,232] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 270/2088 [3:56:42<26:48:22, 53.08s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.09068020341746, 'learning_rate': 8.706896551724138e-07, 'completion_length': 234.93359375, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5467721521854401, 'reward': 1.8788034915924072, 'reward_std': 0.4926663339138031, 'kl': 0.02099609375, 'epoch': 0.26}
 13%|█▎        | 270/2088 [3:56:42<26:48:22, 53.08s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00415.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3712.mp4
[DEBUG] Step 270
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=203): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned corners of the mouth, tears streaming down her face. Her gaze seems to be fixed on the other person, and her facial expression appears very sad and pained.</vis_desc>
<aud_desc>In the audio, the tone is low and conveys a sense of sadness and despair. In the text, the subtitle reads, "If only I had shot the person apart, I would have been accepted by the Angels." This sentence may be the female character expressing her remorse and self-blame for her past actions.</aud_desc>
<think>Based on the distorted gaze and sad facial expression of the female character in the video clues, as well as the low and sad-worth tone in the audio clues, we can infer that this sentence carries a sense of sadness and despair. The female character may be recalling her past actions and expressing her regret and pain.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, there are two female characters shown. The woman on the left is wearing a chef's uniform and hat, while the woman on the right is wearing a nurse's uniform and hat. We mainly analyze the emotional state of the woman on the left. Initially, she is talking to the other person, with her mouth slightly upturned and her facial expression relatively calm. In the following scenes, her expression becomes relaxed, with the corners of her mouth turned up a little, indicating a slight smile. Her head slightly nods, suggesting she is actively participating in the conversation.</vis_desc>
<aud_desc>In the audio, the tone used by the character is serious. When she speaks, there is a sense of importance and urgency in her words, indicating that she is quite anxious about the matter she is discussing. In the text, the subtitle says, "Okay, anyway, you didn't think it was necessary to involve this matter with Zhang Xiaoshi." This sentence may be a suggestion or refusal from the woman on the left to the woman on the right.</aud_desc>
<think>Based on the woman on the left's expressions and actions in the video clues, she initially appears calm but later turns her head and nods, indicating her participation in the conversation. At the same time, her smile suggests that she is happy with the development of the conversation. However, at the same time, she is also discussing a certain matter with Zhang Xiaoshi, which may make her feel anxious. Therefore, she appears to be expressing a compromise or accepting the other person's suggestion or request, but at the same time, she is also expressing her anxiety through her behavior and expression.</think>
<answer>an
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5197.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01784.mp4
[DEBUG] Step 270
[DEBUG] prompt_completion_ids shape: torch.Size([32, 320])
[DEBUG] Shortest output (len=174): <aud_desc>In the audio, there is a long pause at the beginning of the speech, after which the voice sounds calm and relaxed. In the text, the subtitle says, "Are you seriously invited to visit me?" This sentence may be the lady's inquiry or refusal to someone.</aud_desc>
<vis_desc>In the video, we see a lady with a slight upward curve at the corner of her mouth, a slight upward curve at the bottom of her mouth, and soft eye movements, which may indicate a pleasant mood.</vis_desc>
<think>Based on these observations, it can be inferred that the lady may have experienced a pleasant unexpected situation, which triggered her smiling and cheerful voice. She realizes after thinking that this situation is very unusual, so her response is humorous and consistent with her expressed mood.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=320): <vis_desc>In the video, a woman is sitting across from a man. We mainly analyze the emotional state of the lady. In the video, her facial expression is quite friendly, with a slight upward curve at the corners of her mouth and slightly parted lips, showing a relaxed and pleasant conversation. She is holding a drink in one hand and a chopstick in the other hand. The drink has not been used yet, so she is contemplating whether to drink from now on or not. The scene appears to be indoors, possibly in a comfortable dining environment.</vis_desc>
<aud_desc>In the audio, there does not seem to be any obvious emotional change in the character. In the text, the subtitle says, "Okay, I'll leave now." This sentence may be an expression of relaxation or acceptance by the lady after some time of conversation or contemplation.</aud_desc>
<think>Based on the friendly facial expression and relaxed tone of the lady in the video clues, as well as the scene inside the restaurant without a menu, we can infer that this sentence may be the lady commenting on a good restaurant service or something related to the previous conversation. Due to the lack of more emotional clues and background information, it is difficult to determine whether this emotional state is due to satisfaction with the restaurant service or not. However, based on the phrase "Okay, I'll leave now," it can be inferred that the lady is possibly indicating her end of the conversation or response to a question, which usually accompanies a sense of relaxation and acceptance.</think>
<answer>happy</answer>
[2025-11-07 06:07:08,137] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 271/2088 [3:57:35<26:45:54, 53.03s/it]                                                       {'loss': 0.0008, 'grad_norm': 1.9416917675302134, 'learning_rate': 8.702107279693485e-07, 'completion_length': 233.765625, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.44928669929504395, 'reward': 1.714911699295044, 'reward_std': 0.45991015434265137, 'kl': 0.020751953125, 'epoch': 0.26}
 13%|█▎        | 271/2088 [3:57:35<26:45:54, 53.03s/it][2025-11-07 06:07:57,672] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 272/2088 [3:58:25<26:13:16, 51.98s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0004026076402024, 'learning_rate': 8.697318007662835e-07, 'completion_length': 235.98046875, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.48457981646060944, 'reward': 1.7619234919548035, 'reward_std': 0.5565599799156189, 'kl': 0.020751953125, 'epoch': 0.26}
 13%|█▎        | 272/2088 [3:58:25<26:13:16, 51.98s/it][2025-11-07 06:08:50,838] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 273/2088 [3:59:18<26:23:10, 52.34s/it]                                                       {'loss': 0.001, 'grad_norm': 2.327806514313388, 'learning_rate': 8.692528735632183e-07, 'completion_length': 237.2265625, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.6640625, 'rewards/think_step_av_with_neutral_reward': 0.5423789769411087, 'reward': 1.9290977120399475, 'reward_std': 0.4857780337333679, 'kl': 0.02484130859375, 'epoch': 0.26}
 13%|█▎        | 273/2088 [3:59:18<26:23:10, 52.34s/it][2025-11-07 06:09:48,716] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 274/2088 [4:00:16<27:12:33, 54.00s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.09154736132835, 'learning_rate': 8.687739463601532e-07, 'completion_length': 237.58984375, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.43025436997413635, 'reward': 1.617754340171814, 'reward_std': 0.46420884132385254, 'kl': 0.020263671875, 'epoch': 0.26}
 13%|█▎        | 274/2088 [4:00:16<27:12:33, 54.00s/it][2025-11-07 06:10:43,603] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 275/2088 [4:01:11<27:19:42, 54.27s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.076875763087252, 'learning_rate': 8.682950191570881e-07, 'completion_length': 232.91015625, 'rewards/av_format_reward': 0.63671875, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5694246888160706, 'reward': 1.8584871888160706, 'reward_std': 0.4974961578845978, 'kl': 0.02154541015625, 'epoch': 0.26}
 13%|█▎        | 275/2088 [4:01:11<27:19:42, 54.27s/it][2025-11-07 06:11:33,113] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 276/2088 [4:02:00<26:35:45, 52.84s/it]                                                       {'loss': 0.0016, 'grad_norm': 32.42923579449595, 'learning_rate': 8.67816091954023e-07, 'completion_length': 232.24609375, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.3203125, 'rewards/think_step_av_with_neutral_reward': 0.2934058755636215, 'reward': 1.3871558904647827, 'reward_std': 0.5454640686511993, 'kl': 0.03997802734375, 'epoch': 0.26}
 13%|█▎        | 276/2088 [4:02:00<26:35:45, 52.84s/it][2025-11-07 06:12:28,611] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 277/2088 [4:02:56<26:58:55, 53.64s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.8921411109749355, 'learning_rate': 8.673371647509579e-07, 'completion_length': 230.76953125, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.3810783624649048, 'reward': 1.6467034220695496, 'reward_std': 0.4747796952724457, 'kl': 0.02325439453125, 'epoch': 0.27}
 13%|█▎        | 277/2088 [4:02:56<26:58:55, 53.64s/it][2025-11-07 06:13:22,257] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 278/2088 [4:03:49<26:58:06, 53.64s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.0530260355558645, 'learning_rate': 8.668582375478927e-07, 'completion_length': 227.8984375, 'rewards/av_format_reward': 0.95703125, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5520095229148865, 'reward': 2.161384344100952, 'reward_std': 0.5024970471858978, 'kl': 0.02099609375, 'epoch': 0.27}
 13%|█▎        | 278/2088 [4:03:49<26:58:06, 53.64s/it][2025-11-07 06:14:14,508] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 279/2088 [4:04:42<26:44:40, 53.22s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0982076955075204, 'learning_rate': 8.663793103448276e-07, 'completion_length': 226.0625, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.4720470607280731, 'reward': 1.7337658405303955, 'reward_std': 0.533111497759819, 'kl': 0.02294921875, 'epoch': 0.27}
 13%|█▎        | 279/2088 [4:04:42<26:44:40, 53.22s/it][2025-11-07 06:15:04,499] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 280/2088 [4:05:32<26:14:33, 52.25s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0333780913178328, 'learning_rate': 8.659003831417624e-07, 'completion_length': 226.84375, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4535791724920273, 'reward': 1.7231104373931885, 'reward_std': 0.5470536649227142, 'kl': 0.0234375, 'epoch': 0.27}
 13%|█▎        | 280/2088 [4:05:32<26:14:33, 52.25s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8090.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01871.mp4
[DEBUG] Step 280
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=143): <vis_desc>In the video, we see a male character making head movements, seemingly emphasizing his viewpoint or opinion.</vis_desc>
<aud_desc>In the audio, the laughter can be felt, indicating that the character's mood is very pleasant. In the text, the subtitle says, "She's coming." This sentence may be the male character's suggestion or request.</aud_desc>
<think>Based on the video clue of the male character making head movements and the audio clue of the laughter, we can infer that this sentence may carry a sense of excitement or joy. The male character may be anticipating something pleasant that happens, hence expressing his mood with laughter.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=302): <vis_desc>In the video, we see a woman with a neutral expression, her eyes are not looking at the camera, and her mouth is moving slightly, indicating that she is engaged in a conversation with a man who is not in the frame on the left side. We mainly analyze the emotional state of the woman. In the video, her gaze does not look at the camera, but instead, she looks at the man on the right side and seems a bit angry, as if expressing dissatisfaction or impatience.</vis_desc>
<aud_desc>In the audio, the character's tone is aggressive, and the phrase "one hit" combined with "murder case" expresses a very negative emotion. In the text, the subtitle reads: "The guy can't talk fast enough. He didn't sign on any contract for murder, one case, last night." This sentence expresses the woman's dissatisfaction and anger towards the man.</aud_desc>
<think>Based on theangry expression and posture of the woman in the video, as well as the aggressive tone and extremely negative emotion in the audio, we can infer that this sentence carries a sense of anger and dissatisfaction. The woman's gaze does not look at the camera, but instead, she looks at the man on the right side, implying that he is expressing his dissatisfaction or impatience towards him. Overall, this sentence expresses the woman's disappointment and anger towards the guy's behavior.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1488.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2454.mp4
[DEBUG] Step 280
[DEBUG] prompt_completion_ids shape: torch.Size([32, 333])
[DEBUG] Shortest output (len=169): <vis_desc>In the video, we see a woman standing outdoors. Her eyes are wide open and she is looking directly at someone behind her. Her mouth is slightly open, showing a surprised or confused facial expression.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle reads, "I hate working onGit." This sentence may be the woman's evaluation or reaction to a certain topic or idea.</aud_desc>
<think>Based on the surprised or confused facial expression of the woman in the video clues, as well as the description in the subtitle, we can infer that this sentence may carry a sense of dissatisfaction or anger. The woman may be unhappy with a certain topic or idea and expresses her feelings in this sentence.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=333): <vis_desc>In the video, the screen shows a young woman lying on a bed in a dimly lit bedroom. At the beginning of the video, her facial expression is relatively calm. In the following scenes, her facial expression undergoes significant changes. Her eyes are wide open, she tightly grips her hair in her hands, and her facial expression appears very scared and painful.</vis_desc>
<aud_desc>In the audio, the voice is low and weak, with a timid and timid mood. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The multimodal cues from both the visual and auditory channels together strongly indicate that the woman is experiencing fear. Visually, her initially relaxed smile shifts dramatically as she grips her hair, indicating a sudden shift to unease and fear. This behavior is consistent with feelings of apprehension and anxiety that are common in individuals experiencing fear. The dimly lit bedroom accentuates a somber atmosphere, reinforcing a sense of vulnerability that aligns with fear. Audibly, her low and weak voice conveys a heavy emotional burden, while the suggestion of a timid mood through a description of a timid tone and a cautious tone further reinforces a fearful state. Although the subtitle content does not provide emotional clues, the synchronization of her visual expression, vocal tone, and contextual dimness strongly points to fear as the underlying emotional state. Together, these multimodal signals create a coherent picture of a person who is visceral and deeply affected by fear, evident not only through her facial appearance but also through her body language and vocal demeanor.</think>
<answer>fear</answer>
[2025-11-07 06:15:57,346] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 13%|█▎        | 281/2088 [4:06:24<26:19:03, 52.43s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0324307959496135, 'learning_rate': 8.654214559386973e-07, 'completion_length': 239.3828125, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.4040784537792206, 'reward': 1.5993909239768982, 'reward_std': 0.520347535610199, 'kl': 0.02301025390625, 'epoch': 0.27}
 13%|█▎        | 281/2088 [4:06:24<26:19:03, 52.43s/it][2025-11-07 06:16:48,690] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 282/2088 [4:07:16<26:08:25, 52.11s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.099399374049339, 'learning_rate': 8.649425287356321e-07, 'completion_length': 228.2578125, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.48975273966789246, 'reward': 1.86865895986557, 'reward_std': 0.5941231846809387, 'kl': 0.02227783203125, 'epoch': 0.27}
 14%|█▎        | 282/2088 [4:07:16<26:08:25, 52.11s/it][2025-11-07 06:17:43,880] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 283/2088 [4:08:11<26:35:19, 53.03s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.103025282017259, 'learning_rate': 8.64463601532567e-07, 'completion_length': 227.640625, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.46805089712142944, 'reward': 1.8235197067260742, 'reward_std': 0.4580846279859543, 'kl': 0.02239990234375, 'epoch': 0.27}
 14%|█▎        | 283/2088 [4:08:11<26:35:19, 53.03s/it][2025-11-07 06:18:37,420] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 284/2088 [4:09:04<26:39:02, 53.18s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9910105358619279, 'learning_rate': 8.639846743295019e-07, 'completion_length': 224.890625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.4793942719697952, 'reward': 1.8465818166732788, 'reward_std': 0.4749807268381119, 'kl': 0.02276611328125, 'epoch': 0.27}
 14%|█▎        | 284/2088 [4:09:05<26:39:02, 53.18s/it][2025-11-07 06:19:29,984] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 285/2088 [4:09:57<26:32:34, 53.00s/it]                                                       {'loss': 0.0008, 'grad_norm': 2.137016526819395, 'learning_rate': 8.635057471264368e-07, 'completion_length': 232.3125, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.75390625, 'rewards/think_step_av_with_neutral_reward': 0.6750178933143616, 'reward': 2.0890803933143616, 'reward_std': 0.3670806437730789, 'kl': 0.020751953125, 'epoch': 0.27}
 14%|█▎        | 285/2088 [4:09:57<26:32:34, 53.00s/it][2025-11-07 06:20:21,453] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 286/2088 [4:10:49<26:17:55, 52.54s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.001090137399018, 'learning_rate': 8.630268199233716e-07, 'completion_length': 236.8046875, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5610999763011932, 'reward': 1.9048500061035156, 'reward_std': 0.44581764936447144, 'kl': 0.02276611328125, 'epoch': 0.27}
 14%|█▎        | 286/2088 [4:10:49<26:17:55, 52.54s/it][2025-11-07 06:21:18,201] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▎        | 287/2088 [4:11:45<26:54:56, 53.80s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0498030740708373, 'learning_rate': 8.625478927203065e-07, 'completion_length': 236.80078125, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5671067386865616, 'reward': 2.004606783390045, 'reward_std': 0.5301807522773743, 'kl': 0.0245361328125, 'epoch': 0.27}
 14%|█▎        | 287/2088 [4:11:45<26:54:56, 53.80s/it][2025-11-07 06:22:10,484] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 288/2088 [4:12:38<26:40:22, 53.35s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0909763450476078, 'learning_rate': 8.620689655172412e-07, 'completion_length': 226.40625, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.43834666907787323, 'reward': 1.7117841839790344, 'reward_std': 0.5652843415737152, 'kl': 0.0233154296875, 'epoch': 0.28}
 14%|█▍        | 288/2088 [4:12:38<26:40:22, 53.35s/it][2025-11-07 06:23:04,480] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 289/2088 [4:13:32<26:45:20, 53.54s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0116910373389794, 'learning_rate': 8.615900383141762e-07, 'completion_length': 232.984375, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5136875212192535, 'reward': 1.9668124318122864, 'reward_std': 0.5129672735929489, 'kl': 0.0230712890625, 'epoch': 0.28}
 14%|█▍        | 289/2088 [4:13:32<26:45:20, 53.54s/it][2025-11-07 06:23:54,844] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 290/2088 [4:14:22<26:15:52, 52.59s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9849448676080852, 'learning_rate': 8.611111111111111e-07, 'completion_length': 232.421875, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.5002433657646179, 'reward': 1.730712115764618, 'reward_std': 0.5718799531459808, 'kl': 0.0233154296875, 'epoch': 0.28}
 14%|█▍        | 290/2088 [4:14:22<26:15:52, 52.59s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02378.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/13107.mp4
[DEBUG] Step 290
[DEBUG] prompt_completion_ids shape: torch.Size([32, 292])
[DEBUG] Shortest output (len=188): <vis_desc>In the video, we see a woman with a downward-eyed expression, furrowed brows, and slightly parted lips, seemingly expressing anger or frustration.</vis_desc>
<aud_desc>In the audio, the tone is sharp and the intonation rises when expressing "são Shanshan," highlighting the other person's attitude. In the text, the subtitle says, "What are you talking about! Did you forget to bring my Epi-pen card? Shut the hell up!" This sentence may be the woman's accusation or complaint to another person.</aud_desc>
<think>Based on the angry or frustrated expression of the woman in the video clues, as well as the sharp intonation and rising tone in the audio clues, we can infer that this sentence carries a sense of anger and dissatisfaction. The woman may feel ignored or unresponsive, hence expressing such words.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=292): <vis_desc>In the video, we see a woman with furrowed brows and slightly downturned corners of the mouth, tears streaming down her face. Her gaze is fixed on the other person, and her facial expression appears serious and sad.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues provide a clear and compelling indication of the target emotion. Visually, the woman’s furrowed brows and downturned mouth corners, along with the presence of tears, clearly convey a sense of sadness and emotional distress. These facial expressions are compelling indicators of distress and sorrow, as people often express sadness when encountering difficult situations or experiencing an unfavorable outcome. The serious and sad gaze she maintains on the other person further supports this interpretation, as such eye contact often accompanies feelings of heaviness or melancholy. Furthermore, the absence of significant vocal or textual emotional cues in this case means that the visual signals alone strongly evoke sadness. Complementing these observations, the auditory modality offers no additional context to disturb the interpretation. The auditory channel primarily generates tone and intonation, with subtitle content offering no clear emotional signals. Therefore, integrating both modalities points toward the coherent conclusion that the target emotion being expressed is sadness.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8360.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00219.mp4
[DEBUG] Step 290
[DEBUG] prompt_completion_ids shape: torch.Size([32, 346])
[DEBUG] Shortest output (len=202): <aud_desc>In the audio, the character's voice sounds surprised when saying "She could not find it with her teeth, she could not find it with her teeth." This sentence may imply that the boy found something strange but did not understand, and was surprised by the other person's behavior or words.</aud_desc>
<vis_desc>In the video, we see a young girl in a dim environment, with her head slightly lowered and looking downwards. Her facial expression is somewhat melancholic, indicating that she may be reminiscing or contemplating something.</vis_desc>
<think>Based on the video clues of the girl's melancholic facial expression and lowered head, as well as the audio clue of the character's surprised tone when saying the sentence, we can infer that this sentence may carry a sense of surprise or curiosity. The girl may feel surprised by the other person's strange behavior or behavior, and this sentence expresses her feelings of curiosity and astonishment.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=346): <vis_desc>In the video, the screen shows a young female character sitting on the floor in a bedroom. At the beginning of the video, she frowns and slightly turns her head to the right side. In the following scenes, she sits up and looks at someone off-screen, showing a surprised facial expression. Her mouth is wide open, indicating that she may have heard some unexpected news. Overall, she seems to be experiencing a pleasant conversation, with positive emotions.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very scared, with a stuttering sound, possibly because she has just fallen into the water. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The young female character’s visual cues—frown, turn head, surprised expression, and wide-open mouth suggesting an unexpected and sudden reaction—strongly indicate a state of surprise, as if she just learned something unexpected. The fact that she appears to have just sat up and is looking at someone suggests she is reacting to an unforeseen event or stimulus outside the frame. Complementing this, the auditory information, particularly her voice trembling and stuttering, reinforces the interpretation that she is in a moment of shock or sudden anxiety. Although the subtitles do not clarify her emotional state verbally, the combined facial and vocal expressions present a clear and unmistakable sign of surprise triggered by an unpleasant unexpected situation. Together, these multimodal signals—from her anxious facial expression and worried vocal tone to the specific effect of trembling voice and stuttering—cohesively support the conclusion that the character’s predominant emotion during this moment is surprise.</think>
<answer>surprise</answer>
[2025-11-07 06:24:46,374] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 291/2088 [4:15:13<26:05:33, 52.27s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1719079344811516, 'learning_rate': 8.606321839080459e-07, 'completion_length': 225.33984375, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.37109375, 'rewards/think_step_av_with_neutral_reward': 0.33727285265922546, 'reward': 1.5599291324615479, 'reward_std': 0.3997874855995178, 'kl': 0.02587890625, 'epoch': 0.28}
 14%|█▍        | 291/2088 [4:15:13<26:05:33, 52.27s/it][2025-11-07 06:25:40,298] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 292/2088 [4:16:07<26:19:28, 52.77s/it]                                                       {'loss': 0.001, 'grad_norm': 2.097326205881503, 'learning_rate': 8.601532567049809e-07, 'completion_length': 226.265625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.6010270416736603, 'reward': 1.991652011871338, 'reward_std': 0.5274237990379333, 'kl': 0.02392578125, 'epoch': 0.28}
 14%|█▍        | 292/2088 [4:16:07<26:19:28, 52.77s/it][2025-11-07 06:26:32,860] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 293/2088 [4:17:00<26:16:45, 52.70s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0295826034237496, 'learning_rate': 8.596743295019157e-07, 'completion_length': 238.5078125, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5395602434873581, 'reward': 1.8364351987838745, 'reward_std': 0.498199462890625, 'kl': 0.0234375, 'epoch': 0.28}
 14%|█▍        | 293/2088 [4:17:00<26:16:45, 52.70s/it][mpeg4 @ 0xad64ac40] P cbpy damaged at 34 12
[mpeg4 @ 0xad64ac40] Error at MB: 1006
[2025-11-07 06:27:28,589] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 294/2088 [4:17:56<26:42:59, 53.61s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.106142798941958, 'learning_rate': 8.591954022988506e-07, 'completion_length': 226.51171875, 'rewards/av_format_reward': 0.89453125, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.33581967651844025, 'reward': 1.620975911617279, 'reward_std': 0.4331990256905556, 'kl': 0.022705078125, 'epoch': 0.28}
 14%|█▍        | 294/2088 [4:17:56<26:42:59, 53.61s/it][2025-11-07 06:28:21,551] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 295/2088 [4:18:49<26:36:16, 53.42s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0594390748559905, 'learning_rate': 8.587164750957854e-07, 'completion_length': 227.17578125, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5308642983436584, 'reward': 1.8746142387390137, 'reward_std': 0.6384682655334473, 'kl': 0.0244140625, 'epoch': 0.28}
 14%|█▍        | 295/2088 [4:18:49<26:36:16, 53.42s/it][2025-11-07 06:29:08,794] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 296/2088 [4:19:36<25:40:04, 51.56s/it]                                                       {'loss': 0.001, 'grad_norm': 2.080688931077483, 'learning_rate': 8.582375478927202e-07, 'completion_length': 224.33984375, 'rewards/av_format_reward': 0.55859375, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.38619597256183624, 'reward': 1.335414707660675, 'reward_std': 0.5745262503623962, 'kl': 0.02410888671875, 'epoch': 0.28}
 14%|█▍        | 296/2088 [4:19:36<25:40:04, 51.56s/it][2025-11-07 06:30:02,333] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 297/2088 [4:20:29<25:56:53, 52.16s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0416202291647303, 'learning_rate': 8.577586206896551e-07, 'completion_length': 227.94140625, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.44663555920124054, 'reward': 1.5755417346954346, 'reward_std': 0.46445418894290924, 'kl': 0.02325439453125, 'epoch': 0.28}
 14%|█▍        | 297/2088 [4:20:29<25:56:53, 52.16s/it][2025-11-07 06:30:53,644] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 298/2088 [4:21:21<25:48:27, 51.90s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9874771234196171, 'learning_rate': 8.5727969348659e-07, 'completion_length': 229.0703125, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.47604474425315857, 'reward': 1.761201024055481, 'reward_std': 0.4912787973880768, 'kl': 0.02362060546875, 'epoch': 0.29}
 14%|█▍        | 298/2088 [4:21:21<25:48:27, 51.90s/it][2025-11-07 06:31:45,467] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 299/2088 [4:22:13<25:46:51, 51.88s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.021209920396731, 'learning_rate': 8.568007662835248e-07, 'completion_length': 227.3828125, 'rewards/av_format_reward': 0.8984375, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.49952469766139984, 'reward': 1.979993462562561, 'reward_std': 0.49415434896945953, 'kl': 0.023681640625, 'epoch': 0.29}
 14%|█▍        | 299/2088 [4:22:13<25:46:51, 51.88s/it][2025-11-07 06:32:40,012] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 300/2088 [4:23:07<26:09:49, 52.68s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.032727530107993, 'learning_rate': 8.563218390804597e-07, 'completion_length': 223.43359375, 'rewards/av_format_reward': 0.8671875, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.43341805040836334, 'reward': 1.7381054759025574, 'reward_std': 0.5107307732105255, 'kl': 0.02301025390625, 'epoch': 0.29}
 14%|█▍        | 300/2088 [4:23:07<26:09:49, 52.68s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03311.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9869.mp4
[DEBUG] Step 300
[DEBUG] prompt_completion_ids shape: torch.Size([32, 286])
[DEBUG] Shortest output (len=176): <vis_desc>In the video, we see a woman wearing a blue jacket with a white shirt, standing indoors. Her facial expression is neutral, her eyes are looking straight ahead, and she appears to be dressed in formal attire.</vis_desc>
<aud_desc>In the audio, the voice is neutral. In the text, the subtitle reads: "The Great Ming King's daughter's marriage proposal came to me." This sentence may be the woman introducing or reminiscing about a certain story or event.</aud_desc>
<think>Based on the neutral facial expression of the woman in the video and the neutral voice in the audio, we can infer that this sentence may be a recollection or reminiscence about a certain story or event. The woman may be expressing gratitude and happiness about this story to the people listening to her.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=286): <vis_desc>In the video, we see a male character with a dim background, but a light source can be seen through the perforated bars, giving him a somewhat calm and focused impression. His gaze seems to be focused on the surroundings or someone in particular, without direct eye contact. Overall, he is engaged in a conversation with a rather neutral emotional state.</vis_desc>
<aud_desc>In the audio, the entire voice is heavy and low, with a solemn tone. In the text, the subtitle says, "I don't know, were we happy, or were we just pigs?" This sentence may be the male character's questioning or puzzled expression.</aud_desc>
<think>Based on the video clues of the male character's calm and focused expression, as well as his gaze on the surroundings or someone in particular, we can infer that he is engaged in a serious conversation without strong emotional fluctuations. However, the specific emotional state needs to be analyzed in conjunction with additional context. Based on the audio clues of the character's low and solemn voice, as well as the heavy and low tone in the audio, we can speculate that this sentence may carry a heavy emotional burden, such as regret, question, or answer in the context of his previous actions or decisions. Due to the limited information provided so far, it is difficult to determine the emotional state more accurately.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7460.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04465.mp4
[DEBUG] Step 300
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=179): <vis_desc>In the video, we see a male character wearing a formal tie, standing outdoors with trees and light gray sky in the background. His facial expression appears serious and concerned.</vis_desc>
<aud_desc>In the audio, there is a long pause at the beginning of the speech, after which the voice is deep and steady. In the text, the subtitle says, "I'm helpless." This sentence may express the character's feelings of helplessness.</aud_desc>
<think>Based on the serious and concerned facial expression of the male character in the video clues, as well as the low and steady tone in the audio clues, we can infer that this sentence may carry a sense of helplessness. The male character may be expressing his own limitations or difficulties, which aligns with the emotional states described in the other options.</think>
<answer>anxiety</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the opening scene shows a young male character standing in front of a traditional building with a window and door, which may indicate that the character is in a cultural or religious place. At the beginning of the video, he furrows his brow and his facial expression is negative. His gaze seems to be focused downwards, possibly contemplating or recalling something. In the final scene, the male character opens his mouth, seemingly speaking or whispering something, before quickly returning to contemplating. Overall, the emotional state of the male character appears somewhat heavy throughout the video, possibly due to a topic that requires careful consideration or introspection.</vis_desc>
<aud_desc>In the audio, when expressing "my university," the character's tone rises and combined with the previous sentence, there is a sense of nostalgia and reminiscence. In the text, the subtitle reads: "My university, I will go there as soon as I grow up, and I will bring my friends there as well. It's somewhere in my heart." This sentence may describe the male character's aspirations or dreams.</aud_desc>
<think>Based on the changing facial expressions of the male character in the video clues, from furrowing his brow to looking downwards and then to speaking or whispering, as well as his initial heavy expression and final contemplative state, we can infer that this sentence may carry a sense of nostalgia and reminiscence. The rising tone and combined words indicate that the male character may be expressing a feeling of wanting to go back and reminisce about his university experiences. Overall, this sentence expresses the male character's strong feelings and pursuits, consistent with the negative emotions shown in the video.</think>
<answer>happy</answer>
[2025-11-07 06:33:44,314] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 301/2088 [4:24:11<27:52:48, 56.17s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9656837788619324, 'learning_rate': 8.558429118773946e-07, 'completion_length': 235.76171875, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.487087219953537, 'reward': 1.795681118965149, 'reward_std': 0.4914882183074951, 'kl': 0.0233154296875, 'epoch': 0.29}
 14%|█▍        | 301/2088 [4:24:11<27:52:48, 56.17s/it][2025-11-07 06:34:36,524] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 14%|█▍        | 302/2088 [4:25:04<27:16:33, 54.98s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0025435745327864, 'learning_rate': 8.553639846743295e-07, 'completion_length': 229.92578125, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.451944962143898, 'reward': 1.580851137638092, 'reward_std': 0.4519707262516022, 'kl': 0.02191162109375, 'epoch': 0.29}
 14%|█▍        | 302/2088 [4:25:04<27:16:33, 54.98s/it][2025-11-07 06:35:29,666] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 303/2088 [4:25:57<26:59:13, 54.43s/it]                                                       {'loss': 0.001, 'grad_norm': 4.401611304607371, 'learning_rate': 8.548850574712644e-07, 'completion_length': 235.28515625, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.48438557982444763, 'reward': 1.8671980500221252, 'reward_std': 0.5494069159030914, 'kl': 0.02423095703125, 'epoch': 0.29}
 15%|█▍        | 303/2088 [4:25:57<26:59:13, 54.43s/it][2025-11-07 06:36:23,787] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 304/2088 [4:26:51<26:55:37, 54.34s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.1325094356843977, 'learning_rate': 8.544061302681991e-07, 'completion_length': 228.4375, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5885486900806427, 'reward': 1.963548719882965, 'reward_std': 0.43215717375278473, 'kl': 0.02313232421875, 'epoch': 0.29}
 15%|█▍        | 304/2088 [4:26:51<26:55:37, 54.34s/it][2025-11-07 06:37:15,934] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 305/2088 [4:27:43<26:35:08, 53.68s/it]                                                       {'loss': 0.001, 'grad_norm': 2.019536023046384, 'learning_rate': 8.539272030651341e-07, 'completion_length': 228.98046875, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.68359375, 'rewards/think_step_av_with_neutral_reward': 0.5861088633537292, 'reward': 2.078296422958374, 'reward_std': 0.38708817958831787, 'kl': 0.02392578125, 'epoch': 0.29}
 15%|█▍        | 305/2088 [4:27:43<26:35:08, 53.68s/it][2025-11-07 06:38:11,151] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 306/2088 [4:28:38<26:47:57, 54.14s/it]                                                       {'loss': 0.001, 'grad_norm': 2.099932958370356, 'learning_rate': 8.534482758620689e-07, 'completion_length': 231.7890625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5249969065189362, 'reward': 1.9351531863212585, 'reward_std': 0.5274190604686737, 'kl': 0.02484130859375, 'epoch': 0.29}
 15%|█▍        | 306/2088 [4:28:38<26:47:57, 54.14s/it][2025-11-07 06:39:02,984] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 307/2088 [4:29:30<26:26:30, 53.45s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.147854454496269, 'learning_rate': 8.529693486590038e-07, 'completion_length': 225.33203125, 'rewards/av_format_reward': 0.60546875, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.37275393307209015, 'reward': 1.372753918170929, 'reward_std': 0.5750585198402405, 'kl': 0.0233154296875, 'epoch': 0.29}
 15%|█▍        | 307/2088 [4:29:30<26:26:30, 53.45s/it][2025-11-07 06:39:53,832] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 308/2088 [4:30:21<26:02:29, 52.67s/it]                                                       {'loss': 0.001, 'grad_norm': 2.075618110222233, 'learning_rate': 8.524904214559386e-07, 'completion_length': 223.27734375, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5490008890628815, 'reward': 1.9552509188652039, 'reward_std': 0.5293536186218262, 'kl': 0.0247802734375, 'epoch': 0.3}
 15%|█▍        | 308/2088 [4:30:21<26:02:29, 52.67s/it][2025-11-07 06:40:42,569] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 309/2088 [4:31:10<25:26:38, 51.49s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9868772873058713, 'learning_rate': 8.520114942528736e-07, 'completion_length': 226.6171875, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4465845972299576, 'reward': 1.8098658323287964, 'reward_std': 0.49859611690044403, 'kl': 0.023681640625, 'epoch': 0.3}
 15%|█▍        | 309/2088 [4:31:10<25:26:38, 51.49s/it][2025-11-07 06:41:38,310] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 310/2088 [4:32:05<26:03:35, 52.76s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1072306180450493, 'learning_rate': 8.515325670498084e-07, 'completion_length': 230.23046875, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.43521885573863983, 'reward': 1.7203750610351562, 'reward_std': 0.5540556013584137, 'kl': 0.0240478515625, 'epoch': 0.3}
 15%|█▍        | 310/2088 [4:32:05<26:03:35, 52.76s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12723.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04991.mp4
[DEBUG] Step 310
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=178): <vis_desc>In the video, we see a man standing in a kitchen setting. His facial expression is smiling, his eyes slightly squinted, and the corners of his mouth turned up. This indicates that he may be feeling happy or relaxed.</vis_desc>
<aud_desc>In the audio, the character's voice is soft and soothing, with a calm and positive voice. In the text, the subtitle says, "Okay." This sentence may be the man's response to a suggestion or request.</aud_desc>
<think>Based on the man's smiling facial expression and relaxed body language in the video, as well as the description of the character's soft and soothing voice in the audio, we can infer that this sentence may carry a positive meaning, and the man may be happy to have reached an agreement with the other person.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, at the beginning of the video, we see a person standing in front of a shelf with various items. His facial expression is smiling, his mouth is upturned, and his finger points have folded fingers. This kind of hand gesture usually indicates confidence in the person's opinion or statement. As time goes on, the character's mouth is open, indicating that he is speaking. In the final part of the video, we see the character's smile on his face and his expression looks very happy, indicating that the content of his speech has captured the audience's attention and brought him pleasure.</vis_desc>
<aud_desc>In the audio, the overall tone is steady. In the text, the subtitle says, "The more walls, the better sellings you make," indicating the importance of increasing house values alongside adding more walls. In the text, the subtitle says, "We sell our outer walls, buy our houses, sell your neighborhoods, and丑陋中。 This sentence expresses the character's unrealistic expectation and expectation for fast money." From the subtitle, we can infer that the character has aspirations and expectations for the sale of a property, and these expectations go beyond what is generally considered reasonable or possible.</aud_desc>
<think>Based on the smiling facial expression, open mouth, and finger points shown by the character in the video, it can be inferred that he is speaking and expressing an opinion or expectation in this sentence. The tone of the character's voice is steady, and the overall tone is positive. From the subtitle content, it can be inferred that the character's goals and expectations for the house sale are excessive and unrealistic. He hopes to be noticed and praised for his expectations, expressing dissatisfaction and negative
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12787.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09953.mp4
[DEBUG] Step 310
[DEBUG] prompt_completion_ids shape: torch.Size([32, 328])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, the screen shows a woman using her phone to make a call. She is wearing winter clothing and her facial expression appears focused and engaged in the conversation.</vis_desc>
<aud_desc>In the audio, the tone is calm and conveys a sense of ease. In the text, the subtitle reads: "It should be easy to research it, Assistant." This sentence may be the lady's suggestion or response to a question.</aud_desc>
<think>Based on the dress code, winter clothing, and focused facial expression in the video, as well as the calm tone in the audio, we can infer that this sentence may have a neutral or helpful tone. The lady may be providing guidance or responding to a question about a certain topic, so her expression and tone are calm.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=328): <vis_desc>In the video, we see a female character with slightly widened eyeshadows, slightly furrowed brows, slightly parted lips, and slightly open eyes, all of which indicate that she is possibly experiencing an anxious emotion. She appears to be in a social gathering, as suggested by the blurry figures in the background. Her mouth is moving, seemingly engaged in a conversation, and her eyes are fixed on a certain place, possibly looking for something or waiting for a response.</vis_desc>
<aud_desc>In the audio, the character's voice sounds scared. Combined with the text content, it can be inferred that the character is afraid that something bad will happen in the video. The subtitle in the text says, "Stop immediately! The person next to you is lost and needs to be watched carefully." This sentence may be a call to action from the female character to others in the video, asking them to stop quickly and watch over the person next to her.</aud_desc>
<think>Based on the facial expressions and posture of the female character in the video clues, her slightly widened eyeshadows, furrowed brows, parted lips, and fixed gaze on a certain place all indicate that she is possibly experiencing an anxious emotion. She appears to be in a social gathering, as suggested by the blurry figures in the background. Combined with the voice clues describing the character as scared, we can infer that this sentence is likely a way to emphasize the importance of following the person next to her for safety, which aligns with the overall anxious emotion displayed by the female character.</think>
<answer>fear</answer>
[2025-11-07 06:42:33,445] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 311/2088 [4:33:01<26:23:47, 53.48s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0313363913248312, 'learning_rate': 8.510536398467433e-07, 'completion_length': 237.23828125, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.48617905378341675, 'reward': 1.7322728633880615, 'reward_std': 0.4954414665699005, 'kl': 0.0213623046875, 'epoch': 0.3}
 15%|█▍        | 311/2088 [4:33:01<26:23:47, 53.48s/it][2025-11-07 06:43:21,890] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 312/2088 [4:33:49<25:38:12, 51.97s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0833966683031475, 'learning_rate': 8.50574712643678e-07, 'completion_length': 230.7734375, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4305158257484436, 'reward': 1.7586407661437988, 'reward_std': 0.5303688049316406, 'kl': 0.0224609375, 'epoch': 0.3}
 15%|█▍        | 312/2088 [4:33:49<25:38:12, 51.97s/it][2025-11-07 06:44:14,012] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▍        | 313/2088 [4:34:41<25:38:42, 52.01s/it]                                                       {'loss': 0.001, 'grad_norm': 2.123973877357612, 'learning_rate': 8.50095785440613e-07, 'completion_length': 231.546875, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.3620772063732147, 'reward': 1.592545986175537, 'reward_std': 0.484286293387413, 'kl': 0.0244140625, 'epoch': 0.3}
 15%|█▍        | 313/2088 [4:34:41<25:38:42, 52.01s/it][2025-11-07 06:45:09,677] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 314/2088 [4:35:37<26:10:14, 53.11s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0797222930399424, 'learning_rate': 8.496168582375478e-07, 'completion_length': 237.1875, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.487072616815567, 'reward': 1.9050413370132446, 'reward_std': 0.5603861510753632, 'kl': 0.0238037109375, 'epoch': 0.3}
 15%|█▌        | 314/2088 [4:35:37<26:10:14, 53.11s/it][2025-11-07 06:46:05,946] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 315/2088 [4:36:33<26:37:22, 54.06s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9522286716663766, 'learning_rate': 8.491379310344827e-07, 'completion_length': 223.8828125, 'rewards/av_format_reward': 0.875, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.49956752359867096, 'reward': 2.0230050086975098, 'reward_std': 0.49391700327396393, 'kl': 0.02301025390625, 'epoch': 0.3}
 15%|█▌        | 315/2088 [4:36:33<26:37:22, 54.06s/it][2025-11-07 06:46:56,023] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 316/2088 [4:37:23<26:01:13, 52.86s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9435265519571352, 'learning_rate': 8.486590038314176e-07, 'completion_length': 225.5625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.42584478855133057, 'reward': 1.6914698481559753, 'reward_std': 0.45151324570178986, 'kl': 0.0224609375, 'epoch': 0.3}
 15%|█▌        | 316/2088 [4:37:23<26:01:13, 52.86s/it][2025-11-07 06:47:46,530] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 317/2088 [4:38:14<25:39:28, 52.16s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.1309520368554407, 'learning_rate': 8.481800766283525e-07, 'completion_length': 225.56640625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.47246095538139343, 'reward': 1.6990234851837158, 'reward_std': 0.623955488204956, 'kl': 0.0230712890625, 'epoch': 0.3}
 15%|█▌        | 317/2088 [4:38:14<25:39:28, 52.16s/it][2025-11-07 06:48:40,456] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 318/2088 [4:39:08<25:54:16, 52.69s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.057614161162656, 'learning_rate': 8.477011494252874e-07, 'completion_length': 226.44921875, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5516250431537628, 'reward': 2.04771888256073, 'reward_std': 0.4512396603822708, 'kl': 0.02374267578125, 'epoch': 0.3}
 15%|█▌        | 318/2088 [4:39:08<25:54:16, 52.69s/it][2025-11-07 06:49:34,279] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 319/2088 [4:40:01<26:03:26, 53.03s/it]                                                       {'loss': 0.001, 'grad_norm': 2.171108223312978, 'learning_rate': 8.472222222222222e-07, 'completion_length': 235.0859375, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.78125, 'rewards/think_step_av_with_neutral_reward': 0.6250748336315155, 'reward': 2.2539811730384827, 'reward_std': 0.43896910548210144, 'kl': 0.02374267578125, 'epoch': 0.31}
 15%|█▌        | 319/2088 [4:40:01<26:03:26, 53.03s/it][2025-11-07 06:50:28,871] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 320/2088 [4:40:56<26:16:22, 53.50s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.139188666914475, 'learning_rate': 8.46743295019157e-07, 'completion_length': 230.71875, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4489080160856247, 'reward': 1.7965642809867859, 'reward_std': 0.6369600892066956, 'kl': 0.0234375, 'epoch': 0.31}
 15%|█▌        | 320/2088 [4:40:56<26:16:22, 53.50s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07137.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9671.mp4
[DEBUG] Step 320
[DEBUG] prompt_completion_ids shape: torch.Size([32, 349])
[DEBUG] Shortest output (len=166): <vis_desc>In the video, we see a person sitting in an indoor environment with a clock and furniture in the background. Their nose is furrowed and their eyes are wide open, showing a surprised expression.</vis_desc>
<aud_desc>In the audio, footsteps can be heard approaching. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Based on the surprised expression of the person in the video clues, the open eyes and furrowed nose, as well as the sound of footsteps approach in the audio clues, we can infer that this sentence may carry a sense of surprise or uncertainty. Due to the lack of more context information, it is difficult to determine the exact emotional state of the person.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=349): <vis_desc>In the video, the scene shows a male wearing a Fire Rescue Team uniform. In the video, his facial expression is relatively negative, with furrowed brows and downturned corners of the mouth, indicating a serious and negative emotional state. His gaze is fixed on the other person, suggesting that he is engaged in a conversation with that person. There are vehicles in the background, such as a ambulance and a patrol car, possibly indicating that this conversation is taking place in a public place or during a public event. Overall, this scene may be encountering a non-player character and needs to be interpreted in conjunction with additional context.</vis_desc>
<aud_desc>In the audio, the character's voice is sharp and loud, with a stern and aggressive tone. Combined with the text content, it seems that the character is accusing the other person. In the text, the subtitle says, "What's your issue with the Qinite? How's it ironic that you ask about the Qinite?" This sentence may be a question or criticism from the male in charge to the person next to him.</aud_desc>
<think>Based on the negative facial expressions, fixed gaze on the other person, and sharp, loud voice described in the video, it can be inferred that the male in charge may have some anger or accusation towards the person next to him. Additionally, based on the description of the sharp, loud, stern, and aggressive tone in the audio, it can be inferred that the male in charge may have a stern and aggressive attitude towards the person next to him. Therefore, this sentence expresses the male in charge's dissatisfaction and accusation, with a stern and aggressive tone.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11684.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08953.mp4
[DEBUG] Step 320
[DEBUG] prompt_completion_ids shape: torch.Size([32, 290])
[DEBUG] Shortest output (len=176): <aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "What if he wins? It'll be cool, right?" This sentence may be the woman's comment or reaction to someone or something.</aud_desc>
<vis_desc>In the video, we see a woman with a slight smile at the corner of her mouth, looking directly at the other person, possibly engaged in a relaxed conversation.</vis_desc>
<think>Based on the woman's smile and direct eye contact in the video, as well as the neutral tone of the audio, we can infer that this sentence may carry a playful or optimistic emotion. The woman may be commenting on an unlikely or amusing thing to happen, which aligns with the overall relaxed and happy mood displayed by the woman.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=290): <aud_desc>In the audio, the character's voice is urgent and filled with a sense of tension, indicating that the character is likely to have encountered some kind of emergency or danger and is anxious about it. In the text, the subtitle reads, "Kagetan, is this the place you're looking for? If you leave now, I won't be able to catch you." This sentence is likely the male character's warning or reminder to someone on the sofa next to him.</aud_desc>
<vis_desc>In the video, we see a male character wearing a black suit and a tie, with his back to an audience. He seems to be engaged in a conversation with a person on the sofa next to him. His facial expression appears surprised, as if he is seeing the other person for the first time.</vis_desc>
<think>Based on the surprised facial expression of the male character in the video clues, as well as the description of the character's urgent and tense voice in the audio clues, we can infer that this sentence expresses the male character's fear and anxiety. He may have been lost and scared on the sofa, fearing that the other person will also end up in a similar situation, which could lead to danger. Therefore, this sentence is likely the male character's way of reminding the other person to be cautious and waiting for further instructions or advice from someone.</think>
<answer>fear</answer>
[2025-11-07 06:51:20,785] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 321/2088 [4:41:48<26:01:30, 53.02s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0562701374595598, 'learning_rate': 8.462643678160919e-07, 'completion_length': 232.9453125, 'rewards/av_format_reward': 0.62890625, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5098400413990021, 'reward': 1.7364025712013245, 'reward_std': 0.5079949647188187, 'kl': 0.02313232421875, 'epoch': 0.31}
 15%|█▌        | 321/2088 [4:41:48<26:01:30, 53.02s/it][2025-11-07 06:52:10,979] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 322/2088 [4:42:38<25:35:39, 52.17s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0237286389786737, 'learning_rate': 8.457854406130268e-07, 'completion_length': 224.8984375, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.40625, 'rewards/think_step_av_with_neutral_reward': 0.38772885501384735, 'reward': 1.5478851795196533, 'reward_std': 0.41550326347351074, 'kl': 0.02252197265625, 'epoch': 0.31}
 15%|█▌        | 322/2088 [4:42:38<25:35:39, 52.17s/it][2025-11-07 06:53:02,018] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 15%|█▌        | 323/2088 [4:43:29<25:24:45, 51.83s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0468379194140365, 'learning_rate': 8.453065134099616e-07, 'completion_length': 222.14453125, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.41796875, 'rewards/think_step_av_with_neutral_reward': 0.39185889065265656, 'reward': 1.5442025661468506, 'reward_std': 0.47157350182533264, 'kl': 0.02252197265625, 'epoch': 0.31}
 15%|█▌        | 323/2088 [4:43:29<25:24:45, 51.83s/it][2025-11-07 06:53:56,053] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 324/2088 [4:44:23<25:43:21, 52.50s/it]                                                       {'loss': 0.001, 'grad_norm': 1.7747435964614515, 'learning_rate': 8.448275862068965e-07, 'completion_length': 231.31640625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.43844638764858246, 'reward': 1.7118839025497437, 'reward_std': 0.4103245884180069, 'kl': 0.02435302734375, 'epoch': 0.31}
 16%|█▌        | 324/2088 [4:44:23<25:43:21, 52.50s/it][2025-11-07 06:54:49,443] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 325/2088 [4:45:17<25:50:19, 52.76s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0915352673600514, 'learning_rate': 8.443486590038314e-07, 'completion_length': 226.09765625, 'rewards/av_format_reward': 0.6328125, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.43758058547973633, 'reward': 1.5000805854797363, 'reward_std': 0.5919806361198425, 'kl': 0.02587890625, 'epoch': 0.31}
 16%|█▌        | 325/2088 [4:45:17<25:50:19, 52.76s/it][2025-11-07 06:55:39,687] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 326/2088 [4:46:07<25:27:15, 52.01s/it]                                                       {'loss': 0.001, 'grad_norm': 2.072103057292159, 'learning_rate': 8.438697318007663e-07, 'completion_length': 231.375, 'rewards/av_format_reward': 0.484375, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.5175873041152954, 'reward': 1.5488373041152954, 'reward_std': 0.4403443932533264, 'kl': 0.02490234375, 'epoch': 0.31}
 16%|█▌        | 326/2088 [4:46:07<25:27:15, 52.01s/it][2025-11-07 06:56:32,066] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 327/2088 [4:46:59<25:29:40, 52.12s/it]                                                       {'loss': 0.001, 'grad_norm': 1.909274150850732, 'learning_rate': 8.433908045977011e-07, 'completion_length': 234.8203125, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5380364656448364, 'reward': 1.9755364656448364, 'reward_std': 0.5385025590658188, 'kl': 0.02392578125, 'epoch': 0.31}
 16%|█▌        | 327/2088 [4:46:59<25:29:40, 52.12s/it][2025-11-07 06:57:23,718] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 328/2088 [4:47:51<25:24:44, 51.98s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1422322562790543, 'learning_rate': 8.429118773946359e-07, 'completion_length': 224.703125, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.45209936797618866, 'reward': 1.5380369424819946, 'reward_std': 0.5549054443836212, 'kl': 0.0245361328125, 'epoch': 0.31}
 16%|█▌        | 328/2088 [4:47:51<25:24:44, 51.98s/it][2025-11-07 06:58:18,828] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 329/2088 [4:48:46<25:51:22, 52.92s/it]                                                       {'loss': 0.001, 'grad_norm': 2.150071726643205, 'learning_rate': 8.424329501915708e-07, 'completion_length': 230.98046875, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.40730777382850647, 'reward': 1.665120244026184, 'reward_std': 0.5960521101951599, 'kl': 0.0245361328125, 'epoch': 0.32}
 16%|█▌        | 329/2088 [4:48:46<25:51:22, 52.92s/it][2025-11-07 06:59:10,165] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 330/2088 [4:49:37<25:36:34, 52.44s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0827328695143996, 'learning_rate': 8.419540229885057e-07, 'completion_length': 226.63671875, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.69921875, 'rewards/think_step_av_with_neutral_reward': 0.5513798296451569, 'reward': 2.160754919052124, 'reward_std': 0.4911392629146576, 'kl': 0.02325439453125, 'epoch': 0.32}
 16%|█▌        | 330/2088 [4:49:37<25:36:34, 52.44s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12498.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3340.mp4
[DEBUG] Step 330
[DEBUG] prompt_completion_ids shape: torch.Size([32, 320])
[DEBUG] Shortest output (len=121): <vis_desc>In the video, a woman is sitting in a bathtub filled with water. Her eyebrows are slightly raised, her eyes are wide open, and she stares directly at the water, showing a surprised or scared expression.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The woman's facial expression, combined with her body language, strongly conveys a sense of surprise.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=320): <vis_desc>In the video, the screen shows a woman in a bathtub filled with water. At the beginning of the video, her mouth is open, eyes half-closed, and she appears to be experiencing some kind of emotion. She stares directly at the water, showing a somewhat scared facial expression. In the following scenes, her head slowly moves back to the surface of the water, her eyes maintain the expression of fear, and her mouth always trembles, indicating that she is not able to control her trembling. Overall, we can infer that the woman in the video may be experiencing some kind of stress or discomfort, which has triggered her fear.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the woman is experiencing fear. Visually, her open mouth, half-closed eyes, and direct gaze into the water at the start suggest an initial state of unease or nervousness. The subsequent movement to the surface of the water and trembling of her mouth convey a heightened level of distress and anxiety. Although the auditory information and subtitle content do not provide clear emotional indicators, the dramatic visual shifts and tense facial expressions vividly convey a fear response. Together, the multimodal evidence—primarily the visual expressions coupled with the uncertainty stemming from disorienting auditory cues—supports the conclusion that the target emotion being expressed is fear.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07569.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04704.mp4
[DEBUG] Step 330
[DEBUG] prompt_completion_ids shape: torch.Size([32, 315])
[DEBUG] Shortest output (len=171): <vis_desc>In the video, the screen shows a woman wearing traditional Chinese clothing, with her facial expression appearing somewhat angry. Her gaze is direct, her mouth slightly lifted, and her eyes wide open, showing a strong reaction to the dialogue or event occurring.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle says, "I am your father." This sentence is likely said by the woman during the dialogue.</aud_desc>
<think>Based on these scenes, it can be inferred that the woman in the video is likely in a state of anger or dissatisfaction when saying this sentence. Her facial expression and tone both indicate that she is very angry, which aligns with the behavior of the person wearing traditional Chinese clothing in the video.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=315): <vis_desc>In the video, the scene shows a woman standing indoors. In the video, she has her back to the television. Her hands are raised on the left side of the frame, and her facial expression is smiling. It can be inferred that she may be speaking to the person opposite her or engaging in a friendly conversation. Her body posture is somewhat relaxed, which may indicate that she is in a pleasant mood. Overall, she seems to be interacting with someone in a relaxed and pleasant atmosphere.</vis_desc>
<aud_desc>In the audio, the voice starts off weak and slow, with noticeable pauses between sentences. Combined with the text content, it feels forced and awkward, as if the person is trying too hard to express their opinion. In the text, the subtitle reads: "I can't understand it, we can't understand it, because you cannot understand it." This sentence may be the woman expressing her confusion about a certain topic or phenomenon.</aud_desc>
<think>Based on the woman's expressions of smiling and relaxation in the video clues, as well as the accusatory tone and strong emphasis in the audio clues, we can infer that this sentence carries a sarcastic or mocking tone. The woman may feel puzzled by the other person's opinion or statement, but persists in expressing her viewpoint, possibly because she believes in her own way or believes in her own opinion. Therefore, this sentence may carry a sarcastic or mocking tone, consistent with the overall relaxed and pleasant atmosphere displayed by the woman.</think>
<answer>happy</answer>
[2025-11-07 07:00:02,303] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 331/2088 [4:50:29<25:33:04, 52.35s/it]                                                       {'loss': 0.001, 'grad_norm': 1.9895345974777805, 'learning_rate': 8.414750957854406e-07, 'completion_length': 233.65234375, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.4327476769685745, 'reward': 1.835091471672058, 'reward_std': 0.5499757528305054, 'kl': 0.0245361328125, 'epoch': 0.32}
 16%|█▌        | 331/2088 [4:50:29<25:33:04, 52.35s/it][2025-11-07 07:00:55,220] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 332/2088 [4:51:22<25:37:05, 52.52s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9293473457055015, 'learning_rate': 8.409961685823754e-07, 'completion_length': 234.3203125, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.44932907819747925, 'reward': 1.8360477685928345, 'reward_std': 0.39619284868240356, 'kl': 0.0235595703125, 'epoch': 0.32}
 16%|█▌        | 332/2088 [4:51:22<25:37:05, 52.52s/it][2025-11-07 07:01:48,686] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 333/2088 [4:52:16<25:44:31, 52.80s/it]                                                       {'loss': 0.001, 'grad_norm': 1.9975864351626613, 'learning_rate': 8.405172413793104e-07, 'completion_length': 226.6171875, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.5246506631374359, 'reward': 1.8957445621490479, 'reward_std': 0.5593096613883972, 'kl': 0.0244140625, 'epoch': 0.32}
 16%|█▌        | 333/2088 [4:52:16<25:44:31, 52.80s/it][2025-11-07 07:02:40,566] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 334/2088 [4:53:08<25:35:32, 52.53s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0439264959852266, 'learning_rate': 8.400383141762452e-07, 'completion_length': 225.77734375, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.49529701471328735, 'reward': 1.7765470743179321, 'reward_std': 0.535975456237793, 'kl': 0.0228271484375, 'epoch': 0.32}
 16%|█▌        | 334/2088 [4:53:08<25:35:32, 52.53s/it][2025-11-07 07:03:31,982] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 335/2088 [4:53:59<25:24:58, 52.20s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.109181776267762, 'learning_rate': 8.395593869731801e-07, 'completion_length': 230.8515625, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5482107400894165, 'reward': 1.8528982400894165, 'reward_std': 0.5076424479484558, 'kl': 0.02313232421875, 'epoch': 0.32}
 16%|█▌        | 335/2088 [4:53:59<25:24:58, 52.20s/it][mpeg4 @ 0x91404740] Error at MB: 3546
[mpeg4 @ 0x91404740] ac-tex damaged at 3 4
[mpeg4 @ 0x91404740] Error at MB: 327
[2025-11-07 07:04:20,813] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 336/2088 [4:54:48<24:54:35, 51.18s/it]                                                       {'loss': 0.0009, 'grad_norm': 2.0971292850635517, 'learning_rate': 8.390804597701148e-07, 'completion_length': 222.93359375, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5179206877946854, 'reward': 1.935889482498169, 'reward_std': 0.4113473892211914, 'kl': 0.023681640625, 'epoch': 0.32}
 16%|█▌        | 336/2088 [4:54:48<24:54:35, 51.18s/it][2025-11-07 07:05:16,211] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 337/2088 [4:55:43<25:30:38, 52.45s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0451925084714984, 'learning_rate': 8.386015325670498e-07, 'completion_length': 233.72265625, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.52700075507164, 'reward': 1.9840320348739624, 'reward_std': 0.5104807615280151, 'kl': 0.02655029296875, 'epoch': 0.32}
 16%|█▌        | 337/2088 [4:55:43<25:30:38, 52.45s/it][2025-11-07 07:06:06,401] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 338/2088 [4:56:33<25:09:58, 51.77s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.087179802847197, 'learning_rate': 8.381226053639846e-07, 'completion_length': 223.28125, 'rewards/av_format_reward': 0.62109375, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.6055014133453369, 'reward': 1.8594077825546265, 'reward_std': 0.5591337531805038, 'kl': 0.0263671875, 'epoch': 0.32}
 16%|█▌        | 338/2088 [4:56:33<25:09:58, 51.77s/it][2025-11-07 07:07:00,095] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▌        | 339/2088 [4:57:27<25:25:56, 52.35s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0745868433051085, 'learning_rate': 8.376436781609195e-07, 'completion_length': 225.5859375, 'rewards/av_format_reward': 0.82421875, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.4591369777917862, 'reward': 1.787261962890625, 'reward_std': 0.6467389762401581, 'kl': 0.02752685546875, 'epoch': 0.32}
 16%|█▌        | 339/2088 [4:57:27<25:25:56, 52.35s/it][2025-11-07 07:07:51,745] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 340/2088 [4:58:19<25:18:58, 52.14s/it]                                                       {'loss': 0.001, 'grad_norm': 2.011691639177844, 'learning_rate': 8.371647509578543e-07, 'completion_length': 234.125, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.3818846493959427, 'reward': 1.5537596940994263, 'reward_std': 0.493939146399498, 'kl': 0.02471923828125, 'epoch': 0.33}
 16%|█▋        | 340/2088 [4:58:19<25:18:58, 52.14s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07970.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05875.mp4
[DEBUG] Step 340
[DEBUG] prompt_completion_ids shape: torch.Size([32, 320])
[DEBUG] Shortest output (len=182): <vis_desc>In the video, we see a female character with a slight upward curve at the corners of her mouth, wearing a slight smile. Her eyes are direct, suggesting that she may be expressing a positive emotion, such as gratitude or satisfaction.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads, "It's all part of the whole world." This sentence may be the female character's response to someone or something.</aud_desc>
<think>Based on the video clues of the female character's smile and direct gaze, as well as the neutral tone in the audio, we can infer that this sentence may have a relaxed or humorous tone. Therefore, this sentence may be used to express the female character's gratitude or satisfaction, which aligns with the positive emotions displayed by the female character.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=320): <vis_desc>In the video, the screen shows a female character whose facial expression appears sad and pained. Her eyes are not looking at the camera, but instead, she seems to see the ground below her and drinks deeply from the water that is flowing. Her mouth is slightly open, but she speaks with a sob, and her body moves with a certain rhythm, as if she is crying out loud. Taking all these pieces of information into consideration, we can infer that the emotional state of the character in this video is sadness and pain.</vis_desc>
<aud_desc>In the audio, the character's voice sounds extremely sorrowful, with a trembling tone. Overall, the speech is monotonous. In the text, the subtitle says, "Did your parents fight? That's dad's, you don't have the skills." This sentence may be a question or accusation from the female character towards the other person.</aud_desc>
<think>Based on the description of the female character in the video, her facial expression is sad and pained, with tears flowing from her eyes and her body moving with a certain rhythm, as if she is crying out loud. Based on the description of the character's voice in the audio, which is described as sorrowful, trembling, and with a trembling tone, we can infer that the character's emotional state in this sentence is very sad and painful. Therefore, this sentence may be a statement about the other person's skills or features, expressing the female character's disappointment and sadness towards them.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3581.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1474.mp4
[DEBUG] Step 340
[DEBUG] prompt_completion_ids shape: torch.Size([32, 312])
[DEBUG] Shortest output (len=154): <vis_desc>In the video, we see a character with a gun to his head, expressing a very aggressive and threatening facial expression.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle says, "My lady is killed." This sentence may be spoken by the male character when expressing his feelings or intentions.</aud_desc>
<think>Based on the video clue of the male character's aggressive and threatening facial expression, as well as the audio clue of a stern and aggressive tone, we can infer that this sentence expresses the male character's anger and determination. He may be expressing his desire to kill his lady to convey a sense of anger and threat.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=312): <vis_desc>In the video, we see two characters facing each other. The character on the left is looking directly at the other person, while the character on the right has his back to the camera. We mainly analyze the emotional state of the character on the left. In the video, his mouth is wide open and his facial expression appears serious and focused. His eyes are fixed on the other person, suggesting that he is engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the character's voice is deep and the tone rises at the end of the sentence "Now it's your turn." This may indicate the character's evaluation of the other person's skill level or performance. In the text, the subtitle reads, "Now it's your turn." This sentence may be the character on the left asking the other person how good the person below him is at something.</aud_desc>
<think>Based on the serious and focused facial expression of the character on the left in the video, as well as his eye contact with the other person, it can be inferred that he is engaged in a serious conversation and his evaluation of the other person's skill level or performance is focused and sincere. Additionally, the description of the low voice and deep tone in the audio further supports this inference. Therefore, this sentence may be the character on the left asking the other person how good the person below him is at something, while expressing his evaluation of the other person's skill level or performance.</think>
<answer>angry</answer>
[2025-11-07 07:08:42,647] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 341/2088 [4:59:10<25:07:17, 51.77s/it]                                                       {'loss': 0.001, 'grad_norm': 2.032404188772428, 'learning_rate': 8.366858237547893e-07, 'completion_length': 227.69140625, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5019918829202652, 'reward': 2.005898118019104, 'reward_std': 0.3578020930290222, 'kl': 0.02587890625, 'epoch': 0.33}
 16%|█▋        | 341/2088 [4:59:10<25:07:17, 51.77s/it][2025-11-07 07:09:33,934] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 342/2088 [5:00:01<25:02:14, 51.62s/it]                                                       {'loss': 0.001, 'grad_norm': 2.083834697086354, 'learning_rate': 8.362068965517241e-07, 'completion_length': 219.99609375, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.3422824740409851, 'reward': 1.545407474040985, 'reward_std': 0.4679323136806488, 'kl': 0.026123046875, 'epoch': 0.33}
 16%|█▋        | 342/2088 [5:00:01<25:02:14, 51.62s/it][2025-11-07 07:10:26,857] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 343/2088 [5:00:54<25:12:43, 52.01s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1185565873653642, 'learning_rate': 8.35727969348659e-07, 'completion_length': 231.31640625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4794348329305649, 'reward': 1.830997347831726, 'reward_std': 0.5364529192447662, 'kl': 0.0252685546875, 'epoch': 0.33}
 16%|█▋        | 343/2088 [5:00:54<25:12:43, 52.01s/it][2025-11-07 07:11:18,017] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 16%|█▋        | 344/2088 [5:01:45<25:04:24, 51.76s/it]                                                       {'loss': 0.001, 'grad_norm': 2.004223487430161, 'learning_rate': 8.352490421455939e-07, 'completion_length': 218.234375, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.5646332204341888, 'reward': 1.8693206906318665, 'reward_std': 0.5265990793704987, 'kl': 0.0244140625, 'epoch': 0.33}
 16%|█▋        | 344/2088 [5:01:45<25:04:24, 51.76s/it][2025-11-07 07:12:08,248] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 345/2088 [5:02:35<24:50:15, 51.30s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.1141540347062917, 'learning_rate': 8.347701149425287e-07, 'completion_length': 229.1328125, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.66796875, 'rewards/think_step_av_with_neutral_reward': 0.5225116908550262, 'reward': 2.0381367206573486, 'reward_std': 0.5061483830213547, 'kl': 0.02642822265625, 'epoch': 0.33}
 17%|█▋        | 345/2088 [5:02:35<24:50:15, 51.30s/it][2025-11-07 07:12:57,373] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 346/2088 [5:03:24<24:30:26, 50.65s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.1118176300247438, 'learning_rate': 8.342911877394636e-07, 'completion_length': 225.07421875, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.3985447585582733, 'reward': 1.5430760383605957, 'reward_std': 0.4651077389717102, 'kl': 0.02642822265625, 'epoch': 0.33}
 17%|█▋        | 346/2088 [5:03:24<24:30:26, 50.65s/it][2025-11-07 07:13:51,184] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 347/2088 [5:04:18<24:57:09, 51.60s/it]                                                       {'loss': 0.001, 'grad_norm': 1.8768899716038983, 'learning_rate': 8.338122605363984e-07, 'completion_length': 231.66015625, 'rewards/av_format_reward': 0.6796875, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.4363361597061157, 'reward': 1.6433674693107605, 'reward_std': 0.5103466510772705, 'kl': 0.02398681640625, 'epoch': 0.33}
 17%|█▋        | 347/2088 [5:04:18<24:57:09, 51.60s/it][2025-11-07 07:14:41,947] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 348/2088 [5:05:09<24:49:03, 51.35s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0216308955313855, 'learning_rate': 8.333333333333333e-07, 'completion_length': 228.6171875, 'rewards/av_format_reward': 0.65625, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.3883969485759735, 'reward': 1.4391781687736511, 'reward_std': 0.6421323120594025, 'kl': 0.0267333984375, 'epoch': 0.33}
 17%|█▋        | 348/2088 [5:05:09<24:49:03, 51.35s/it][2025-11-07 07:15:33,184] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 349/2088 [5:06:00<24:47:13, 51.31s/it]                                                       {'loss': 0.001, 'grad_norm': 2.193384571666784, 'learning_rate': 8.328544061302682e-07, 'completion_length': 224.87109375, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.5348391830921173, 'reward': 1.9410892128944397, 'reward_std': 0.5332621037960052, 'kl': 0.0255126953125, 'epoch': 0.33}
 17%|█▋        | 349/2088 [5:06:00<24:47:13, 51.31s/it][2025-11-07 07:16:27,210] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 350/2088 [5:06:54<25:09:56, 52.13s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0019997473050752, 'learning_rate': 8.323754789272031e-07, 'completion_length': 220.84375, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4924772381782532, 'reward': 1.8206022381782532, 'reward_std': 0.4356830567121506, 'kl': 0.02484130859375, 'epoch': 0.34}
 17%|█▋        | 350/2088 [5:06:54<25:09:56, 52.13s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9594.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04023.mp4
[DEBUG] Step 350
[DEBUG] prompt_completion_ids shape: torch.Size([32, 286])
[DEBUG] Shortest output (len=167): <vis_desc>In the video, we see a man with a neutral facial expression, his gaze directly focused on the other person, seemingly engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads: "If you can't accept it, you can't have it." This sentence may be the man's advice or comment on something.</aud_desc>
<think>Based on the facial expression of the man in the video, which appears to be neutral, and the description of the neutral tone in the audio, we can infer that this sentence may have a serious or explanatory tone. Therefore, this sentence may be the man's way of expressing advice or comment on something, rather than a request or temptation.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=286): <vis_desc>In the video, we see a man with a furrowed brow and slightly open mouth, showing a surprised and negative emotion. His gaze is fixed on the other person, and his expression seems to be a neutral expression or reaction.</vis_desc>
<aud_desc>In the audio, the tone is neutral, and the intonation upward is noticeable, indicating that the character is actively expressing his opinions. In the text, the subtitle reads: "If you can't handle it, let someone else handle it." This sentence may be a form of advice or encouragement from the man to the other person.</aud_desc>
<think>Based on the facial expression of the man in the video, where his furrowed brow and slightly open mouth indicate a surprised and negative emotion, as well as his fixed gaze and neutral expression on the other person's face, we can infer that this sentence may carry a sense of surprise or criticism. The man may be expressing advice or emphasizing that if someone is unsuitable, they should be handled by another person. Therefore, this sentence may carry a neutral tone, but based on the surprised and negative emotion displayed by the man, as well as the description of the neutral and calm tone in the audio, we can speculate that this sentence may have a tone of advice or encouragement, but still with a somewhat negative or confrontational tone.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09154.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10607.mp4
[DEBUG] Step 350
[DEBUG] prompt_completion_ids shape: torch.Size([32, 334])
[DEBUG] Shortest output (len=170): <vis_desc>In the video, we see a character with his hands placed on the desk, gripping the sides tightly. His eyes are tightly closed, and his facial expression appears very sad and pained.</vis_desc>
<aud_desc>In the audio, the character's sobbing can be felt. In the text, the subtitle reads: "I've been threatened byGod, I didn't threat those people." This sentence expresses the character's regret and pain.</aud_desc>
<think>Based on the visual clue of the character's closed eyes and tightly closed facial expression, as well as the audio clue of the character's sobbing, we can infer that this sentence expresses the character's regret and pain about his actions, aligning with the emotion of sadness and pain displayed by the character.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=334): <vis_desc>In the video, we see a male character with his back to the camera, sitting in a dimly lit environment. His facial expression is serious and his eyes are fixed on the distance, possibly focusing on something specific or observing the surroundings. He has his hands tightly gripping the arms of the chair, which can be felt as pressure marks on his back. Overall, he may be experiencing an important task or event, concentrating or being focused.</vis_desc>
<aud_desc>In the audio, the character's sobbing can be felt, suggesting that the character's heart has been broken many times. In the text, the subtitle says, "I have seen your people killed. I claim not to have killed them with a gun, I承诺不杀 single persons." This sentence may be spoken by the male character during the process of reporting a crime or calling for the end of violence against persons of a certain race.</aud_desc>
<think>Based on the serious facial expression, tight gripping of the arms of the chair, and pressure marks on the back as far as the eye sockets are concerned, it can be inferred that the male character may have experienced the loss of his family members or friends in the previous events, causing him to feel guilty and remorseful. The mention of the word "violation" at the beginning of the sentence further emphasizes his anger and pain towards the people he killed. Additionally, the mention of "claiming not to have killed them with a gun" implies that he did not carry out or participate in murder, which aligns with his overall display of anger and remorse.</think>
<answer>neutral</answer>
[2025-11-07 07:17:19,558] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 351/2088 [5:07:47<25:11:00, 52.19s/it]                                                       {'loss': 0.001, 'grad_norm': 1.9754724825582055, 'learning_rate': 8.318965517241379e-07, 'completion_length': 223.765625, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.45250172913074493, 'reward': 1.7493767738342285, 'reward_std': 0.4481208473443985, 'kl': 0.02471923828125, 'epoch': 0.34}
 17%|█▋        | 351/2088 [5:07:47<25:11:00, 52.19s/it][2025-11-07 07:18:09,854] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 352/2088 [5:08:37<24:53:39, 51.62s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.171721479953361, 'learning_rate': 8.314176245210728e-07, 'completion_length': 224.10546875, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5112398862838745, 'reward': 1.7807711362838745, 'reward_std': 0.5074155926704407, 'kl': 0.02703857421875, 'epoch': 0.34}
 17%|█▋        | 352/2088 [5:08:37<24:53:39, 51.62s/it][2025-11-07 07:19:04,126] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 353/2088 [5:09:31<25:15:45, 52.42s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0980307643072393, 'learning_rate': 8.309386973180075e-07, 'completion_length': 223.19140625, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5759859085083008, 'reward': 2.044735908508301, 'reward_std': 0.420630618929863, 'kl': 0.0284423828125, 'epoch': 0.34}
 17%|█▋        | 353/2088 [5:09:31<25:15:45, 52.42s/it][2025-11-07 07:19:53,986] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 354/2088 [5:10:21<24:52:45, 51.65s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.069216211606506, 'learning_rate': 8.304597701149425e-07, 'completion_length': 219.66796875, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.5286836177110672, 'reward': 1.9114961624145508, 'reward_std': 0.5704401731491089, 'kl': 0.0277099609375, 'epoch': 0.34}
 17%|█▋        | 354/2088 [5:10:21<24:52:45, 51.65s/it][2025-11-07 07:20:45,035] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 355/2088 [5:11:12<24:46:37, 51.47s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0639492338024734, 'learning_rate': 8.299808429118773e-07, 'completion_length': 230.68359375, 'rewards/av_format_reward': 0.640625, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5487399399280548, 'reward': 1.8299898505210876, 'reward_std': 0.49923329055309296, 'kl': 0.02667236328125, 'epoch': 0.34}
 17%|█▋        | 355/2088 [5:11:12<24:46:37, 51.47s/it][2025-11-07 07:21:32,671] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 356/2088 [5:12:00<24:12:33, 50.32s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1123623235895326, 'learning_rate': 8.295019157088122e-07, 'completion_length': 218.5859375, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.6640625, 'rewards/think_step_av_with_neutral_reward': 0.4932834208011627, 'reward': 1.9112521409988403, 'reward_std': 0.4828268438577652, 'kl': 0.0257568359375, 'epoch': 0.34}
 17%|█▋        | 356/2088 [5:12:00<24:12:33, 50.32s/it][2025-11-07 07:22:22,821] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 357/2088 [5:12:50<24:10:14, 50.27s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0629839983603335, 'learning_rate': 8.29022988505747e-07, 'completion_length': 225.578125, 'rewards/av_format_reward': 0.67578125, 'rewards/accuracy_reward': 0.60546875, 'rewards/think_step_av_with_neutral_reward': 0.5505677461624146, 'reward': 1.831817865371704, 'reward_std': 0.48804542422294617, 'kl': 0.024658203125, 'epoch': 0.34}
 17%|█▋        | 357/2088 [5:12:50<24:10:14, 50.27s/it][2025-11-07 07:23:12,701] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 358/2088 [5:13:40<24:06:03, 50.15s/it]                                                       {'loss': 0.001, 'grad_norm': 2.101519834734668, 'learning_rate': 8.28544061302682e-07, 'completion_length': 223.5390625, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.7265625, 'rewards/think_step_av_with_neutral_reward': 0.6333503425121307, 'reward': 2.156787872314453, 'reward_std': 0.5222666263580322, 'kl': 0.02557373046875, 'epoch': 0.34}
 17%|█▋        | 358/2088 [5:13:40<24:06:03, 50.15s/it][2025-11-07 07:24:04,645] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 359/2088 [5:14:32<24:20:42, 50.69s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0343142514019656, 'learning_rate': 8.280651340996169e-07, 'completion_length': 224.8359375, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.6875, 'rewards/think_step_av_with_neutral_reward': 0.6194789409637451, 'reward': 1.9944789409637451, 'reward_std': 0.47220471501350403, 'kl': 0.02557373046875, 'epoch': 0.34}
 17%|█▋        | 359/2088 [5:14:32<24:20:42, 50.69s/it][2025-11-07 07:24:59,178] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 360/2088 [5:15:26<24:53:04, 51.84s/it]                                                       {'loss': 0.0009, 'grad_norm': 1.9951867631466695, 'learning_rate': 8.275862068965517e-07, 'completion_length': 234.69921875, 'rewards/av_format_reward': 0.890625, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.41614824533462524, 'reward': 1.8106794357299805, 'reward_std': 0.5541767776012421, 'kl': 0.02349853515625, 'epoch': 0.34}
 17%|█▋        | 360/2088 [5:15:26<24:53:04, 51.84s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5092.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9884.mp4
[DEBUG] Step 360
[DEBUG] prompt_completion_ids shape: torch.Size([32, 342])
[DEBUG] Shortest output (len=169): <vis_desc>In the video, we see a close-up of a male character's face. He has a serious expression, sharp eyes, and is looking directly at the other person, possibly engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the subtitle says "Yes, I'll spare it." This sentence may be the male character's response to a question or statement.</aud_desc>
<think>Based on the video clue of the male character's serious expression and sharp eyes, as well as the audio clue of the character's calm tone, we can infer that this sentence may carry a sense of seriousness or decision-making. Therefore, this sentence may indicate that the male character agrees with or is willing to handle a problem, and his attitude appears confident and serious.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=342): <vis_desc>In the video, we see a character wearing military uniform and a blue headscarf standing in an industrial area with ruins and metal beams. In the video, his eyes are wide open, his mouth is open, and his hands are raised, palm-up side up, towards the sky. It seems that he is speaking or shouting loudly, causing a stir among the people around him.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "Let's go, everyone, we are at the end of the road. Let's go together, let's go as a group, let's go as a group, let's go as a group, let's go as a group, let's go as a group, let's go as a group, let's go together, let's go together, let's go together, let's go together, let's go together, let's go together, let's go together, let's go together." This sentence is likely spoken by the character to reassure the people around him or express his own strength.</aud_desc>
<think>Based on the wide-open eyes, open mouth, and raised hands in the video, it can be inferred that the character is speaking or shouting loudly, causing a stir among the people around him, which aligns with the emotions of fear and anxiety displayed by the characters in the video. Therefore, this sentence is likely the character comforting and urging the people around him to leave the dangerous area and come together for survival, consistent with the theme of the character expressing his strength.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1680.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09638.mp4
[DEBUG] Step 360
[DEBUG] prompt_completion_ids shape: torch.Size([32, 323])
[DEBUG] Shortest output (len=161): <vis_desc>In the video, we see a female character with wide-open mouth and eye sockets, expressions of extreme joy and excitement, and a high pitch voice.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very excited and urgent, with a questioning tone. In the text, the subtitle says, "Yamaime, what?" This sentence may be the female character's response to someone or something.</aud_desc>
<think>Based on the joyful and excited expressions, high pitch voice, and the questioning tone in the audio, we can infer that this sentence may carry a sense of curiosity or questioning. The female character may feel surprised or disagrees with the other person's statement or action, hence expressing this sentence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=323): <vis_desc>In the video, the scene shows a woman sitting in an office environment. At the beginning of the video, she is looking down, seemingly contemplating something. In the following scenes, she slightly lifts her head, looking directly at someone or something outside the frame, showing a more noticeable change in emotion. At the end of the video, she lowers her head again and looks down again, with no significant change in emotion. Overall, we can infer that the woman in the video is going through a process of inner struggle or contemplation.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues in this video and audio sequence strongly suggest that the woman is experiencing emotional turmoil or inner contemplation. Visually, her shifting emotions—from focused consideration to increased internal conflict and then calm, reflective look—along with slow head movements and direct eye contact, indicate complex internal turmoil possibly associated with anxiety, frustration, or uncertainty. The lack of clear vocal emotional indicators removes herself from a specific mood context, but the interplay of her nuanced emotional shifts and sustained calm reflection creates a rich tapestry of introspection. Auditory signals, while ambiguous, conveys neutral affect—the specific emotional state remains unspecified. Together, the multimodal evidence points to a person grappling with ambiguous feelings, supporting the inference that this emotion is likely contemplative rather than overtly positive or negative.</think>
<answer>anxiety</answer>
[2025-11-07 07:25:51,705] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 361/2088 [5:16:19<24:58:07, 52.05s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.064769298543786, 'learning_rate': 8.271072796934865e-07, 'completion_length': 228.25390625, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.47763729095458984, 'reward': 1.7276372909545898, 'reward_std': 0.43439532816410065, 'kl': 0.027099609375, 'epoch': 0.35}
 17%|█▋        | 361/2088 [5:16:19<24:58:07, 52.05s/it][2025-11-07 07:26:43,594] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 362/2088 [5:17:11<24:55:53, 52.00s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.020348022779525, 'learning_rate': 8.266283524904214e-07, 'completion_length': 229.73828125, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.4453125, 'rewards/think_step_av_with_neutral_reward': 0.40584661066532135, 'reward': 1.6245965957641602, 'reward_std': 0.4538777619600296, 'kl': 0.02691650390625, 'epoch': 0.35}
 17%|█▋        | 362/2088 [5:17:11<24:55:53, 52.00s/it][2025-11-07 07:27:34,995] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 363/2088 [5:18:02<24:49:50, 51.82s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.6884467434545622, 'learning_rate': 8.261494252873563e-07, 'completion_length': 217.359375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.44041769206523895, 'reward': 1.737292766571045, 'reward_std': 0.4483155757188797, 'kl': 0.0299072265625, 'epoch': 0.35}
 17%|█▋        | 363/2088 [5:18:02<24:49:50, 51.82s/it][2025-11-07 07:28:31,465] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 364/2088 [5:18:59<25:29:02, 53.22s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.00181755013201, 'learning_rate': 8.256704980842911e-07, 'completion_length': 227.41796875, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.5516687631607056, 'reward': 1.8407312631607056, 'reward_std': 0.49391870200634, 'kl': 0.02667236328125, 'epoch': 0.35}
 17%|█▋        | 364/2088 [5:18:59<25:29:02, 53.22s/it][2025-11-07 07:29:21,313] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 17%|█▋        | 365/2088 [5:19:48<24:59:08, 52.20s/it]                                                       {'loss': 0.001, 'grad_norm': 2.062413735890798, 'learning_rate': 8.25191570881226e-07, 'completion_length': 225.94921875, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.3686850517988205, 'reward': 1.653841257095337, 'reward_std': 0.5220497697591782, 'kl': 0.02520751953125, 'epoch': 0.35}
 17%|█▋        | 365/2088 [5:19:48<24:59:08, 52.20s/it][2025-11-07 07:30:08,947] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 366/2088 [5:20:36<24:18:55, 50.83s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0904758472851213, 'learning_rate': 8.247126436781609e-07, 'completion_length': 227.4296875, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4729633331298828, 'reward': 1.6448383331298828, 'reward_std': 0.5222146362066269, 'kl': 0.02685546875, 'epoch': 0.35}
 18%|█▊        | 366/2088 [5:20:36<24:18:55, 50.83s/it][2025-11-07 07:30:59,623] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 367/2088 [5:21:27<24:16:43, 50.79s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9553202561854603, 'learning_rate': 8.242337164750958e-07, 'completion_length': 228.55078125, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.49146415293216705, 'reward': 1.9641203880310059, 'reward_std': 0.5212976932525635, 'kl': 0.02740478515625, 'epoch': 0.35}
 18%|█▊        | 367/2088 [5:21:27<24:16:43, 50.79s/it][2025-11-07 07:31:53,776] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 368/2088 [5:22:21<24:44:49, 51.80s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.234909930574585, 'learning_rate': 8.237547892720306e-07, 'completion_length': 227.98046875, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.4819464087486267, 'reward': 1.856946349143982, 'reward_std': 0.5244188010692596, 'kl': 0.02740478515625, 'epoch': 0.35}
 18%|█▊        | 368/2088 [5:22:21<24:44:49, 51.80s/it][2025-11-07 07:32:46,871] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 369/2088 [5:23:14<24:55:07, 52.19s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9775522670001529, 'learning_rate': 8.232758620689654e-07, 'completion_length': 232.296875, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.46174468100070953, 'reward': 1.8640884160995483, 'reward_std': 0.5136371552944183, 'kl': 0.0274658203125, 'epoch': 0.35}
 18%|█▊        | 369/2088 [5:23:14<24:55:07, 52.19s/it][2025-11-07 07:33:37,747] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 370/2088 [5:24:05<24:42:59, 51.79s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0283828656081657, 'learning_rate': 8.227969348659003e-07, 'completion_length': 231.43359375, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.6171875, 'rewards/think_step_av_with_neutral_reward': 0.47374992072582245, 'reward': 1.8370311856269836, 'reward_std': 0.5974756181240082, 'kl': 0.025634765625, 'epoch': 0.35}
 18%|█▊        | 370/2088 [5:24:05<24:42:59, 51.79s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09874.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5530.mp4
[DEBUG] Step 370
[DEBUG] prompt_completion_ids shape: torch.Size([32, 315])
[DEBUG] Shortest output (len=174): <vis_desc>In the video, the screen shows a middle-aged man focused and serious, looking directly at the other person, seeming to be engaged in a tense conversation.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle reads: "I will slay you where you sit right now, for what you have done." This sentence may be a threat or warning from the middle-aged man to the other person.</aud_desc>
<think>Based on the serious expression and direct gaze of the man in the video, as well as the stern and aggressive tone in the audio, we can infer that this sentence carries a sense of anger or severity. The man may be unhappy with the other person's behavior or attitude and expresses his dissatisfaction in this sentence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=315): <vis_desc>In the video, the screen shows a middle-aged man in a dim environment, possibly a corridor or a similar dark place. In the video, his eyes are wide open, eyebrows raised, veins on his head and neck are prominent, and his facial muscles are tense, giving the impression of great anger. His mouth is open, and his gaze is fixed on the target, indicating that he is preparing to shoot at someone. Overall, the character's emotions are very intense and tense, suggesting that he may be engaged in a highly charged conversation or interaction.</vis_desc>
<aud_desc>In the audio, the tone is very aggressive, and combined with the text content, it can be felt that the character's emotional state is anger and sadness. In the text, the subtitle reads: "If I am powerful, I will slay you where you stand today. What you have done for me in the past will be considered by God." This sentence expresses the man's anger and sadness.</aud_desc>
<think>Based on the video clues of the man's wide-open eyes, raised eyebrows, prominent veins on the head and neck, and fixed gaze on the target, it can be inferred that the man is in a highly charged conversation or interaction. Additionally, the audio clue describing an aggressive tone and God referencing in the text further supports the man's anger and sadness emotions. Combining these clues, it can be deduced that the man is expressing his strong anger and sadness emotions in this sentence.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3283.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2246.mp4
[DEBUG] Step 370
[DEBUG] prompt_completion_ids shape: torch.Size([32, 285])
[DEBUG] Shortest output (len=169): <vis_desc>In the video, we see a female character with slightly furrowed eyes and slightly open mouth, seemingly expressing surprise or uncertainty.</vis_desc>
<aud_desc>In the audio, the voice is raised and the tone is excited. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Based on the facial expression of the female character in the video clues, with slightly furrowed eyes and slightly open mouth, as well as the description of an excited voice and raised tone in the audio clues, we can infer that this emotion may be surprise or uncertainty. The facial expression and tone both indicate that the character is confused and surprised, which aligns with the displayed facial expression of the female character in the video.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=285): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned corners of her mouth, which may indicate that she is experiencing some inner conflict or unease. Her gaze is fixed on the other person, and her expression appears serious and focused, suggesting that she is carefully listening and observing what they have to say.</vis_desc>
<aud_desc>In the audio, the character's voice is calm. Combined with the text content, it seems that she is asking someone a question and hopes that they can make a certain decision. The subtitle in the text says, "Do you want to leave with me?" This sentence may be an inquiry from the female character to the male character.</aud_desc>
<think>Based on the visual clues of the female character's furrowed brows, downturned corners of the mouth, and serious, focused expression, it can be inferred that she may be expressing a state of unease or impatience, possibly due to uncertainty or hesitation about her next move. At the same time, the calm tone of the male character suggests that her own emotions are neutral. Therefore, this sentence may be an expression of concern or suggestion from the female character to the male character, asking if he can stay for a while to make a decision, in order to alleviate the other person's worry or uncertainty.</think>
<answer>surprise</answer>
[2025-11-07 07:34:25,985] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 371/2088 [5:24:53<24:11:37, 50.73s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.121453837691217, 'learning_rate': 8.223180076628352e-07, 'completion_length': 225.2734375, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.4296875, 'rewards/think_step_av_with_neutral_reward': 0.39677298069000244, 'reward': 1.5686479210853577, 'reward_std': 0.5045201629400253, 'kl': 0.0277099609375, 'epoch': 0.36}
 18%|█▊        | 371/2088 [5:24:53<24:11:37, 50.73s/it][2025-11-07 07:35:16,046] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 372/2088 [5:25:43<24:05:04, 50.53s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.08086915065826, 'learning_rate': 8.218390804597701e-07, 'completion_length': 219.94140625, 'rewards/av_format_reward': 0.890625, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.47357434034347534, 'reward': 1.8954493403434753, 'reward_std': 0.5806278586387634, 'kl': 0.0277099609375, 'epoch': 0.36}
 18%|█▊        | 372/2088 [5:25:43<24:05:04, 50.53s/it][2025-11-07 07:36:07,839] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 373/2088 [5:26:35<24:15:05, 50.91s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9624418536019894, 'learning_rate': 8.213601532567049e-07, 'completion_length': 217.52734375, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.42979754507541656, 'reward': 1.7540162205696106, 'reward_std': 0.4358520805835724, 'kl': 0.0272216796875, 'epoch': 0.36}
 18%|█▊        | 373/2088 [5:26:35<24:15:05, 50.91s/it][h264 @ 0xb81be940] cabac decode of qscale diff failed at 34 35
[h264 @ 0xb81be940] error while decoding MB 34 35, bytestream 762
[h264 @ 0xb81be940] cabac decode of qscale diff failed at 53 9
[h264 @ 0xb81be940] error while decoding MB 53 9, bytestream 10773
[2025-11-07 07:36:59,907] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 374/2088 [5:27:27<24:24:11, 51.26s/it]                                                       {'loss': 0.001, 'grad_norm': 2.085163244900987, 'learning_rate': 8.208812260536399e-07, 'completion_length': 223.19140625, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.3542688637971878, 'reward': 1.6628625988960266, 'reward_std': 0.42725303769111633, 'kl': 0.0255126953125, 'epoch': 0.36}
 18%|█▊        | 374/2088 [5:27:27<24:24:11, 51.26s/it][2025-11-07 07:37:51,495] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 375/2088 [5:28:19<24:26:10, 51.35s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.936787795575641, 'learning_rate': 8.204022988505747e-07, 'completion_length': 222.1875, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.42560145258903503, 'reward': 1.7615389227867126, 'reward_std': 0.4975316673517227, 'kl': 0.02679443359375, 'epoch': 0.36}
 18%|█▊        | 375/2088 [5:28:19<24:26:10, 51.35s/it][2025-11-07 07:38:43,293] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 376/2088 [5:29:10<24:29:07, 51.49s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0488497925672373, 'learning_rate': 8.199233716475096e-07, 'completion_length': 230.0390625, 'rewards/av_format_reward': 0.90234375, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.49754874408245087, 'reward': 1.958486258983612, 'reward_std': 0.4476063996553421, 'kl': 0.02630615234375, 'epoch': 0.36}
 18%|█▊        | 376/2088 [5:29:10<24:29:07, 51.49s/it][2025-11-07 07:39:35,450] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 377/2088 [5:30:03<24:33:59, 51.69s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.027015592125349, 'learning_rate': 8.194444444444443e-07, 'completion_length': 231.71875, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4582872688770294, 'reward': 1.8684436082839966, 'reward_std': 0.5045646131038666, 'kl': 0.028076171875, 'epoch': 0.36}
 18%|█▊        | 377/2088 [5:30:03<24:33:59, 51.69s/it][2025-11-07 07:40:25,215] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 378/2088 [5:30:52<24:16:40, 51.11s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0839016226455485, 'learning_rate': 8.189655172413793e-07, 'completion_length': 220.1015625, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.39824365079402924, 'reward': 1.5154312252998352, 'reward_std': 0.5177527964115143, 'kl': 0.027587890625, 'epoch': 0.36}
 18%|█▊        | 378/2088 [5:30:52<24:16:40, 51.11s/it][2025-11-07 07:41:17,071] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 379/2088 [5:31:44<24:22:11, 51.33s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.061284434864871, 'learning_rate': 8.184865900383141e-07, 'completion_length': 229.375, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.4858636409044266, 'reward': 1.7866449356079102, 'reward_std': 0.45800459384918213, 'kl': 0.0274658203125, 'epoch': 0.36}
 18%|█▊        | 379/2088 [5:31:44<24:22:11, 51.33s/it][2025-11-07 07:42:09,619] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 380/2088 [5:32:37<24:31:41, 51.70s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.064517435063654, 'learning_rate': 8.18007662835249e-07, 'completion_length': 216.4453125, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.4339378625154495, 'reward': 1.629250407218933, 'reward_std': 0.5228630900382996, 'kl': 0.02716064453125, 'epoch': 0.36}
 18%|█▊        | 380/2088 [5:32:37<24:31:41, 51.70s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3528.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03167.mp4
[DEBUG] Step 380
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a character wearing traditional Chinese clothing standing in a traditional Chinese indoor setting. His facial expression is serious and his mouth is open, seemingly speaking or shouting loudly.</vis_desc>
<aud_desc>In the audio, the voice is sharp and aggressive, with a stern tone. In the text, the subtitle says, "Be quiet!" This sentence may be a threat or warning from the male character to another person.</aud_desc>
<think>Based on the serious facial expression and open mouth of the male character in the video clues, as well as the sharp and aggressive description in the audio clues, we can infer that this sentence carries a sense of anger or severity. The male character may be displeased with the behavior or words of the other person and expresses his stance with a stern tone.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=295): <vis_desc>In the video, we see an elderly man wearing traditional Chinese clothing, holding a baton, and the environment appears to be a traditional Chinese palace. From the ornate golden decorations and traditional Chinese furniture in the background, as well as the way the light reflects off the golden decorations, we can infer that the scene is possibly a celebration or a visit to a significant place of power, like a funeral home or an important official's residence. The elderly man is likely expressing his view or command with the baton.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. Combined with the text content, it appears that the character is accusing others. In the text, the subtitle says, "No escape!" This sentence may be an expression of the elderly man's determination or anger.</aud_desc>
<think>Based on the video clue of the elderly man wearing traditional Chinese clothing, the presence of traditional Chinese furniture in the environment, and the way the light reflects off the golden decorations, we can infer that this sentence may carry a sense of power and sternness. Since the character speaks in a stern and aggressive tone described in the audio clue, it can be inferred that this sentence may be an expression of the elderly man's determination or anger. Therefore, this sentence may express the elderly man's determination or anger, which aligns with the context of him possibly accusing others.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08354.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8191.mp4
[DEBUG] Step 380
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=172): <vis_desc>In the video, we see a character with outstretched hands and a fierce facial expression, seemingly expressing anger or a feeling of excitement.</vis_desc>
<aud_desc>In the audio, the tone is fierce and the character's emotions are intense. In the text, the subtitle says, "The King is vanquished." This sentence may be the character's reaction to a sudden situation or event.</aud_desc>
<think>Based on the video clues of the character's outstretched hands and fierce facial expression, as well as the audio clue of the character's fierce and intense tone, we can infer that this sentence may carry a sense of determination and anger. The character may feel angry and excited about the King being vanquished, expressing his emotions of determination and anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=295): <vis_desc>In the video, the screen shows a female character in a dim environment, possibly indoors. In the video, her eyes are wide open and she maintains a relatively still posture with only her mouth moving. It can be inferred that she may be experiencing an emotional shock or recalling a sad memory. Her gaze is fixed on a certain place, potentially indicating that something or someone in the environment has triggered her emotions.</vis_desc>
<aud_desc>In the audio, the character sobbing can be felt, with a slow pace and a low tone, expressing the character's sadness and feelings of being sad. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The pronounced visual cues strongly suggest a sad emotional state: the character’s wide-open eyes and calm, static posture imply a neutral but overwhelmingly emotional moment rather than an overt display of strong feelings. Her gaze fixed on an ambiguous, potentially sadplace further supports this interpretation. Complementing this, the auditory layer adds another layer of sadnessality—the presence of the sobbing vocal tone paired with a low vocal pace aligns naturally with a heavy emotional weight, reflecting both the somber atmosphere and internal anguish that often precedes sadness. Although the subtitle text does not clarify the emotion, the integration of vocal sadness and visual signs into a cohesive multimodal analysis unmistakably points to sadness as the target emotion.</think>
<answer>sad</answer>
[2025-11-07 07:43:01,027] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 381/2088 [5:33:28<24:28:21, 51.61s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0793398019994487, 'learning_rate': 8.175287356321838e-07, 'completion_length': 228.390625, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.5419063419103622, 'reward': 1.9442501068115234, 'reward_std': 0.4495042562484741, 'kl': 0.02874755859375, 'epoch': 0.36}
 18%|█▊        | 381/2088 [5:33:28<24:28:21, 51.61s/it][2025-11-07 07:43:50,576] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 382/2088 [5:34:18<24:09:53, 50.99s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.02757860235619, 'learning_rate': 8.170498084291188e-07, 'completion_length': 226.68359375, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.4140625, 'rewards/think_step_av_with_neutral_reward': 0.3968789577484131, 'reward': 1.584378957748413, 'reward_std': 0.4361660033464432, 'kl': 0.02685546875, 'epoch': 0.37}
 18%|█▊        | 382/2088 [5:34:18<24:09:53, 50.99s/it][2025-11-07 07:44:39,816] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 383/2088 [5:35:07<23:54:06, 50.47s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1240712941421784, 'learning_rate': 8.165708812260536e-07, 'completion_length': 224.31640625, 'rewards/av_format_reward': 0.875, 'rewards/accuracy_reward': 0.72265625, 'rewards/think_step_av_with_neutral_reward': 0.5748185962438583, 'reward': 2.1724748611450195, 'reward_std': 0.40354425460100174, 'kl': 0.02880859375, 'epoch': 0.37}
 18%|█▊        | 383/2088 [5:35:07<23:54:06, 50.47s/it][2025-11-07 07:45:36,112] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 384/2088 [5:36:03<24:42:55, 52.22s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9675266398507818, 'learning_rate': 8.160919540229885e-07, 'completion_length': 223.36328125, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4894145131111145, 'reward': 1.7980083227157593, 'reward_std': 0.4739120304584503, 'kl': 0.026611328125, 'epoch': 0.37}
 18%|█▊        | 384/2088 [5:36:03<24:42:55, 52.22s/it][mpeg4 @ 0xb4cf5400] ac-tex damaged at 33 24
[mpeg4 @ 0xb4cf5400] Error at MB: 1977
[2025-11-07 07:46:25,999] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 385/2088 [5:36:53<24:22:13, 51.52s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.067272760574393, 'learning_rate': 8.156130268199233e-07, 'completion_length': 221.90625, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.44140625, 'rewards/think_step_av_with_neutral_reward': 0.40539589524269104, 'reward': 1.5499271750450134, 'reward_std': 0.4368584603071213, 'kl': 0.02874755859375, 'epoch': 0.37}
 18%|█▊        | 385/2088 [5:36:53<24:22:13, 51.52s/it][2025-11-07 07:47:18,414] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|█▊        | 386/2088 [5:37:45<24:28:59, 51.79s/it]                                                       {'loss': 0.001, 'grad_norm': 2.1358550460582055, 'learning_rate': 8.151340996168582e-07, 'completion_length': 232.16015625, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.4907469004392624, 'reward': 1.9126219153404236, 'reward_std': 0.5317341685295105, 'kl': 0.02569580078125, 'epoch': 0.37}
 18%|█▊        | 386/2088 [5:37:45<24:28:59, 51.79s/it][2025-11-07 07:48:09,958] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▊        | 387/2088 [5:38:37<24:26:04, 51.71s/it]                                                       {'loss': 0.001, 'grad_norm': 2.0534542630184704, 'learning_rate': 8.146551724137931e-07, 'completion_length': 229.48046875, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.4875110387802124, 'reward': 1.9015734195709229, 'reward_std': 0.5558082759380341, 'kl': 0.0255126953125, 'epoch': 0.37}
 19%|█▊        | 387/2088 [5:38:37<24:26:04, 51.71s/it][2025-11-07 07:49:00,910] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▊        | 388/2088 [5:39:28<24:18:44, 51.49s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.986528782084201, 'learning_rate': 8.141762452107279e-07, 'completion_length': 227.3125, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5745398849248886, 'reward': 1.8323523998260498, 'reward_std': 0.4619140177965164, 'kl': 0.0274658203125, 'epoch': 0.37}
 19%|█▊        | 388/2088 [5:39:28<24:18:44, 51.49s/it][2025-11-07 07:49:50,961] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▊        | 389/2088 [5:40:18<24:05:44, 51.06s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.119342349074266, 'learning_rate': 8.136973180076628e-07, 'completion_length': 226.60546875, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.69921875, 'rewards/think_step_av_with_neutral_reward': 0.5777252465486526, 'reward': 2.1128814220428467, 'reward_std': 0.4197361469268799, 'kl': 0.02783203125, 'epoch': 0.37}
 19%|█▊        | 389/2088 [5:40:18<24:05:44, 51.06s/it][2025-11-07 07:50:39,123] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▊        | 390/2088 [5:41:06<23:40:16, 50.19s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0732494736419906, 'learning_rate': 8.132183908045977e-07, 'completion_length': 220.4296875, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.49932730197906494, 'reward': 1.8118272423744202, 'reward_std': 0.45762309432029724, 'kl': 0.030029296875, 'epoch': 0.37}
 19%|█▊        | 390/2088 [5:41:06<23:40:16, 50.19s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4144.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07050.mp4
[DEBUG] Step 390
[DEBUG] prompt_completion_ids shape: torch.Size([32, 262])
[DEBUG] Shortest output (len=174): <vis_desc>In the video, the screen shows a male character sitting in a narrow corridor with a patterned wall and ceiling. He is looking directly at the other person and his facial expression exhibits a happy emotion.</vis_desc>
<aud_desc>In the audio, the character's voice sounds cheerful and optimistic, with a relaxed tone. In the text, the subtitle says, "OK, ok, let's go." This sentence may be the male character's response to an invitation or suggestion.</aud_desc>
<think>Based on the audio clue describing the character's joyful and optimistic voice, as well as the video clue of the male character displaying a happy emotion, we can infer that this sentence is likely the male character responding positively to a request or suggestion, expressing his willingness to go or agree to the proposal.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=262): <vis_desc>In the video, the screen shows a female character with tightly furrowed brows and slightly downturned corners of the mouth, indicating sadness and pain. Her gaze is fixed on the other person, and her mouth moves, indicating that she is crying and expressing her feelings through emotion. Overall, she is experiencing an emotional dilemma.</vis_desc>
<aud_desc>In the audio, the character's sobbing and pleading tone are prominent, as well as the cry of "Yes, I really know what you're going through," which may be the female character's expression of pleading or begging for mercy. In the text, the subtitle reads, "I forced her to come, and she just got rescued, really, she's the one who saved us." This sentence is possibly the female character's expression of pleading or begging.</aud_desc>
<think>Based on the facial expression of the female character in the video, where her brows are tightly furrowed and her mouth is moving, as well as the description of her sobbing and pleading tone in the audio, we can infer that the female character is going through an emotional dilemma. She may be worried about the other person's safety or emotional state, expressing her sadness and pain through emotion.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2878.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10675.mp4
[DEBUG] Step 390
[DEBUG] prompt_completion_ids shape: torch.Size([32, 344])
[DEBUG] Shortest output (len=162): <vis_desc>In the video, the screen shows a female character standing in a doorway. Her facial expression appears very scared, with wide-open eyes and raised eyebrows. She seems to be looking directly at someone or something outside the door, possibly waiting for their response.</vis_desc>
<aud_desc>In the audio, there is obvious laughter, which may indicate that the character is joking or expressing herself flexing strength. In the text, the subtitle says, "Ah."</aud_desc>
<think>Based on the female character's scared facial expression and the obvious laughter in the audio, it can be inferred that the female character may be joking and expressing herself flexing strength. She may be about to encounter something frightening, hence displaying a surprised and scared emotion.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=344): <vis_desc>In the video, the scene shows an elderly man sitting in a restaurant. At the beginning of the video, his facial expression is serious. His brow is furrowed and the corners of his mouth are slightly downturned, which may indicate that he is concentrating on hearing or reading some important things. His gestures are quite pronounced, constantly raising his hand, possibly emphasizing a point or explaining something to others.</vis_desc>
<aud_desc>In the audio, the tone is low and conveys a sense of powerlessness, as if the character is speaking forcefully. In the text, the subtitle reads: "This seven-meeting situation was about how we were to divide this country. This meeting is how we are to survive. You have made people in the Fenchies, and for the feds, they have hired them." This sentence may be the elderly man emphasizing his point or sharing his viewpoint.</aud_desc>
<think>Based on the serious expression and furrowed brow of the elderly man in the video clues, as well as the action of raising his hand, in the audio clues, we can infer that this sentence may carry a heavy tone of blame or encouragement. The expression "this is how we are to survive" suggests that the elderly man is emphasizing the importance of unity and persistence in dealing with difficulties, while "you have hired people for the Fenchies and hired officers for the feds" implies that he blames someone or a group for not providing enough help or support. Therefore, this sentence may indicate that the elderly man is criticizing and blaming a certain individual or group, while also expressing his expectations and encouragement for them.</think>
<answer>neutral</answer>
[2025-11-07 07:51:31,063] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▊        | 391/2088 [5:41:58<23:54:19, 50.71s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.1084002114600042, 'learning_rate': 8.127394636015326e-07, 'completion_length': 226.99609375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.8203125, 'rewards/think_step_av_with_neutral_reward': 0.6454984843730927, 'reward': 2.247061014175415, 'reward_std': 0.43135973811149597, 'kl': 0.02862548828125, 'epoch': 0.37}
 19%|█▊        | 391/2088 [5:41:58<23:54:19, 50.71s/it][2025-11-07 07:52:20,975] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 392/2088 [5:42:48<23:46:41, 50.47s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.032584652743923, 'learning_rate': 8.122605363984674e-07, 'completion_length': 229.203125, 'rewards/av_format_reward': 0.90234375, 'rewards/accuracy_reward': 0.6953125, 'rewards/think_step_av_with_neutral_reward': 0.6464673578739166, 'reward': 2.2441235780715942, 'reward_std': 0.3681868463754654, 'kl': 0.02691650390625, 'epoch': 0.38}
 19%|█▉        | 392/2088 [5:42:48<23:46:41, 50.47s/it][2025-11-07 07:53:09,753] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 393/2088 [5:43:37<23:31:29, 49.96s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0525782401487462, 'learning_rate': 8.117816091954022e-07, 'completion_length': 222.2578125, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.40852315723896027, 'reward': 1.6663357019424438, 'reward_std': 0.47141049802303314, 'kl': 0.0279541015625, 'epoch': 0.38}
 19%|█▉        | 393/2088 [5:43:37<23:31:29, 49.96s/it][2025-11-07 07:54:00,398] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 394/2088 [5:44:27<23:36:25, 50.17s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.057153873054884, 'learning_rate': 8.113026819923371e-07, 'completion_length': 228.4296875, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5164306461811066, 'reward': 1.7078368663787842, 'reward_std': 0.5343047380447388, 'kl': 0.028564453125, 'epoch': 0.38}
 19%|█▉        | 394/2088 [5:44:27<23:36:25, 50.17s/it][2025-11-07 07:54:49,121] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 395/2088 [5:45:16<23:23:20, 49.73s/it]                                                       {'loss': 0.0217, 'grad_norm': 66.50411376905828, 'learning_rate': 8.10823754789272e-07, 'completion_length': 228.28125, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4436104744672775, 'reward': 1.752204179763794, 'reward_std': 0.48805728554725647, 'kl': 0.54229736328125, 'epoch': 0.38}
 19%|█▉        | 395/2088 [5:45:16<23:23:20, 49.73s/it][2025-11-07 07:55:39,454] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 396/2088 [5:46:07<23:27:34, 49.91s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1285267599362756, 'learning_rate': 8.103448275862068e-07, 'completion_length': 229.24609375, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.42080824077129364, 'reward': 1.6942458152770996, 'reward_std': 0.500822126865387, 'kl': 0.02911376953125, 'epoch': 0.38}
 19%|█▉        | 396/2088 [5:46:07<23:27:34, 49.91s/it][2025-11-07 07:56:32,512] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 397/2088 [5:47:00<23:53:19, 50.86s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1695539304136617, 'learning_rate': 8.098659003831417e-07, 'completion_length': 230.30078125, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.5098628401756287, 'reward': 1.8965815901756287, 'reward_std': 0.3629733473062515, 'kl': 0.029052734375, 'epoch': 0.38}
 19%|█▉        | 397/2088 [5:47:00<23:53:19, 50.86s/it][2025-11-07 07:57:20,026] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 398/2088 [5:47:47<23:24:13, 49.85s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.037006563697557, 'learning_rate': 8.093869731800766e-07, 'completion_length': 220.515625, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.34765625, 'rewards/think_step_av_with_neutral_reward': 0.3525272160768509, 'reward': 1.3915897011756897, 'reward_std': 0.4807218015193939, 'kl': 0.03082275390625, 'epoch': 0.38}
 19%|█▉        | 398/2088 [5:47:47<23:24:13, 49.85s/it][2025-11-07 07:58:06,224] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 399/2088 [5:48:33<22:52:31, 48.76s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.049523444855481, 'learning_rate': 8.089080459770115e-07, 'completion_length': 217.52734375, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.6640625, 'rewards/think_step_av_with_neutral_reward': 0.5637432038784027, 'reward': 2.1379618644714355, 'reward_std': 0.5125469267368317, 'kl': 0.02880859375, 'epoch': 0.38}
 19%|█▉        | 399/2088 [5:48:33<22:52:31, 48.76s/it][2025-11-07 07:58:58,921] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 400/2088 [5:49:26<23:24:57, 49.94s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.053521659205944, 'learning_rate': 8.084291187739464e-07, 'completion_length': 227.74609375, 'rewards/av_format_reward': 0.6328125, 'rewards/accuracy_reward': 0.36328125, 'rewards/think_step_av_with_neutral_reward': 0.32194602489471436, 'reward': 1.3180397748947144, 'reward_std': 0.5007802248001099, 'kl': 0.03021240234375, 'epoch': 0.38}
 19%|█▉        | 400/2088 [5:49:26<23:24:57, 49.94s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8654.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01223.mp4
[DEBUG] Step 400
[DEBUG] prompt_completion_ids shape: torch.Size([32, 318])
[DEBUG] Shortest output (len=143): <vis_desc>In the video, we see a woman wearing a brown dress carrying a white draped garment. Her facial expression is happy and her eyes are shining, indicating that she may be enjoying a relaxed and pleasant moment.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle says "Hello," which is the woman's welcoming greeting.</aud_desc>
<think>Based on the woman's joyful facial expression and happy mood shown in the video clues, as well as the description in the subtitle that this is a welcome greeting, we can infer that this sentence expresses the woman's joy and pleasant emotions.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=318): <aud_desc>In the audio, there are no valuable emotional clues provided. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, we see a woman wearing ancient attire walking along a fence in a park. She is holding a clothesline, which is surrounded by clothes, indicating that she is doing laundry. In the video, her facial expression is very happy and smiling, with raised cheeks and wide open eyes, showing a sense of surprise or joy. Her arms are flung freely, which may suggest that she is engaged in a pleasant conversation or exchange with someone.</vis_desc>
<think>The overall mood created by these visual elements—happy facial expression, wide-open eyes, and apparent pleasant communication through arms Flung freely—is consistent with a sense of surprise or joy often associated with surprise or joy in such situations. Although the audio lacks valuable emotional cues, the visual signals of the woman’s happy and smiling demeanor, with raised cheeks and wide open eyes, along with her engaged posture on the clothesline alongside other clothes, strongly indicate that she is in a moment of pleasant surprise or happiness. The environment, suggested to be a public park with trees and other indistinct clothing items, further supports the idea that her activities might be in a relaxed, social setting. Together, these visual indicators—a person in ancient attire engaged in a clothes-pullering activity in a public, visually pleasing setting—cohesively point to a mood of surprise or joy.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8331.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10611.mp4
[DEBUG] Step 400
[DEBUG] prompt_completion_ids shape: torch.Size([32, 332])
[DEBUG] Shortest output (len=162): <vis_desc>In the video, we see two female characters with a dim background, possibly an indoor environment. They are engaged in a minor argument, with their facial expression showing frustration.</vis_desc>
<aud_desc>In the audio, the character's voice is calm. In the text, the subtitle reads: "Is that what tea is?" This sentence is likely spoken by the female character during the argument with the other person.</aud_desc>
<think>Based on the video clue of the female character's joyful and angry emotions, as well as the audio clue of the character's calm voice, we can infer that this sentence may carry a teasing or mocking tone. The female character may be arguing with the other person and expressing her dissatisfaction and anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=332): <vis_desc>In the video, there are two women shown. The woman on the left appears to be in a relatively negative emotional state, with downturned corners of the mouth and furrowed eyebrows, indicating anger or sadness. Her mouth is closed, and her facial expression is somewhat dull. The woman on the right has a more positive emotional state. Her gaze is directly facing the other person, and her facial expression is soft and friendly, seemingly listening attentively. Their body language suggests a positive and lively interaction.</vis_desc>
<aud_desc>In the audio, the character's voice starts off urgent and then becomes louder and more intense as the conversation progresses. In the text, the subtitle reads: "But when will he come?" This sentence may be the woman on the left questioning the woman on the right.</aud_desc>
<think>Based on the facial expressions and body language of the woman on the left in the video clues, her facial expression is urgent, furrowed eyebrows, and with a dull appearance, indicating anger or sadness. Her closed mouth suggests she may be angry or unhappy. Although the audio clue describes the character's voice as urgent and then becomes louder and more intense, indicating a build-up of emotions, this may indicate that her evaluation or reaction to the woman on the right is more excited or excited, possibly due to her positive and lively attitude. Therefore, this sentence may express the woman on the left's dissatisfaction or frustration towards the other person's lack of response or unexpectedness, aligning with the overall lively and excited demeanor displayed by the woman on the right.</think>
<answer>angry</answer>
[2025-11-07 08:00:00,440] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 401/2088 [5:50:28<25:01:48, 53.41s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.89855295258391, 'learning_rate': 8.079501915708811e-07, 'completion_length': 220.58203125, 'rewards/av_format_reward': 0.90625, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.429467111825943, 'reward': 1.8591545820236206, 'reward_std': 0.4035063087940216, 'kl': 0.02911376953125, 'epoch': 0.38}
 19%|█▉        | 401/2088 [5:50:28<25:01:48, 53.41s/it][2025-11-07 08:00:50,795] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 402/2088 [5:51:18<24:35:07, 52.50s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9920923311981624, 'learning_rate': 8.074712643678161e-07, 'completion_length': 233.45703125, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5567372739315033, 'reward': 1.9395498633384705, 'reward_std': 0.5215919315814972, 'kl': 0.0284423828125, 'epoch': 0.39}
 19%|█▉        | 402/2088 [5:51:18<24:35:07, 52.50s/it][2025-11-07 08:01:41,485] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 403/2088 [5:52:09<24:19:04, 51.96s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1885011569720474, 'learning_rate': 8.069923371647509e-07, 'completion_length': 224.5234375, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.5041944533586502, 'reward': 1.9299757480621338, 'reward_std': 0.43795372545719147, 'kl': 0.030029296875, 'epoch': 0.39}
 19%|█▉        | 403/2088 [5:52:09<24:19:04, 51.96s/it][2025-11-07 08:02:33,914] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 404/2088 [5:53:01<24:22:10, 52.10s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.120706114948094, 'learning_rate': 8.065134099616858e-07, 'completion_length': 227.0234375, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.39979442954063416, 'reward': 1.7357318997383118, 'reward_std': 0.5986416339874268, 'kl': 0.02978515625, 'epoch': 0.39}
 19%|█▉        | 404/2088 [5:53:01<24:22:10, 52.10s/it][2025-11-07 08:03:27,220] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 405/2088 [5:53:54<24:31:31, 52.46s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.091524060413651, 'learning_rate': 8.060344827586206e-07, 'completion_length': 226.3359375, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.4140625, 'rewards/think_step_av_with_neutral_reward': 0.40498819947242737, 'reward': 1.654988169670105, 'reward_std': 0.39736151695251465, 'kl': 0.03125, 'epoch': 0.39}
 19%|█▉        | 405/2088 [5:53:54<24:31:31, 52.46s/it][2025-11-07 08:04:17,054] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 406/2088 [5:54:44<24:08:30, 51.67s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8553662437809413, 'learning_rate': 8.055555555555556e-07, 'completion_length': 232.08203125, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5188280493021011, 'reward': 1.8586718440055847, 'reward_std': 0.4230187386274338, 'kl': 0.02899169921875, 'epoch': 0.39}
 19%|█▉        | 406/2088 [5:54:44<24:08:30, 51.67s/it][2025-11-07 08:05:08,848] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 19%|█▉        | 407/2088 [5:55:36<24:08:41, 51.71s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0528342069297105, 'learning_rate': 8.050766283524904e-07, 'completion_length': 220.05859375, 'rewards/av_format_reward': 0.890625, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.49131734669208527, 'reward': 1.9014735221862793, 'reward_std': 0.5605675131082535, 'kl': 0.0299072265625, 'epoch': 0.39}
 19%|█▉        | 407/2088 [5:55:36<24:08:41, 51.71s/it][2025-11-07 08:06:00,278] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 408/2088 [5:56:27<24:05:29, 51.62s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9605987657930957, 'learning_rate': 8.045977011494253e-07, 'completion_length': 220.06640625, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5618103444576263, 'reward': 1.991497814655304, 'reward_std': 0.4450341761112213, 'kl': 0.03076171875, 'epoch': 0.39}
 20%|█▉        | 408/2088 [5:56:27<24:05:29, 51.62s/it][2025-11-07 08:06:52,679] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 409/2088 [5:57:20<24:11:08, 51.86s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.046302784708377, 'learning_rate': 8.0411877394636e-07, 'completion_length': 226.6796875, 'rewards/av_format_reward': 0.9609375, 'rewards/accuracy_reward': 0.4140625, 'rewards/think_step_av_with_neutral_reward': 0.4086030423641205, 'reward': 1.7836030721664429, 'reward_std': 0.45794470608234406, 'kl': 0.0284423828125, 'epoch': 0.39}
 20%|█▉        | 409/2088 [5:57:20<24:11:08, 51.86s/it][2025-11-07 08:07:43,583] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 410/2088 [5:58:11<24:02:18, 51.57s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.128451690299826, 'learning_rate': 8.03639846743295e-07, 'completion_length': 222.3203125, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5171328634023666, 'reward': 1.899945318698883, 'reward_std': 0.5142683982849121, 'kl': 0.03082275390625, 'epoch': 0.39}
 20%|█▉        | 410/2088 [5:58:11<24:02:18, 51.57s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01776.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/942.mp4
[DEBUG] Step 410
[DEBUG] prompt_completion_ids shape: torch.Size([32, 314])
[DEBUG] Shortest output (len=165): <vis_desc>In the video, we see a woman in a dimly lit environment, possibly indoors. Her facial expression seems joyful, with a slight upward curve at the corners of her mouth and neatly shaped eyes, which are typical features of a friendly or confident smile. Her gaze is soft and gentle, as if she is enjoying a relaxed conversation.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the person's emotional state. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman’s visual cues—her joyful demeanor, friendly gaze, and neatly shaped eyes—strongly suggest a positive emotional state, possibly indicating that she is having a pleasant conversation or experiencing an enjoyable moment.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=314): <vis_desc>In the video, the screen shows a male character wearing traditional Chinese clothing, possibly engaged in a formal conversation or debate. In the video, he has a serious facial expression, with herbs in his hand possibly indicating a special custom or traditional hairstyle, and a certain degree of forehead wrinkles, indicating a well-nourished and experienced character. He also slightly furrows his brows, which is usually seen in recognition or serious interaction.</vis_desc>
<aud_desc>In the audio, the character's tone is serious, and combined with the text content, it can be felt that the character is questioning the other person's abilities or behavior. In the text, the subtitle reads: "We have another form of tradition called I Dreamed It Up." This sentence could be the male character's response to the other person's question or statement.</aud_desc>
<think>Based on the serious expression, herthy in hand, furrowed brows, and slightly curved mouth, it is possible that the male character has experienced, formal attire, and focused gaze, as well as the serious tone, questioning behavior, and overall serious attitude displayed by the character in the video clues, it can be inferred that the male character holds a certain level of authority and solemnity in this conversation, and he may have a serious and purposeful attitude towards the other person's statement or question. Therefore, this sentence could be the male character's way of questioning the other person's abilities or behavior, expressing his stance or viewpoint.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2942.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01044.mp4
[DEBUG] Step 410
[DEBUG] prompt_completion_ids shape: torch.Size([32, 289])
[DEBUG] Shortest output (len=133): <vis_desc>In the video, we see a woman with furrowed brows and slightly downturned mouth, her facial expression appearing quite sad. Her gaze seems to be directed towards someone or something outside the frame, contributing to an overall feeling of melancholy.</vis_desc>
<aud_desc>In the audio, there are obvious sobbing sounds and the voice sounds tearful, indicating that the character is experiencing sadness. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think> combined, the visual and auditory cues strongly suggest that the woman is experiencing sadness.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=289): <vis_desc>In the video, we see a woman with slightly furrowed brows and slightly downturned corners of her mouth, which may indicate that she is experiencing some inner pain or discomfort. Her gaze appears somewhat vacant, as if she is staring straight ahead with no clear emotional cues. The background is an indoor setting that looks like an old house, possibly her home or a place she used to call home. Her facial expression and posture suggest she may be thinking about something sad or sadful.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Although the auditory cues and subtitle content do not offer additional emotional information, the visual cues strongly suggest a sad emotion. The woman's slightly furrowed brows and downturned mouth corners suggest inner pain or sorrow, while her vacant gaze implies she is contemplating sadness. The old house setting, hinted at through background context, further underscores a home environment that might have been cherished or been difficult to leave. Although the audio does not convey emotion, the visual expression alone conveys a subdued sense of sadness. Taken together, the visual appearance of the woman in a state of melancholy—marked by emotional intensity and an absence of positive facial expression—supports the inference that she is indeed feeling sad.</think>
<answer>sad</answer>
[2025-11-07 08:08:36,259] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 411/2088 [5:59:03<24:10:43, 51.90s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.053521312268289, 'learning_rate': 8.031609195402298e-07, 'completion_length': 231.19921875, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5024134516716003, 'reward': 1.8266322612762451, 'reward_std': 0.42372503876686096, 'kl': 0.02777099609375, 'epoch': 0.39}
 20%|█▉        | 411/2088 [5:59:03<24:10:43, 51.90s/it][2025-11-07 08:09:25,531] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 412/2088 [5:59:53<23:47:46, 51.11s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1428695296830442, 'learning_rate': 8.026819923371647e-07, 'completion_length': 227.48046875, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5833284854888916, 'reward': 2.013016104698181, 'reward_std': 0.5471828430891037, 'kl': 0.03106689453125, 'epoch': 0.39}
 20%|█▉        | 412/2088 [5:59:53<23:47:46, 51.11s/it][2025-11-07 08:10:16,388] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 413/2088 [6:00:43<23:44:45, 51.04s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0093313913968394, 'learning_rate': 8.022030651340996e-07, 'completion_length': 225.79296875, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5149099677801132, 'reward': 1.862566351890564, 'reward_std': 0.47724220156669617, 'kl': 0.0306396484375, 'epoch': 0.4}
 20%|█▉        | 413/2088 [6:00:43<23:44:45, 51.04s/it][2025-11-07 08:11:09,227] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 414/2088 [6:01:36<23:59:00, 51.58s/it]                                                       {'loss': 0.0015, 'grad_norm': 1.938663281917479, 'learning_rate': 8.017241379310345e-07, 'completion_length': 226.01171875, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.44180504977703094, 'reward': 1.7972738146781921, 'reward_std': 0.46267618238925934, 'kl': 0.036376953125, 'epoch': 0.4}
 20%|█▉        | 414/2088 [6:01:36<23:59:00, 51.58s/it][2025-11-07 08:12:00,386] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 415/2088 [6:02:27<23:54:37, 51.45s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9902157219058192, 'learning_rate': 8.012452107279694e-07, 'completion_length': 214.76953125, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.41796875, 'rewards/think_step_av_with_neutral_reward': 0.3867049217224121, 'reward': 1.6132673621177673, 'reward_std': 0.5619258284568787, 'kl': 0.03173828125, 'epoch': 0.4}
 20%|█▉        | 415/2088 [6:02:27<23:54:37, 51.45s/it][2025-11-07 08:12:52,731] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 416/2088 [6:03:20<24:01:14, 51.72s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9926987290711724, 'learning_rate': 8.007662835249042e-07, 'completion_length': 229.85546875, 'rewards/av_format_reward': 0.91796875, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5994725823402405, 'reward': 2.1150975823402405, 'reward_std': 0.506645917892456, 'kl': 0.0303955078125, 'epoch': 0.4}
 20%|█▉        | 416/2088 [6:03:20<24:01:14, 51.72s/it][2025-11-07 08:13:40,118] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|█▉        | 417/2088 [6:04:07<23:24:10, 50.42s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.076358839140059, 'learning_rate': 8.00287356321839e-07, 'completion_length': 214.6171875, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.6953125, 'rewards/think_step_av_with_neutral_reward': 0.6367765069007874, 'reward': 2.074276566505432, 'reward_std': 0.3810037672519684, 'kl': 0.0313720703125, 'epoch': 0.4}
 20%|█▉        | 417/2088 [6:04:07<23:24:10, 50.42s/it][2025-11-07 08:14:32,762] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 418/2088 [6:05:00<23:41:54, 51.09s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0052503573531064, 'learning_rate': 7.998084291187739e-07, 'completion_length': 227.6875, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5302582085132599, 'reward': 1.8271331787109375, 'reward_std': 0.45109015703201294, 'kl': 0.0283203125, 'epoch': 0.4}
 20%|██        | 418/2088 [6:05:00<23:41:54, 51.09s/it][2025-11-07 08:15:21,903] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 419/2088 [6:05:49<23:24:49, 50.50s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9290224575619146, 'learning_rate': 7.993295019157088e-07, 'completion_length': 224.078125, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5101258158683777, 'reward': 1.9788758158683777, 'reward_std': 0.4412085711956024, 'kl': 0.02874755859375, 'epoch': 0.4}
 20%|██        | 419/2088 [6:05:49<23:24:49, 50.50s/it][2025-11-07 08:16:10,777] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 420/2088 [6:06:38<23:10:23, 50.01s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8933298989876914, 'learning_rate': 7.988505747126436e-07, 'completion_length': 223.71484375, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.34448516368865967, 'reward': 1.625735104084015, 'reward_std': 0.4300948232412338, 'kl': 0.02947998046875, 'epoch': 0.4}
 20%|██        | 420/2088 [6:06:38<23:10:23, 50.01s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1939.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12461.mp4
[DEBUG] Step 420
[DEBUG] prompt_completion_ids shape: torch.Size([32, 331])
[DEBUG] Shortest output (len=157): <vis_desc>In the video, we see a character wearing a white robes performing in a traditional Korean style. His facial expression shows happiness and clarity.</vis_desc>
<aud_desc>In the audio, the voice is calm and the tone is steady. In the text, the subtitle says, "Something is wrong with the cable." This sentence may be spoken by the character when observing or troubleshooting something.</aud_desc>
<think>Based on the character's joyful facial expression and his body language showing "happiness," it can be inferred that he is likely very pleased with the news he heard. Additionally, the calm voice and steady tone in the audio further support this inference. Therefore, this sentence expresses the character's excitement and satisfaction.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=331): <vis_desc>In the video, we see a male character wearing a white shirt, standing in a traditional East Asian-style room with wooden floors and curtains. The background has calligraphy on the walls, which may indicate that the character is in a place of importance or importance. In the video, his facial expression appears to be of a pleasant mood, with a slight upward curve at the corners of his mouth, bright eyes, and a smile, suggesting that he may be happy or engaged in a pleasant conversation. His body posture is natural, without any signs of tension or discomfort, and he is looking directly at the character on the right side, which usually indicates that he is engaged in a friendly or formal conversation.</vis_desc>
<aud_desc>In the audio, footsteps and distant chatter can be heard. The specific meanings of these sounds need to be analyzed in context. In the text, the subtitle says, "I'm just a visitor here." This sentence may be a statement or response from the male character to someone.</aud_desc>
<think>Based on the pleasant facial expression and smile of the male character in the video clues, as well as the description of traditional East Asian-style rooms and the presence of calligraphy on the walls, we can infer that this sentence may have a positive tone, meaning that the male character is a guest and is happy and engaged in the interaction with someone. The sounds of footsteps and distant chatter in the audio clues also support this inference. Therefore, this sentence may be a friendly or humorous way of expressing the male character's attitude and attitude towards the other person.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2002.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00725.mp4
[DEBUG] Step 420
[DEBUG] prompt_completion_ids shape: torch.Size([32, 281])
[DEBUG] Shortest output (len=170): <vis_desc>In the video, we see a woman with a smile on her face, slightly opening her mouth, indicating that she may have heard something positive. Her hands are fidgeting, and her posture looks relaxed, suggesting that she may be having a friendly conversation.</vis_desc>
<aud_desc>In the audio, the character expresses genuine happiness through laughter. In the text, the subtitle says, "Sorry about the curlers, it's Thursday." This sentence shows the woman's remorse and happiness.</aud_desc>
<think>Based on the woman's smiling and relaxed posture in the video clues, as well as the description of the character expressing genuine happiness through laughter in the audio clues, we can infer that this sentence is the woman sincerely expressing her apologies and satisfaction with the curlers.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=281): <vis_desc>In the video, we see a male character wearing a white shirt and suspenders. In the video, his eyes are red and his mouth is wide open, showing a very frightened expression. His body is submerged in sand, which appears to be a sandy battlefield. His mouth movements suggest that he is speaking with an aggressive tone, "Let the dogs be what they are," he is shouting, and in the voice, he is wearing traditional Chinese clothing, carrying a sense of aggression and determination.</vis_desc>
<aud_desc>In the audio, the voice sounds very loud and vigorous, with a sound pace of 143bis, suggesting that the character is shouting loudly and effort is being put in to move. In the text, the subtitle says, "Let the dogs be what they are, let the cat be a cat." This sentence may be a warning or encouragement from the male character to someone.</aud_desc>
<think>Based on the frightened expression and body submerged in sand in the video clues, as well as the description of the loud and vigorous voice with a fast pace in the audio clues, we can infer that this sentence carries a sense of anger or excitement. The male character may be shouting loudly and effort is being put in to express his dissatisfaction with the current situation and his determination to be free from violence.</think>
<answer>fear</answer>
[2025-11-07 08:17:00,917] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 421/2088 [6:07:28<23:10:36, 50.05s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9505377284240526, 'learning_rate': 7.983716475095785e-07, 'completion_length': 217.265625, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.40754687786102295, 'reward': 1.5794218182563782, 'reward_std': 0.3723627030849457, 'kl': 0.03216552734375, 'epoch': 0.4}
 20%|██        | 421/2088 [6:07:28<23:10:36, 50.05s/it][2025-11-07 08:17:52,839] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 422/2088 [6:08:20<23:25:22, 50.61s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.046011959125619, 'learning_rate': 7.978927203065134e-07, 'completion_length': 224.33203125, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.75, 'rewards/think_step_av_with_neutral_reward': 0.5464099645614624, 'reward': 2.112816095352173, 'reward_std': 0.4905548393726349, 'kl': 0.02978515625, 'epoch': 0.4}
 20%|██        | 422/2088 [6:08:20<23:25:22, 50.61s/it][2025-11-07 08:18:41,260] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 423/2088 [6:09:08<23:06:17, 49.96s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0735090244492933, 'learning_rate': 7.974137931034483e-07, 'completion_length': 222.4765625, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.66796875, 'rewards/think_step_av_with_neutral_reward': 0.5652670860290527, 'reward': 2.0847983360290527, 'reward_std': 0.4598677307367325, 'kl': 0.031982421875, 'epoch': 0.41}
 20%|██        | 423/2088 [6:09:08<23:06:17, 49.96s/it][2025-11-07 08:19:34,576] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 424/2088 [6:10:02<23:33:22, 50.96s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.901334957783586, 'learning_rate': 7.969348659003831e-07, 'completion_length': 228.28125, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.4781307280063629, 'reward': 1.8960995078086853, 'reward_std': 0.4970874786376953, 'kl': 0.029541015625, 'epoch': 0.41}
 20%|██        | 424/2088 [6:10:02<23:33:22, 50.96s/it][2025-11-07 08:20:25,952] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 425/2088 [6:10:53<23:35:58, 51.09s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.001081378539272, 'learning_rate': 7.964559386973179e-07, 'completion_length': 222.18359375, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4565882235765457, 'reward': 1.7183068990707397, 'reward_std': 0.5124275237321854, 'kl': 0.0308837890625, 'epoch': 0.41}
 20%|██        | 425/2088 [6:10:53<23:35:58, 51.09s/it][2025-11-07 08:21:18,724] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 426/2088 [6:11:46<23:49:06, 51.59s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8773063668621146, 'learning_rate': 7.959770114942529e-07, 'completion_length': 223.51171875, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4494989216327667, 'reward': 1.7112176418304443, 'reward_std': 0.3771354705095291, 'kl': 0.029296875, 'epoch': 0.41}
 20%|██        | 426/2088 [6:11:46<23:49:06, 51.59s/it][2025-11-07 08:22:09,901] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 427/2088 [6:12:37<23:44:48, 51.47s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0617378793042094, 'learning_rate': 7.954980842911877e-07, 'completion_length': 221.78515625, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.64453125, 'rewards/think_step_av_with_neutral_reward': 0.532962441444397, 'reward': 1.989993691444397, 'reward_std': 0.3778854310512543, 'kl': 0.030517578125, 'epoch': 0.41}
 20%|██        | 427/2088 [6:12:37<23:44:48, 51.47s/it][2025-11-07 08:23:02,356] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 20%|██        | 428/2088 [6:13:29<23:52:08, 51.76s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0369293287523873, 'learning_rate': 7.950191570881226e-07, 'completion_length': 226.06640625, 'rewards/av_format_reward': 0.63671875, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.5030062347650528, 'reward': 1.6866000890731812, 'reward_std': 0.6075599789619446, 'kl': 0.03173828125, 'epoch': 0.41}
 20%|██        | 428/2088 [6:13:29<23:52:08, 51.76s/it][2025-11-07 08:23:51,169] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 429/2088 [6:14:18<23:26:47, 50.88s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9958469186999808, 'learning_rate': 7.945402298850574e-07, 'completion_length': 226.94921875, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.45465464890003204, 'reward': 1.7124671339988708, 'reward_std': 0.5366720259189606, 'kl': 0.0306396484375, 'epoch': 0.41}
 21%|██        | 429/2088 [6:14:18<23:26:47, 50.88s/it][2025-11-07 08:24:39,876] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 430/2088 [6:15:07<23:07:56, 50.23s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9874640455267587, 'learning_rate': 7.940613026819924e-07, 'completion_length': 217.44140625, 'rewards/av_format_reward': 0.54296875, 'rewards/accuracy_reward': 0.6796875, 'rewards/think_step_av_with_neutral_reward': 0.619472324848175, 'reward': 1.8421285152435303, 'reward_std': 0.4011571854352951, 'kl': 0.033447265625, 'epoch': 0.41}
 21%|██        | 430/2088 [6:15:07<23:07:56, 50.23s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10782.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05895.mp4
[DEBUG] Step 430
[DEBUG] prompt_completion_ids shape: torch.Size([32, 329])
[DEBUG] Shortest output (len=140): <aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, the subtitle says "All right." This sentence may be the woman's response to a suggestion or request.</aud_desc>
<vis_desc>In the video, we see a woman standing outdoors at night. Her facial expression appears surprised and ecstatic.</vis_desc>
<think>Based on the surprised and ecstatic facial expression of the woman in the video clues, as well as the inquiry in the subtitle "All right," we can infer that this sentence may carry a positive emotion, such as the woman agreeing to something or accepting a proposal.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=329): <vis_desc>In the video, the screen shows a woman standing outdoors at night, carrying a green bag. In the video, her eyes are wide open and she is staring straight at someone, showing a surprised or scared expression. Her mouth is slightly open, and her body posture is stiff and herely, as if she is listening attentively or anticipating something. The environment suggests that it may be a special occasion, with curtains and decorations visible, possibly in a family setting.</vis_desc>
<aud_desc>In the audio, footsteps and panting sounds can be heard. These sounds indicate that the woman is walking along the house at a fast pace. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman’s visual cues—particularly her wide-open eyes, stiff and ehonely posture—strongly convey surprise and alertness, consistent with her sudden reaction to something unexpected or alarming in the night setting. The fact that she is carrying a green bag suggests she may have just noticed something peculiar on the ground or inadvertently left behind, which further supports her immediate, heightened alertness. Although the auditory information of footsteps and panting sounds adds context to her hurried pace, which typically accompanies tension or distress, these sounds do not contradict the overall display of surprise and alertness. Together, these multimodal signals—primarily driven by her visual appearance integrated with the ambient alertness markers—supports the inference that the woman is experiencing surprise and fear, as embodied by her facial expression and body language in this unusual nighttime scenario.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2414.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5370.mp4
[DEBUG] Step 430
[DEBUG] prompt_completion_ids shape: torch.Size([32, 323])
[DEBUG] Shortest output (len=183): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned mouth, seemingly expressing dissatisfaction or frustration.</vis_desc>
<aud_desc>In the audio, the character's tone is stern and aggressive, conveying a sense of accusation and dissatisfaction. In the text, the subtitle reads, "Then why do you want to be near? What's your advantage over her?" This sentence may be the female character's evaluation or complaint of another person.</aud_desc>
<think>Based on the video clues of the female character's furrowed brows and downturned mouth, as well as the audio clue of the character's stern and aggressive tone, we can infer that this sentence carries a sense of anger or resentment. The female character may be unhappy with the other person's neglect or rejection of her, hence expressing such words.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=323): <vis_desc>In the video, the screen shows a middle-aged male character, possibly in an outdoor environment, as the background appears to be a field with reeds. At the beginning of the video, his brow is furrowed, and his facial expression is very sad. His mouth is moving, seemingly speaking or crying out loud. Over time, he slowly turns his head to the right side. At the end of the video, he closes his mouth and starts sobbing.</vis_desc>
<aud_desc>In the audio, combined with the text content, there is a crying tone, expressing the character's helplessness and sadness. Together with the text content, it also shows tenderness. In the text, the subtitle says, "I'll let you pressure yourself, but you shouldn't press me." This sentence may be an expression of helplessness and sadness from the male character towards his family.</aud_desc>
<think>Based on the slow head turn and the action of closing the mouth in the video clues, it can be inferred that the male character is slowly moving towards the right side, and he is sobbing, which indicates that he may be experiencing emotional pain, possibly due to a terrible illness, a difficult situation with his family, or some unsolvable emotional issues. Taking into account the description of the crying tone in the audio clues, it can be inferred that this sentence aims to express the male character's helplessness and sadness, as he may be feeling forced or constrained by his family, preventing him from doing what he wanted to.</think>
<answer>sad</answer>
[2025-11-07 08:25:30,900] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 431/2088 [6:15:58<23:13:42, 50.47s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0089820591993823, 'learning_rate': 7.935823754789272e-07, 'completion_length': 223.9609375, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.4925331771373749, 'reward': 1.996439278125763, 'reward_std': 0.3806571364402771, 'kl': 0.02899169921875, 'epoch': 0.41}
 21%|██        | 431/2088 [6:15:58<23:13:42, 50.47s/it][2025-11-07 08:26:20,478] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 432/2088 [6:16:48<23:05:30, 50.20s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9985831298833514, 'learning_rate': 7.931034482758621e-07, 'completion_length': 222.33984375, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.44346633553504944, 'reward': 1.5879976749420166, 'reward_std': 0.44462284445762634, 'kl': 0.0308837890625, 'epoch': 0.41}
 21%|██        | 432/2088 [6:16:48<23:05:30, 50.20s/it][2025-11-07 08:27:10,828] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 433/2088 [6:17:38<23:05:55, 50.25s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0365670497501336, 'learning_rate': 7.926245210727968e-07, 'completion_length': 223.18359375, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.7421875, 'rewards/think_step_av_with_neutral_reward': 0.6006949841976166, 'reward': 2.194445013999939, 'reward_std': 0.45961904525756836, 'kl': 0.0289306640625, 'epoch': 0.41}
 21%|██        | 433/2088 [6:17:38<23:05:55, 50.25s/it][2025-11-07 08:28:03,852] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 434/2088 [6:18:31<23:28:04, 51.08s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.059180429157764, 'learning_rate': 7.921455938697317e-07, 'completion_length': 223.6328125, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.76953125, 'rewards/think_step_av_with_neutral_reward': 0.601151168346405, 'reward': 2.2300573587417603, 'reward_std': 0.46338215470314026, 'kl': 0.03179931640625, 'epoch': 0.42}
 21%|██        | 434/2088 [6:18:31<23:28:04, 51.08s/it][2025-11-07 08:28:55,629] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 435/2088 [6:19:23<23:32:59, 51.29s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1124685628798128, 'learning_rate': 7.916666666666666e-07, 'completion_length': 223.52734375, 'rewards/av_format_reward': 0.921875, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.5262841880321503, 'reward': 2.0184717178344727, 'reward_std': 0.46810661256313324, 'kl': 0.030029296875, 'epoch': 0.42}
 21%|██        | 435/2088 [6:19:23<23:32:59, 51.29s/it][2025-11-07 08:29:45,254] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 436/2088 [6:20:12<23:18:23, 50.79s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0763973646980993, 'learning_rate': 7.911877394636015e-07, 'completion_length': 230.86328125, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.6953125, 'rewards/think_step_av_with_neutral_reward': 0.5568153262138367, 'reward': 2.099784195423126, 'reward_std': 0.4495505690574646, 'kl': 0.0299072265625, 'epoch': 0.42}
 21%|██        | 436/2088 [6:20:12<23:18:23, 50.79s/it][2025-11-07 08:30:33,660] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 437/2088 [6:21:01<22:57:51, 50.07s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0877543348237326, 'learning_rate': 7.907088122605363e-07, 'completion_length': 226.9609375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.4716223329305649, 'reward': 1.717716097831726, 'reward_std': 0.5321995913982391, 'kl': 0.03375244140625, 'epoch': 0.42}
 21%|██        | 437/2088 [6:21:01<22:57:51, 50.07s/it][2025-11-07 08:31:24,027] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 438/2088 [6:21:51<22:59:26, 50.16s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0224763622953086, 'learning_rate': 7.902298850574712e-07, 'completion_length': 225.8203125, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.421875, 'rewards/think_step_av_with_neutral_reward': 0.38661617040634155, 'reward': 1.5936473608016968, 'reward_std': 0.4702421575784683, 'kl': 0.032958984375, 'epoch': 0.42}
 21%|██        | 438/2088 [6:21:51<22:59:26, 50.16s/it][2025-11-07 08:32:16,215] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 439/2088 [6:22:43<23:15:19, 50.77s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.1860158763453383, 'learning_rate': 7.897509578544061e-07, 'completion_length': 225.5234375, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5655537247657776, 'reward': 1.9171162843704224, 'reward_std': 0.4130653738975525, 'kl': 0.03143310546875, 'epoch': 0.42}
 21%|██        | 439/2088 [6:22:43<23:15:19, 50.77s/it][2025-11-07 08:33:09,440] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 440/2088 [6:23:37<23:34:42, 51.51s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.160506780545469, 'learning_rate': 7.89272030651341e-07, 'completion_length': 226.125, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5797356069087982, 'reward': 2.0172354578971863, 'reward_std': 0.4862290471792221, 'kl': 0.03009033203125, 'epoch': 0.42}
 21%|██        | 440/2088 [6:23:37<23:34:42, 51.51s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9042.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8824.mp4
[DEBUG] Step 440
[DEBUG] prompt_completion_ids shape: torch.Size([32, 257])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a young female character with a bright expression and a smile on her face, smiling slightly, looking very happy. Her eyes are shining, giving a sense of joy and pleasure.</vis_desc>
<aud_desc>In the audio, the character's voice expresses a relaxed and playful tone. In the text, the subtitle says, "Oh, deceived again." This sentence may be the female character's expression of disappointment or surprise.</aud_desc>
<think>Based on the video clues of the female character's bright expression and smile, as well as her happy attitude, we can infer that this sentence may have a relaxed and playful tone. Therefore, this sentence may be the female character laughing or joking about something, which aligns with the overall display of happiness by the character.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=257): <vis_desc>In the video, we see a man standing in an indoor environment with a blue lock and safety barriers. His facial expression is serious, his mouth is open, and it seems like he is speaking or shouting loudly, possibly encountering some difficulties or something unforeseen happening that has caught his attention.</vis_desc>
<aud_desc>In the audio, the character's volume is very loud and the tone is high-pitched and loud, conveying strong emotions such as surprise and excitement. In the text, the subtitle reads: "Hello, Hello, did you meet him? It's really nice of you to visit me."</aud_desc>
<think>The combined visual and auditory cues indicate that the man is experiencing surprise. Visually, his serious facial expression combined with an open mouth and upward gaze suggests an unexpected loud sound or an unforeseen event that caught his attention. Audibly, the high-pitched and loud tone aligns with heightened emotions such as surprise and excitement. When language is not employed, nonverbal signals converge logically to confirm that the character is reacting to an startling but unexpected visitation, presenting a coherent natural response. Together, these multimodal signals create a confident impression that the man is reacting with surprise.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/10425.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4395.mp4
[DEBUG] Step 440
[DEBUG] prompt_completion_ids shape: torch.Size([32, 310])
[DEBUG] Shortest output (len=176): <vis_desc>In the video, we see a male character standing in the dark background. He is wearing a scout uniform and has a serious facial expression. His mouth moves slightly and his gaze is direct, as if he is talking to someone.</vis_desc>
<aud_desc>In the audio, speech is steady and conversational. In the text, the subtitle reads, "Hello, everyone. I'm Dr.cheng." This sentence may be the male character's greeting or introduction.</aud_desc>
<think>Based on the serious facial expression and mouth movement of the male character in the video, as well as the steady and conversational tone in the audio, we can infer that this sentence is likely the male character's way of greeting or introducing himself. Therefore, this sentence expresses the male character's confidence and calmness.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=310): <vis_desc>In the video, the screen shows a woman in an indoor setting. In the video, her eyes are wide open, looking directly at someone, with a somewhat surprised expression on her face. Her mouth is slightly open, as if speaking or reacting to someone. Overall, she seems to be experiencing an unexpected event or conversation, and this event makes her feel surprised and confused.</vis_desc>
<aud_desc>In the audio, footsteps and a hurried footsteps sound can be heard. It can be inferred that the woman in the video is not used to large movements, and she may have encountered some problem while walking, causing her to feel uneasy. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman's wide-open eyes and slightly open mouth visually convey a natural response to an unexpected stimulus, suggesting surprise and confusion. Her direct gaze also indicates heightened focus and reaction to the immediate environment, further reinforcing the feeling of being caught off guard. The combination of these facial cues, combined with the auditory clue of hurried footsteps, suggests she is experiencing a moment of uncertainty, discomfort, or surprise in a seemingly uncontrollable way. Although the background description and subtitle content do not provide emotional information, the visual and auditory signals together strongly indicate that she is feeling surprise. Her body language—open eyes and mouth—along with the tense contextual sound of hurried footsteps, create a coherent and authentic expression of an unsettling or surprising emotional state.</think>
<answer>surprise</answer>
[2025-11-07 08:33:59,199] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 441/2088 [6:24:26<23:19:28, 50.98s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0756240658615934, 'learning_rate': 7.887931034482759e-07, 'completion_length': 226.94921875, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.5096282213926315, 'reward': 1.7518157362937927, 'reward_std': 0.5398403704166412, 'kl': 0.03173828125, 'epoch': 0.42}
 21%|██        | 441/2088 [6:24:26<23:19:28, 50.98s/it][2025-11-07 08:34:48,674] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 442/2088 [6:25:16<23:06:12, 50.53s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0852584827130825, 'learning_rate': 7.883141762452106e-07, 'completion_length': 228.9609375, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5248892307281494, 'reward': 1.9741079807281494, 'reward_std': 0.395345002412796, 'kl': 0.0322265625, 'epoch': 0.42}
 21%|██        | 442/2088 [6:25:16<23:06:12, 50.53s/it][2025-11-07 08:35:40,589] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██        | 443/2088 [6:26:08<23:16:45, 50.95s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9895401562479456, 'learning_rate': 7.878352490421456e-07, 'completion_length': 220.484375, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.49950239062309265, 'reward': 1.823721170425415, 'reward_std': 0.49797943234443665, 'kl': 0.03070068359375, 'epoch': 0.42}
 21%|██        | 443/2088 [6:26:08<23:16:45, 50.95s/it][2025-11-07 08:36:30,659] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 444/2088 [6:26:58<23:08:42, 50.68s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0015413167636353, 'learning_rate': 7.873563218390804e-07, 'completion_length': 216.48046875, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.46467673778533936, 'reward': 1.8123330473899841, 'reward_std': 0.4667302668094635, 'kl': 0.030029296875, 'epoch': 0.43}
 21%|██▏       | 444/2088 [6:26:58<23:08:42, 50.68s/it][2025-11-07 08:37:19,896] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 445/2088 [6:27:47<22:55:59, 50.25s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0420940336258497, 'learning_rate': 7.868773946360153e-07, 'completion_length': 223.94140625, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.594318151473999, 'reward': 2.016193211078644, 'reward_std': 0.5872427523136139, 'kl': 0.028564453125, 'epoch': 0.43}
 21%|██▏       | 445/2088 [6:27:47<22:55:59, 50.25s/it][2025-11-07 08:38:11,154] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 446/2088 [6:28:38<23:03:26, 50.55s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1116648125830264, 'learning_rate': 7.863984674329501e-07, 'completion_length': 231.51171875, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.43910926580429077, 'reward': 1.6852030158042908, 'reward_std': 0.639319658279419, 'kl': 0.0289306640625, 'epoch': 0.43}
 21%|██▏       | 446/2088 [6:28:38<23:03:26, 50.55s/it][2025-11-07 08:39:01,788] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 447/2088 [6:29:29<23:03:15, 50.58s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.031568150077961, 'learning_rate': 7.859195402298851e-07, 'completion_length': 225.7578125, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.45495256781578064, 'reward': 1.7401087880134583, 'reward_std': 0.45090028643608093, 'kl': 0.03033447265625, 'epoch': 0.43}
 21%|██▏       | 447/2088 [6:29:29<23:03:15, 50.58s/it][2025-11-07 08:39:54,847] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 21%|██▏       | 448/2088 [6:30:22<23:22:46, 51.32s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9437066536493703, 'learning_rate': 7.854406130268199e-07, 'completion_length': 228.87109375, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.444925993680954, 'reward': 1.6011760234832764, 'reward_std': 0.5436117053031921, 'kl': 0.02947998046875, 'epoch': 0.43}
 21%|██▏       | 448/2088 [6:30:22<23:22:46, 51.32s/it][2025-11-07 08:40:43,702] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 449/2088 [6:31:11<23:01:42, 50.58s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.980161881433066, 'learning_rate': 7.849616858237548e-07, 'completion_length': 229.88671875, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.4612303227186203, 'reward': 1.8479490280151367, 'reward_std': 0.491598516702652, 'kl': 0.0283203125, 'epoch': 0.43}
 22%|██▏       | 449/2088 [6:31:11<23:01:42, 50.58s/it][2025-11-07 08:41:41,281] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 450/2088 [6:32:08<23:58:10, 52.68s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0445402573143623, 'learning_rate': 7.844827586206895e-07, 'completion_length': 227.64453125, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.4532608240842819, 'reward': 1.7110733389854431, 'reward_std': 0.5416353195905685, 'kl': 0.030029296875, 'epoch': 0.43}
 22%|██▏       | 450/2088 [6:32:08<23:58:10, 52.68s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12854.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10667.mp4
[DEBUG] Step 450
[DEBUG] prompt_completion_ids shape: torch.Size([32, 246])
[DEBUG] Shortest output (len=155): <vis_desc>In the video, we see a female character with wide-open eyes and slightly downturned mouth, seemingly looking at something or someone outside the frame, showing a surprised expression.</vis_desc>
<aud_desc>In the audio, the character's voice sounds surprised when saying "Ah, Cai E." This sentence may be the female character's reaction to something unexpected.</aud_desc>
<think>Based on the surprised expression shown by the female character in the video clues, as well as the description of the character's surprised tone in the audio clues, we can infer that this sentence may carry a sense of surprise or unexpectedness. The female character may have seen or heard something unexpected, which triggered her surprise.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=246): <vis_desc>In the video, we see a man sitting on a sofa, facing another person who is not in the frame. The man's facial expression appears serious and focused, with his gaze fixed on the other person, indicating that he is engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone of the character is calm and the tone of the character's voice is also described as calm. Combined with the text content, it seems that he is asking about the other person's advice or for their opinion. The subtitle in the text says, "What's your suggestion? What do you think of this dish?" This sentence is likely the man asking the other person about their opinion or suggestion.</aud_desc>
<think>Based on the serious and focused facial expression of the man in the video, as well as his gaze fixed on the other person, it can be inferred that he is engaged in a serious conversation. Furthermore, the description of the character's calm tone in the audio also supports this inference. Therefore, this sentence is a polite way of asking about the other person's views or suggestions, rather than an aggressive or blaming expression.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/415.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07168.mp4
[DEBUG] Step 450
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=175): <vis_desc>In the video, we see a lady wearing a traditional Chinese dragon costume. Her mouth is wide open and her facial expression is fierce, as if she is angrily accusing or complaining to the other person.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle says, "She's also my person." This sentence may be a comment or accusation from the lady wearing the dragon costume towards the other person.</aud_desc>
<think>Based on the facial expression of the lady wearing the dragon costume in the video, which is fierce, and the audio clue describing the stern and aggressive tone, we can infer that this sentence carries a sense of anger or dissatisfaction. Therefore, this sentence may be expressing the lady's dissatisfaction and anger towards the other person.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see three characters standing in an indoor environment. The female on the left is wearing an animal head costume, seemingly playfully interacting with the other characters. We mainly analyze the emotional state of the female on the left. At the beginning of the video, her facial expression is quite playful and inquisitive, with a noticeable smile and direct eye contact with the other person. Her mouth moves slightly, indicating that she may be engaged in a friendly conversation. In the following scenes, her gaze starts to scan the other person, as if she is engaged in a more intense conversation. Taking these clues together, we can speculate that the female on the left may feel very happy, confident, and justified when jokingly interacting with the other characters.</vis_desc>
<aud_desc>In the audio, according to the text, the character's emotional state is quite excited and angry. The subtitle in the text says, "She is my person, don't call her petty." This sentence may be an expression of blame, dissatisfaction, or anger from the female on the left towards the other person.</aud_desc>
<think>Based on the playful and inquisitive facial expression of the female on the left in the video clues, as well as the direct eye contact and smiling actions, it can be inferred that she is emotionally excited and positive when expressing the phrase. Furthermore, based on the description of the character's emotional state in the audio clues as lively, excited, and angry, it can also be predicted that the female on the left feels very excited and angry when using this expression. Therefore, this sentence expresses the female on the left's dissatisfaction and anger towards the other person.</think>
<answer>angry</
[2025-11-07 08:42:32,564] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 451/2088 [6:33:00<23:45:52, 52.26s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.098959659539275, 'learning_rate': 7.840038314176245e-07, 'completion_length': 227.9375, 'rewards/av_format_reward': 0.53515625, 'rewards/accuracy_reward': 0.46875, 'rewards/think_step_av_with_neutral_reward': 0.41387519240379333, 'reward': 1.4177814722061157, 'reward_std': 0.4351525455713272, 'kl': 0.0296630859375, 'epoch': 0.43}
 22%|██▏       | 451/2088 [6:33:00<23:45:52, 52.26s/it][2025-11-07 08:43:21,402] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 452/2088 [6:33:48<23:16:58, 51.23s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9438644971617922, 'learning_rate': 7.835249042145593e-07, 'completion_length': 222.92578125, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.42761968076229095, 'reward': 1.6776195764541626, 'reward_std': 0.39813002943992615, 'kl': 0.028564453125, 'epoch': 0.43}
 22%|██▏       | 452/2088 [6:33:48<23:16:58, 51.23s/it][2025-11-07 08:44:11,259] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 453/2088 [6:34:38<23:04:52, 50.82s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.093337056012736, 'learning_rate': 7.830459770114942e-07, 'completion_length': 225.0390625, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.3828125, 'rewards/think_step_av_with_neutral_reward': 0.35448867082595825, 'reward': 1.5068324208259583, 'reward_std': 0.41863492131233215, 'kl': 0.029541015625, 'epoch': 0.43}
 22%|██▏       | 453/2088 [6:34:38<23:04:52, 50.82s/it][2025-11-07 08:45:02,447] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 454/2088 [6:35:30<23:07:03, 50.93s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9935569656188754, 'learning_rate': 7.825670498084291e-07, 'completion_length': 221.84765625, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.45802736282348633, 'reward': 1.7275585532188416, 'reward_std': 0.5057628154754639, 'kl': 0.02923583984375, 'epoch': 0.43}
 22%|██▏       | 454/2088 [6:35:30<23:07:03, 50.93s/it][2025-11-07 08:45:53,441] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 455/2088 [6:36:21<23:06:41, 50.95s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.1984773877279635, 'learning_rate': 7.82088122605364e-07, 'completion_length': 233.4296875, 'rewards/av_format_reward': 0.546875, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.41825902462005615, 'reward': 1.4612277150154114, 'reward_std': 0.6169456541538239, 'kl': 0.03271484375, 'epoch': 0.44}
 22%|██▏       | 455/2088 [6:36:21<23:06:41, 50.95s/it][2025-11-07 08:46:41,484] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 456/2088 [6:37:09<22:42:06, 50.08s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.154622908193443, 'learning_rate': 7.816091954022989e-07, 'completion_length': 225.90234375, 'rewards/av_format_reward': 0.58984375, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5644829869270325, 'reward': 1.7285454869270325, 'reward_std': 0.5200295746326447, 'kl': 0.0294189453125, 'epoch': 0.44}
 22%|██▏       | 456/2088 [6:37:09<22:42:06, 50.08s/it][2025-11-07 08:47:32,204] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 457/2088 [6:37:59<22:46:30, 50.27s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0123899518353445, 'learning_rate': 7.811302681992337e-07, 'completion_length': 224.2734375, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.379411444067955, 'reward': 1.5512863993644714, 'reward_std': 0.4479529857635498, 'kl': 0.02984619140625, 'epoch': 0.44}
 22%|██▏       | 457/2088 [6:37:59<22:46:30, 50.27s/it][2025-11-07 08:48:26,216] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 458/2088 [6:38:53<23:16:10, 51.39s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0015226630878926, 'learning_rate': 7.806513409961685e-07, 'completion_length': 225.734375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.66796875, 'rewards/think_step_av_with_neutral_reward': 0.6023003458976746, 'reward': 2.0515191555023193, 'reward_std': 0.5475400239229202, 'kl': 0.02923583984375, 'epoch': 0.44}
 22%|██▏       | 458/2088 [6:38:53<23:16:10, 51.39s/it][2025-11-07 08:49:14,982] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 459/2088 [6:39:42<22:53:55, 50.60s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.023356243023143, 'learning_rate': 7.801724137931034e-07, 'completion_length': 232.1796875, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.4427228569984436, 'reward': 1.6145978569984436, 'reward_std': 0.5616221129894257, 'kl': 0.02880859375, 'epoch': 0.44}
 22%|██▏       | 459/2088 [6:39:42<22:53:55, 50.60s/it][2025-11-07 08:50:07,668] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 460/2088 [6:40:35<23:10:00, 51.23s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.876181590911348, 'learning_rate': 7.796934865900383e-07, 'completion_length': 234.3203125, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.5462146252393723, 'reward': 2.0149646401405334, 'reward_std': 0.49881474673748016, 'kl': 0.027587890625, 'epoch': 0.44}
 22%|██▏       | 460/2088 [6:40:35<23:10:00, 51.23s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2863.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3540.mp4
[DEBUG] Step 460
[DEBUG] prompt_completion_ids shape: torch.Size([32, 279])
[DEBUG] Shortest output (len=168): <vis_desc>In the video, we see a character with an angry facial expression. His eyebrows are furrowed, and his mouth is open, showing a angry or scared expression.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle says, "I am your guest, why are you forcing you to leave?" This sentence may be the angry response from the character to the other person.</aud_desc>
<think>Based on the angry response from the character in the video clues and the description of a stern and aggressive tone in the audio clues, we can infer that this sentence carries a sense of dissatisfaction or anger. The character may be unhappy with the other person's refusal to leave and expresses his emotions of anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=279): <vis_desc>In the video, we see a man wearing a hat and a brown shirt, with a background that appears to be an outdoor night scene. In the video, his facial expression is smiling, his eyes are looking directly at the other person, and it seems that he is talking to the other person. His mouth moves slightly, indicating that he is engaged in a pleasant conversation.</vis_desc>
<aud_desc>In the audio, the character's voice is positive and the tone is relaxed and humorous. Combined with the text content, it seems to carry a sense of teasing. In the text, the subtitle says, "Friend, can I tell you something?" This sentence may be a way of reminding the other person to tell his friend something.</aud_desc>
<think>Based on the man's smiling facial expression, direct eye contact with the other person, and slightly moving mouth in the video clues, it can be inferred that he is sharing something positive and pleasant with his friend. Additionally, based on the positive description of the character's voice and relaxed and humorous tone in the audio clues, it can further determine that this sentence is spoken in a relaxed and pleasant mood. Therefore, this sentence may be an upbeat way of reminding the other person to tell his friend something, aligning with the overall positive emotions displayed by the man.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07479.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00586.mp4
[DEBUG] Step 460
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=176): <aud_desc>In the audio, the tone of the character is calm. Combined with the text content, it can be inferred that the character is commenting on a certain behavior or state of things. The subtitle in the text says, "But dancing itself guarantees you a chance to evolve,".</aud_desc>
<vis_desc>In the video, we see a female character looking up at the ceiling with a slight frown, as if she is trying to control her emotions or deal with some pressure.</vis_desc>
<think>Based on the audio clue describing the character's calm tone and the behavior of looking down at the ceiling, we can infer that the character is simply commenting on the behavior or state of things in a restrained manner. Therefore, this sentence may be a neutral observation or comment, rather than expressing a strong feeling.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=302): <vis_desc>In the video, the screen shows a male character in a dim environment, possibly indoors. In the video, his eyes are wide open and his facial expression appears somewhat nervous and serious. His gaze is fixed on a certain place, possibly searching for something or someone. The corners of his mouth are slightly open, as if speaking or reacting to someone or something.</vis_desc>
<aud_desc>In the audio, footsteps can be heard approaching. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Considering both the visual and auditory cues, the male character’s wide-open eyes, tense and serious facial expression, as well as his fixed gaze toward a specific place, strongly convey a sense of alertness and anxiety. His slightly open mouth suggests engagement with a Conversational Gender Fluid, responding carefully to an approaching opponent. The dim environment further reinforces a grounded, anxious intensity, as if he is trying to assess potential threats or locate a potential ally. Although the subtitle content does not provide emotional context, the combined visual signals—his fixed, attentive gaze and hesitant facial markers—collectively indicate a fearful but cautious atmosphere. The auditory footstep sounds add a subtle dynamic but do notaltively clarify the emotion from vocal tone. Taken together, these multimodal elements create a coherent picture of an individual caught in a potentially threatening moment within a controlled, measured setting, integrating anxious but balanced characteristics.</think>
<answer>neutral</answer>
[2025-11-07 08:50:56,298] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 461/2088 [6:41:23<22:48:00, 50.45s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9512404374723653, 'learning_rate': 7.792145593869731e-07, 'completion_length': 230.86328125, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5703215301036835, 'reward': 1.9609464406967163, 'reward_std': 0.4899378716945648, 'kl': 0.02978515625, 'epoch': 0.44}
 22%|██▏       | 461/2088 [6:41:23<22:48:00, 50.45s/it][2025-11-07 08:51:48,185] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 462/2088 [6:42:15<22:58:52, 50.88s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.978279700257398, 'learning_rate': 7.78735632183908e-07, 'completion_length': 235.38671875, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5332744121551514, 'reward': 1.9629619717597961, 'reward_std': 0.44243358075618744, 'kl': 0.0291748046875, 'epoch': 0.44}
 22%|██▏       | 462/2088 [6:42:15<22:58:52, 50.88s/it][2025-11-07 08:52:34,757] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 463/2088 [6:43:02<22:23:00, 49.59s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.007340175561366, 'learning_rate': 7.782567049808429e-07, 'completion_length': 228.93359375, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4982309341430664, 'reward': 1.8419809341430664, 'reward_std': 0.5875485092401505, 'kl': 0.02880859375, 'epoch': 0.44}
 22%|██▏       | 463/2088 [6:43:02<22:23:00, 49.59s/it][2025-11-07 08:53:27,967] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 464/2088 [6:43:55<22:51:35, 50.67s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0622843845612797, 'learning_rate': 7.777777777777778e-07, 'completion_length': 232.20703125, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.6953125, 'rewards/think_step_av_with_neutral_reward': 0.607746958732605, 'reward': 2.1233718395233154, 'reward_std': 0.4482146054506302, 'kl': 0.02740478515625, 'epoch': 0.44}
 22%|██▏       | 464/2088 [6:43:55<22:51:35, 50.67s/it][2025-11-07 08:54:19,674] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 465/2088 [6:44:47<22:59:07, 50.98s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0919733375767255, 'learning_rate': 7.772988505747126e-07, 'completion_length': 232.80859375, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4596337378025055, 'reward': 1.811196208000183, 'reward_std': 0.666299045085907, 'kl': 0.02911376953125, 'epoch': 0.45}
 22%|██▏       | 465/2088 [6:44:47<22:59:07, 50.98s/it][2025-11-07 08:55:11,025] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 466/2088 [6:45:38<23:01:14, 51.09s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9357360115030393, 'learning_rate': 7.768199233716474e-07, 'completion_length': 228.2578125, 'rewards/av_format_reward': 0.5625, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5811072587966919, 'reward': 1.795951008796692, 'reward_std': 0.46261024475097656, 'kl': 0.0279541015625, 'epoch': 0.45}
 22%|██▏       | 466/2088 [6:45:38<23:01:14, 51.09s/it][2025-11-07 08:56:01,810] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 467/2088 [6:46:29<22:57:53, 51.00s/it]                                                       {'loss': 0.001, 'grad_norm': 1.9874057214328709, 'learning_rate': 7.763409961685823e-07, 'completion_length': 236.5078125, 'rewards/av_format_reward': 0.5859375, 'rewards/accuracy_reward': 0.31640625, 'rewards/think_step_av_with_neutral_reward': 0.3422248363494873, 'reward': 1.2445685267448425, 'reward_std': 0.509284645318985, 'kl': 0.02587890625, 'epoch': 0.45}
 22%|██▏       | 467/2088 [6:46:29<22:57:53, 51.00s/it][2025-11-07 08:56:52,156] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 468/2088 [6:47:19<22:51:43, 50.80s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9985172084284792, 'learning_rate': 7.758620689655172e-07, 'completion_length': 231.79296875, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.558330237865448, 'reward': 1.874736487865448, 'reward_std': 0.5040159374475479, 'kl': 0.0264892578125, 'epoch': 0.45}
 22%|██▏       | 468/2088 [6:47:19<22:51:43, 50.80s/it][2025-11-07 08:57:43,863] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 22%|██▏       | 469/2088 [6:48:11<22:58:11, 51.08s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9383523502103885, 'learning_rate': 7.753831417624521e-07, 'completion_length': 232.55859375, 'rewards/av_format_reward': 0.890625, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5311081558465958, 'reward': 2.07017058134079, 'reward_std': 0.3871559649705887, 'kl': 0.02813720703125, 'epoch': 0.45}
 22%|██▏       | 469/2088 [6:48:11<22:58:11, 51.08s/it][2025-11-07 08:58:34,530] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 470/2088 [6:49:02<22:54:01, 50.95s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.007111192430553, 'learning_rate': 7.749042145593869e-07, 'completion_length': 228.71875, 'rewards/av_format_reward': 0.83203125, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5329212844371796, 'reward': 1.9626086950302124, 'reward_std': 0.5146798342466354, 'kl': 0.0267333984375, 'epoch': 0.45}
 23%|██▎       | 470/2088 [6:49:02<22:54:01, 50.95s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00370.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00045.mp4
[DEBUG] Step 470
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=179): <vis_desc>In the video, we see a woman standing next to a door. Her furrowed brow and tight lips indicate that she is feeling down. She seems to be staring outside the door, possibly seeing something that made her unhappy.</vis_desc>
<aud_desc>In the audio, footsteps and coins can be heard approaching. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Although visually characterized by a slight frown and tight lips, the woman’s distant gaze implies an internal monologue or unease that may be difficult to decipher apart from contextual clues. Audibly, the footsteps and coins add context but do not convey emotional intensity. Therefore, these environmental noises seem to carry a silence or lack of meaningful information that could strongly suggest a particular emotion other than sadness.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, we see a woman standing next to a closed door. Her facial expression is relatively calm, with a slight upward curve at the corners of her mouth, and her eyes are looking downwards, not directly at the camera, possibly indicating that she is contemplating something or feeling relaxed and contemplative.</vis_desc>
<aud_desc>In the audio, footsteps and the sound of paper being turned are noticeable. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman’s visual cues suggest a composed and measured demeanor: her calm facial expression with a subtle upward curve at the corners of her mouth combined with a gaze that seems contemplative indicates a relaxed state rather than strong emotion. The slight downward gaze also reflects a slow or deliberate mindset, which can lean toward a more subdued or composed emotional palette. The presence of consecutive sounds of footsteps within close proximity to the door suggests that she may have just noticed someone entering or has just left, which, coupled with her composed expression, might imply a moment of quiet reflection or a mundane, unperturbed moment in time rather than an intense emotional response. Additionally, the subtle presence of an overhead sound, likely from a nearby source like furniture or floor movements, adds a calm environmental layer but does not contradict the overall subdued feeling. The audio’s inability to clarify the emotional state further implies a neutral background noise, emphasizing the openness of her emotional processing to external influences or contexts rather than particular feelings tied to one word or vocal indicator. Together, these multimodal hints—her subtle demeanor, the quiet, mundane backdrop, and the neutral audio signals—cohesively support the interpretation that she is exhibiting a neutral emotional state, characterized by
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9057.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09075.mp4
[DEBUG] Step 470
[DEBUG] prompt_completion_ids shape: torch.Size([32, 307])
[DEBUG] Shortest output (len=139): <vis_desc>In the video, we see a shirtless man with a furrowed brow and wide-open eyes, seemingly seeing or hearing something unexpected, showing a very scared expression.</vis_desc>
<aud_desc>In the audio, the subtitle reads "Ah, it is true. I understand." This sentence may be the man's response to the others.</aud_desc>
<think>Based on the man's very scared expression in the video clues, as well as the response in the audio clue, we can infer that this sentence may carry a sense of surprise or realization. The man may realize or understand something, hence showing this sentence.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=307): <vis_desc>In the video, the frame includes a man on the left and a woman on the right. We mainly analyze the man's emotional state. In the video, he frowns, his mouth is wide open, and his gaze is directed towards the other person, which may indicate that he is engaged in a heated argument with the woman on the right. In the middle of the video, his facial expression becomes even more intense, accompanied by rapid mouth movements, suggesting a hostile and mocking emotion.</vis_desc>
<aud_desc>In the audio, the tone is stern, and combined with the text content, it can be felt that the character is accusing the other person. In the text, the subtitle says, "Don't think you can distract me, I'm as blind as an ox." This sentence may be a threat or warning from the male character to the woman on the right.</aud_desc>
<think>Based on the facial expressions of the male character in the video, such as frowning, wide open mouth, and directed attention towards the other person, it can be inferred that he is engaged in a heated and hostile conversation. Furthermore, based on the description of the stern tone and detected accusation emotions in the audio, it can be inferred that the male character's mindset and emotional state when saying this sentence are one of threatening or warning the other person. Therefore, this sentence expresses the male character's anger and accusation towards the woman on the right.</think>
<answer>angry</answer>
[2025-11-07 08:59:26,681] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 471/2088 [6:49:54<23:02:52, 51.31s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9932769641966037, 'learning_rate': 7.744252873563219e-07, 'completion_length': 232.96875, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4888092279434204, 'reward': 1.6919341683387756, 'reward_std': 0.5831342339515686, 'kl': 0.02655029296875, 'epoch': 0.45}
 23%|██▎       | 471/2088 [6:49:54<23:02:52, 51.31s/it][2025-11-07 09:00:14,625] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 472/2088 [6:50:42<22:34:48, 50.30s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0186382869368216, 'learning_rate': 7.739463601532567e-07, 'completion_length': 224.41015625, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.452917143702507, 'reward': 1.890417218208313, 'reward_std': 0.429859459400177, 'kl': 0.029052734375, 'epoch': 0.45}
 23%|██▎       | 472/2088 [6:50:42<22:34:48, 50.30s/it][2025-11-07 09:01:02,378] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 473/2088 [6:51:29<22:13:22, 49.54s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.965371709804258, 'learning_rate': 7.734674329501916e-07, 'completion_length': 231.1640625, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.7421875, 'rewards/think_step_av_with_neutral_reward': 0.6305141150951385, 'reward': 2.2086390256881714, 'reward_std': 0.5321228206157684, 'kl': 0.02777099609375, 'epoch': 0.45}
 23%|██▎       | 473/2088 [6:51:29<22:13:22, 49.54s/it][2025-11-07 09:01:55,399] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 474/2088 [6:52:22<22:40:39, 50.58s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.7884289843323773, 'learning_rate': 7.729885057471263e-07, 'completion_length': 226.52734375, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.37718436121940613, 'reward': 1.5881218910217285, 'reward_std': 0.40523749589920044, 'kl': 0.026611328125, 'epoch': 0.45}
 23%|██▎       | 474/2088 [6:52:22<22:40:39, 50.58s/it][2025-11-07 09:02:44,890] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 475/2088 [6:53:12<22:31:01, 50.26s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9836371344128607, 'learning_rate': 7.725095785440613e-07, 'completion_length': 241.36328125, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.34375, 'rewards/think_step_av_with_neutral_reward': 0.3204728811979294, 'reward': 1.3360978960990906, 'reward_std': 0.4720925837755203, 'kl': 0.02789306640625, 'epoch': 0.45}
 23%|██▎       | 475/2088 [6:53:12<22:31:01, 50.26s/it][2025-11-07 09:03:34,950] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 476/2088 [6:54:02<22:28:36, 50.20s/it]                                                       {'loss': 0.001, 'grad_norm': 2.022564561006838, 'learning_rate': 7.720306513409961e-07, 'completion_length': 232.5234375, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.69140625, 'rewards/think_step_av_with_neutral_reward': 0.5600535571575165, 'reward': 1.9389597177505493, 'reward_std': 0.47082000970840454, 'kl': 0.0252685546875, 'epoch': 0.46}
 23%|██▎       | 476/2088 [6:54:02<22:28:36, 50.20s/it][2025-11-07 09:04:26,855] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 477/2088 [6:54:54<22:41:32, 50.71s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0202253162231347, 'learning_rate': 7.71551724137931e-07, 'completion_length': 229.7109375, 'rewards/av_format_reward': 0.6484375, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.555572897195816, 'reward': 1.7782291173934937, 'reward_std': 0.5625212788581848, 'kl': 0.02850341796875, 'epoch': 0.46}
 23%|██▎       | 477/2088 [6:54:54<22:41:32, 50.71s/it][2025-11-07 09:05:19,698] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 478/2088 [6:55:47<22:57:52, 51.35s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9293174034015244, 'learning_rate': 7.710727969348658e-07, 'completion_length': 223.12109375, 'rewards/av_format_reward': 0.89453125, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.432717964053154, 'reward': 1.8233429789543152, 'reward_std': 0.3580634891986847, 'kl': 0.0283203125, 'epoch': 0.46}
 23%|██▎       | 478/2088 [6:55:47<22:57:52, 51.35s/it][2025-11-07 09:06:09,216] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 479/2088 [6:56:36<22:42:16, 50.80s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.038294641109156, 'learning_rate': 7.705938697318008e-07, 'completion_length': 226.4609375, 'rewards/av_format_reward': 0.875, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.4884461313486099, 'reward': 1.9181334972381592, 'reward_std': 0.5471046566963196, 'kl': 0.02923583984375, 'epoch': 0.46}
 23%|██▎       | 479/2088 [6:56:36<22:42:16, 50.80s/it][2025-11-07 09:06:58,924] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 480/2088 [6:57:26<22:32:39, 50.47s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.049893544015946, 'learning_rate': 7.701149425287356e-07, 'completion_length': 222.88671875, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.68359375, 'rewards/think_step_av_with_neutral_reward': 0.5296976864337921, 'reward': 1.9828226566314697, 'reward_std': 0.5087749063968658, 'kl': 0.03082275390625, 'epoch': 0.46}
 23%|██▎       | 480/2088 [6:57:26<22:32:39, 50.47s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09620.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07950.mp4
[DEBUG] Step 480
[DEBUG] prompt_completion_ids shape: torch.Size([32, 304])
[DEBUG] Shortest output (len=163): <aud_desc>In the audio, the character's voice sounds surprised when saying "How could you even ask me about this?" This sentence may be the female character's reaction to the information she asked.</aud_desc>
<vis_desc>In the video, we see a female character with furrowed brows and slightly downturned mouth, seemingly expressing a negative emotion towards the content she is discussing.</vis_desc>
<think>Based on the visual clue of the female character's furrowed brows and downturned mouth, as well as the audio clue of the character's surprised tone when saying the sentence, we can infer that this sentence may carry a sense of surprise or unexpectedness. The female character may feel surprised by the information she asked and finds it unexpected.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=304): <vis_desc>In the video, we see a character wearing traditional Chinese clothing, including a hat and a band-aid on his head, possibly indicating that he is injured. In the video, his facial expression is surprised and serious. His eyes are focused on a certain place, possibly indicating that something or someone in the environment has caused his surprise. He shakes his head up and down, which usually means thinking positively or encouraging himself to think positively.</vis_desc>
<aud_desc>In the audio, the character's tone is aggressive and the phrase "What? I'm going to start begging now!" expresses the character's shock and disbelief. In the text, the subtitle says, "Ah, let's do it! Agreed!" This sentence may be the character's response to the other person's suggestion or request.</aud_desc>
<think>Based on the surprised and serious facial expression of the character in the video, as well as his head shaking, he may have heard some unexpected news or been shocked by a certain event. Based on the aggression of the character's tone in the audio, we can infer that this sentence is spoken by the character during an angry or excited tone. Therefore, this sentence may be the character's response to the other person's suggestion or request, implying that the other person is begging and the character agrees to it, possibly because the other person are sick of being begging or because he doesn't have the strength to continue begging.</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12285.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11525.mp4
[DEBUG] Step 480
[DEBUG] prompt_completion_ids shape: torch.Size([32, 304])
[DEBUG] Shortest output (len=176): <vis_desc>In the video, we see a female character with a furrowed brow and slightly downturned mouth, tears flowing from her eyes. Her facial expression appears very sad and painful.</vis_desc>
<aud_desc>In the audio, the character's voice is accompanied by obvious sobbing. In the text, the subtitle reads: "Why are you treating me like this? Why is she treating you like this?" This sentence may express the female character's anger and resentment.</aud_desc>
<think>Based on the sad and pained facial expression of the female character in the video, as well as the obvious sobbing in the audio, we can infer that this sentence carries a sense of anger and resentment. The female character may feel dissatisfied and angry with the other person for treating her in a unfair manner.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=304): <vis_desc>In the video, the screen shows a man standing in a dim environment, possibly indoors. At the beginning of the video, his eyes are wide open, eyebrows raised, and his facial expression appears happy, as if he is seeing or hearing something that made him happy. In the following scenes, his gaze lowers, his smile decreases, and his expression becomes relaxed and friendly, seemingly reminiscing or thinking about something. Overall, his emotions are quite positive.</vis_desc>
<aud_desc>In the audio, the character's tone is calm and the voice feels soothing. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The man’s visual cues clearly indicate a positive emotional state. His wide-open eyes, raised eyebrows, and relaxed, happy facial expression with gentle eyes suggest surprise and camaraderie, not typically associated with sadness. The lowering of his smile reflects a natural transition from pleasant memory recall back to a more relaxed, cheerful demeanor, reinforcing the idea that his overall mood is positive. Complementing this, the calm and soothing tone of his voice conveys warmth and a genuine sense of happiness, reinforcing the idea that despite the absence of sadness, his inner joy and contentment are enduring. Together, these visual and auditory signals create a compelling picture of a manrowned individual experiencing a moment of pleasant memory-making, undimented and uplifted by the familiar comforting companionship of happiness.</think>
<answer>happy</answer>
[2025-11-07 09:07:49,334] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 481/2088 [6:58:16<22:31:19, 50.45s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.004141303221139, 'learning_rate': 7.696360153256705e-07, 'completion_length': 228.09765625, 'rewards/av_format_reward': 0.86328125, 'rewards/accuracy_reward': 0.71875, 'rewards/think_step_av_with_neutral_reward': 0.5776491165161133, 'reward': 2.1596803665161133, 'reward_std': 0.3891359120607376, 'kl': 0.0289306640625, 'epoch': 0.46}
 23%|██▎       | 481/2088 [6:58:16<22:31:19, 50.45s/it][2025-11-07 09:08:42,995] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 482/2088 [6:59:10<22:56:13, 51.42s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0850419772323487, 'learning_rate': 7.691570881226053e-07, 'completion_length': 227.1875, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5087896883487701, 'reward': 1.9892584085464478, 'reward_std': 0.4518212378025055, 'kl': 0.027587890625, 'epoch': 0.46}
 23%|██▎       | 482/2088 [6:59:10<22:56:13, 51.42s/it][2025-11-07 09:09:30,278] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 483/2088 [6:59:57<22:22:12, 50.18s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0527404414238557, 'learning_rate': 7.686781609195402e-07, 'completion_length': 227.8828125, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.5206335186958313, 'reward': 1.797977328300476, 'reward_std': 0.441785529255867, 'kl': 0.02691650390625, 'epoch': 0.46}
 23%|██▎       | 483/2088 [6:59:57<22:22:12, 50.18s/it][2025-11-07 09:10:19,893] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 484/2088 [7:00:47<22:16:51, 50.01s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.063544284664854, 'learning_rate': 7.681992337164751e-07, 'completion_length': 225.75, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.5352147668600082, 'reward': 1.8867772817611694, 'reward_std': 0.537170797586441, 'kl': 0.0299072265625, 'epoch': 0.46}
 23%|██▎       | 484/2088 [7:00:47<22:16:51, 50.01s/it][2025-11-07 09:11:14,048] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 485/2088 [7:01:41<22:49:17, 51.25s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9579674980577872, 'learning_rate': 7.677203065134099e-07, 'completion_length': 232.26171875, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.40625, 'rewards/think_step_av_with_neutral_reward': 0.39994996786117554, 'reward': 1.5835437774658203, 'reward_std': 0.47998443245887756, 'kl': 0.02728271484375, 'epoch': 0.46}
 23%|██▎       | 485/2088 [7:01:41<22:49:17, 51.25s/it][2025-11-07 09:12:07,554] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 486/2088 [7:02:35<23:06:28, 51.93s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9589818983783145, 'learning_rate': 7.672413793103448e-07, 'completion_length': 231.07421875, 'rewards/av_format_reward': 0.91015625, 'rewards/accuracy_reward': 0.69921875, 'rewards/think_step_av_with_neutral_reward': 0.6127621084451675, 'reward': 2.222137153148651, 'reward_std': 0.4793683886528015, 'kl': 0.02630615234375, 'epoch': 0.47}
 23%|██▎       | 486/2088 [7:02:35<23:06:28, 51.93s/it][2025-11-07 09:13:05,403] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 487/2088 [7:03:32<23:53:00, 53.70s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9782907816051907, 'learning_rate': 7.667624521072797e-07, 'completion_length': 235.1796875, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.5450871586799622, 'reward': 1.8185245990753174, 'reward_std': 0.5352163910865784, 'kl': 0.02685546875, 'epoch': 0.47}
 23%|██▎       | 487/2088 [7:03:32<23:53:00, 53.70s/it][2025-11-07 09:13:56,192] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 488/2088 [7:04:23<23:28:47, 52.83s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.090228778908858, 'learning_rate': 7.662835249042146e-07, 'completion_length': 244.8203125, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.4961162805557251, 'reward': 1.667991280555725, 'reward_std': 0.5511558055877686, 'kl': 0.029052734375, 'epoch': 0.47}
 23%|██▎       | 488/2088 [7:04:23<23:28:47, 52.83s/it][2025-11-07 09:14:43,785] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 489/2088 [7:05:11<22:46:03, 51.26s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0804618716427297, 'learning_rate': 7.658045977011494e-07, 'completion_length': 232.37890625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5027684569358826, 'reward': 1.8543310165405273, 'reward_std': 0.5780441462993622, 'kl': 0.0291748046875, 'epoch': 0.47}
 23%|██▎       | 489/2088 [7:05:11<22:46:03, 51.26s/it][2025-11-07 09:15:36,101] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 23%|██▎       | 490/2088 [7:06:03<22:53:38, 51.58s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.083820877462308, 'learning_rate': 7.653256704980842e-07, 'completion_length': 231.51171875, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.55078125, 'rewards/think_step_av_with_neutral_reward': 0.40528208017349243, 'reward': 1.8154383897781372, 'reward_std': 0.458344042301178, 'kl': 0.0303955078125, 'epoch': 0.47}
 23%|██▎       | 490/2088 [7:06:03<22:53:38, 51.58s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7103.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/861.mp4
[DEBUG] Step 490
[DEBUG] prompt_completion_ids shape: torch.Size([32, 348])
[DEBUG] Shortest output (len=191): <vis_desc>In the video, we see a young woman standing indoors with a dim background. Her eyes are wide open and she seems to be looking upwards, possibly seeing something or someone outside the frame. Her facial expression appears surprised.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The young woman's wide-open eyes and upward gaze suggest a sudden shift in attention or unexpected stimulus, commonly associated with surprise. Although the audio and subtitle content do not provide additional emotional cues, these visual signals provide the strongest indication of her feeling. The combination of her startled expression and fixed gaze on something external explains why surprise would be the most appropriate emotional reaction in this moment, as she seems caught off guard by what is happening outside the frame.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=348): <vis_desc>In the video, the screen shows a woman sitting on a boat, wearing traditional Chinese clothing, and appears to be in a state of fear. In the video, her facial expression is very distorted, with furrowed brows, tightly closed lips, and large-open eyes, giving the impression of being caught by a hostile crowd. Her body posture is also rigid, with one hand touching her hair and the other arm resting by her waist, which may indicate that she is trying to move away from the danger but is confronted by the opposite force.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very scared, with a sense of being chased and scared. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly suggest that the woman is experiencing fear. Visually, her distorted facial expression, tightly closed lips, and rigid body posture clearly convey a heightened state of anxiety and caution, especially noticeable as she tries to move away from a danger pose. The environment, a boat coupled with traditional Chinese clothing, adds a layer of tension and formality, reinforcing a sense of vulnerability linked to fear. Audibly, the voice trembling and sounding scared conveys an immediate and visceral reaction consistent with fear, even in the absence of clear vocal cues in speech. Even though the subtitle text does not explicitly reveal her emotions, the multimodal signals—visual appearance combined with the frightened pitch-black tone of the voice—jointly provide a convincing and coherent picture of a fearful emotional state. Together, these multimodal experiences support the conclusion that the target emotional expression is indeed fear.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8738.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07160.mp4
[DEBUG] Step 490
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=185): <vis_desc>In the video, we see a woman wearing a doctor's uniform. Her facial expression appears neutral, her mouth slightly open, and her gaze focused on the computer screen in front of her.</vis_desc>
<aud_desc>In the audio, the tone is calm and the intonation is steady. In the text, the subtitle says, "Actually, he is on stitches." This sentence may be the woman criticizing or blaming the other person.</aud_desc>
<think>Based on the neutral facial expression and focused gaze of the woman in the video, as well as the calm tone and steady intonation of the woman in the audio, we can infer that this sentence may have a serious or neutral tone. Therefore, this sentence may be the woman criticizing the other person's behavior or expressing an objective view, rather than a strong emotional expression like anger or sadness.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=302): <vis_desc>In the video, the screen shows a woman. In the video, she slightly furrows her brows, lifts the corners of her mouth, and her eyes show a sad expression. Her gaze seems somewhat vacant, as if she is experiencing some kind of dilemma or sadness. Her body posture seems heavy and sorrowful, possibly because she is sad and worried about something.</vis_desc>
<aud_desc>In the audio, the character's voice is accompanied by the sound of falling soapops, indicating that the character's emotions are somewhat sad and melancholic. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the woman is experiencing sadness. Visually, her slight furrowing of the brows, lifted mouth, and evident sad expression clearly convey an emotional labor of grief or despair. The vacant gaze and heavy, sorrowful body language further reinforce an intense emotional burden. Auditory signals reinforce this interpretation: the character's voice being accompanied by the sound of falling soapops adds a poignant, somber layer that corresponds well with the visual appearance of sadness. Although the subtitle content does not clarify her emotions, the harmonious blend of her facial expressions and the way falling soapbons evoke melancholic tones convincingly point to sadness as her predominant emotion. Together, these multimodal signals create a coherent and convincing portrayal of someone immersed in a sad pandemic scenario.</think>
<answer>sad</answer>
[2025-11-07 09:16:22,756] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 491/2088 [7:06:50<22:13:29, 50.10s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.024498597681796, 'learning_rate': 7.64846743295019e-07, 'completion_length': 226.9921875, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5316176563501358, 'reward': 1.7503677606582642, 'reward_std': 0.5366370677947998, 'kl': 0.0289306640625, 'epoch': 0.47}
 24%|██▎       | 491/2088 [7:06:50<22:13:29, 50.10s/it][2025-11-07 09:17:13,476] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 492/2088 [7:07:41<22:17:36, 50.29s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.989944599749707, 'learning_rate': 7.64367816091954e-07, 'completion_length': 233.25390625, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.4421904683113098, 'reward': 1.7664092779159546, 'reward_std': 0.5214505791664124, 'kl': 0.0289306640625, 'epoch': 0.47}
 24%|██▎       | 492/2088 [7:07:41<22:17:36, 50.29s/it][2025-11-07 09:18:05,354] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 493/2088 [7:08:32<22:29:27, 50.76s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9282153096686858, 'learning_rate': 7.638888888888888e-07, 'completion_length': 237.00390625, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5610398352146149, 'reward': 1.8930709958076477, 'reward_std': 0.5091403126716614, 'kl': 0.0269775390625, 'epoch': 0.47}
 24%|██▎       | 493/2088 [7:08:32<22:29:27, 50.76s/it][2025-11-07 09:18:57,136] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 494/2088 [7:09:24<22:36:43, 51.07s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9729136077363416, 'learning_rate': 7.634099616858237e-07, 'completion_length': 224.81640625, 'rewards/av_format_reward': 0.7265625, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.3509867191314697, 'reward': 1.5267679691314697, 'reward_std': 0.4419323801994324, 'kl': 0.028564453125, 'epoch': 0.47}
 24%|██▎       | 494/2088 [7:09:24<22:36:43, 51.07s/it][2025-11-07 09:19:48,804] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▎       | 495/2088 [7:10:16<22:40:39, 51.25s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.146334501172357, 'learning_rate': 7.629310344827587e-07, 'completion_length': 241.3359375, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.4178878366947174, 'reward': 1.6913254261016846, 'reward_std': 0.5469207167625427, 'kl': 0.030029296875, 'epoch': 0.47}
 24%|██▎       | 495/2088 [7:10:16<22:40:39, 51.25s/it][2025-11-07 09:20:39,110] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 496/2088 [7:11:06<22:32:18, 50.97s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0382992643249893, 'learning_rate': 7.624521072796935e-07, 'completion_length': 232.19921875, 'rewards/av_format_reward': 0.578125, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5541383922100067, 'reward': 1.7885133028030396, 'reward_std': 0.4542243927717209, 'kl': 0.029541015625, 'epoch': 0.48}
 24%|██▍       | 496/2088 [7:11:06<22:32:18, 50.97s/it][2025-11-07 09:21:29,833] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 497/2088 [7:11:57<22:29:31, 50.89s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9283217778595838, 'learning_rate': 7.619731800766284e-07, 'completion_length': 233.5, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.43832577764987946, 'reward': 1.8133257627487183, 'reward_std': 0.49943065643310547, 'kl': 0.02874755859375, 'epoch': 0.48}
 24%|██▍       | 497/2088 [7:11:57<22:29:31, 50.89s/it][2025-11-07 09:22:19,113] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 498/2088 [7:12:46<22:15:50, 50.41s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.032090071371574, 'learning_rate': 7.614942528735631e-07, 'completion_length': 230.05859375, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4621506631374359, 'reward': 1.6652756929397583, 'reward_std': 0.504942461848259, 'kl': 0.0291748046875, 'epoch': 0.48}
 24%|██▍       | 498/2088 [7:12:46<22:15:50, 50.41s/it][2025-11-07 09:23:07,608] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 499/2088 [7:13:35<21:59:47, 49.83s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.956726879236311, 'learning_rate': 7.61015325670498e-07, 'completion_length': 235.4140625, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.703125, 'rewards/think_step_av_with_neutral_reward': 0.6045559048652649, 'reward': 1.9326808452606201, 'reward_std': 0.5935628116130829, 'kl': 0.02984619140625, 'epoch': 0.48}
 24%|██▍       | 499/2088 [7:13:35<21:59:47, 49.83s/it][2025-11-07 09:23:59,017] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 500/2088 [7:14:26<22:11:28, 50.31s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0400882839074668, 'learning_rate': 7.605363984674329e-07, 'completion_length': 231.8671875, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.47889623045921326, 'reward': 1.8851462602615356, 'reward_std': 0.5450828373432159, 'kl': 0.03033447265625, 'epoch': 0.48}
 24%|██▍       | 500/2088 [7:14:26<22:11:28, 50.31s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/05647.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08826.mp4
[DEBUG] Step 500
[DEBUG] prompt_completion_ids shape: torch.Size([32, 316])
[DEBUG] Shortest output (len=133): <vis_desc>In the video, the screen shows a woman using a cellphone to make a call indoors. Her brows are furrowed and her eyes are wide open, showing a very serious facial expression. She appears to be concentrating on the conversation or waiting for a response.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>Due to the lack of emotional clues or background information, a more general emotional analysis is unable to be provided.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=316): <vis_desc>In the video, the screen shows a woman using a cellphone in what appears to be an indoor setting. In the video, her eyebrows are furrowed, and her facial expression looks quite sad. Her eyes are looking downwards, not directly at the camera, giving a sense of confusion. Her mouth moves slightly, but her gaze does not meet the other person, indicating that she is speaking to someone off-screen. Overall, she is engaged in a conversation that makes her feel displeased.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>The woman’s visual cues—frown, downturned mouth, and sad expression—strongly suggest an emotional state of distress or confusion, as if she is going through an unpleasant situation. Her downward gaze and lack of direct eye contact further indicate that she is processing information or trying to make sense of something sad or ambiguous situation. Although the audible information, including vocal tone and subtitle content, does not provide clear emotional signals, the consistent visual expression and subtle nonverbal cues together strongly imply sadness. Complementarily, the vocal tone and subtitle content offer no additional cues to contradict or enhance this interpretation. Altogether, the multimodal evidence, particularly the visual modality, reliably supports the conclusion that the woman is experiencing sadness, demonstrating how multimodal information can be integrated and used together to infer emotion.</think>
<answer>sad</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9490.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5989.mp4
[DEBUG] Step 500
[DEBUG] prompt_completion_ids shape: torch.Size([32, 331])
[DEBUG] Shortest output (len=177): <vis_desc>In the video, we see a woman standing indoors. Her eyes are looking directly at the other person, and her mouth is moving, indicating that she is engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, the tone is neutral. In the text, the subtitle says, "Nina, is everything okay?" This sentence could be the woman's inquiry or reaction to someone named Nina.</aud_desc>
<think>Based on the visual clue of the woman's eyes directly focusing on the other person and her mouth moving, as well as the neutral tone described in the audio clue, we can infer that this sentence may be a serious inquiry or reaction from the woman to someone named Nina. Therefore, this sentence expresses the woman's attentiveness, concern, or direct communication with the other person.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=331): <vis_desc>In the video, we see a woman sitting in the car. Her facial expression appears relaxed and comfortable, with a slight upward curve at the corners of her mouth. Her eyes are looking directly at the road ahead, not directly at the other car or cloudy lights, suggesting that she is focused on observing or listening to something outside of the frame. Overall, she appears to be experiencing a pleasant journey.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, the subtitle says, "Just like you, like me." This sentence may be a response or comment from the woman on whether she is similar to the person she is talking to.</aud_desc>
<think>Based on the woman's relaxed and comfortable facial expression in the video clues, as well as her direct eye contact and focus on observing or listening, it can be inferred that she is expressing a sense of curiosity or interest in the other person. Additionally, the woman's mention of "just like you, like me" in the subtitle emphasizes her identity and emotional connection with the other person, which aligns with her displayed demeanor of relaxation and comfort. Since the audio clues cannot determine the emotional state of the person, we primarily rely on the video clues of the woman's relaxed and focused expression, direct eye contact, and attention on the road, to speculate on her emotional state. The mention of "just like you, like me" further supports this inference as the woman may feel happy or proud about her similar characteristics to the person she is talking to.</think>
<answer>neutral</answer>
[2025-11-07 09:25:02,861] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 501/2088 [7:15:30<23:58:02, 54.37s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.895715617596089, 'learning_rate': 7.600574712643678e-07, 'completion_length': 233.890625, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.41029079258441925, 'reward': 1.675915777683258, 'reward_std': 0.42108154296875, 'kl': 0.029052734375, 'epoch': 0.48}
 24%|██▍       | 501/2088 [7:15:30<23:58:02, 54.37s/it][2025-11-07 09:25:53,324] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 502/2088 [7:16:20<23:26:09, 53.20s/it]                                                       {'loss': 0.0018, 'grad_norm': 10.421796113772857, 'learning_rate': 7.595785440613026e-07, 'completion_length': 231.140625, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.4457375109195709, 'reward': 1.7621437311172485, 'reward_std': 0.4459279775619507, 'kl': 0.04595947265625, 'epoch': 0.48}
 24%|██▍       | 502/2088 [7:16:20<23:26:09, 53.20s/it][2025-11-07 09:26:41,968] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 503/2088 [7:17:09<22:49:11, 51.83s/it]                                                       {'loss': 0.0011, 'grad_norm': 2.0025768701031645, 'learning_rate': 7.590996168582375e-07, 'completion_length': 231.39453125, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.4920210838317871, 'reward': 1.8279585242271423, 'reward_std': 0.4857720285654068, 'kl': 0.028564453125, 'epoch': 0.48}
 24%|██▍       | 503/2088 [7:17:09<22:49:11, 51.83s/it][2025-11-07 09:27:34,125] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 504/2088 [7:18:01<22:50:54, 51.93s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.80010262574846, 'learning_rate': 7.586206896551724e-07, 'completion_length': 236.55859375, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.41845814883708954, 'reward': 1.6879894733428955, 'reward_std': 0.3645464926958084, 'kl': 0.03125, 'epoch': 0.48}
 24%|██▍       | 504/2088 [7:18:01<22:50:54, 51.93s/it][2025-11-07 09:28:25,072] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 505/2088 [7:18:52<22:42:17, 51.63s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0075258686494326, 'learning_rate': 7.581417624521073e-07, 'completion_length': 231.09375, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.4453125, 'rewards/think_step_av_with_neutral_reward': 0.3827672749757767, 'reward': 1.4804234504699707, 'reward_std': 0.5132881700992584, 'kl': 0.0294189453125, 'epoch': 0.48}
 24%|██▍       | 505/2088 [7:18:52<22:42:17, 51.63s/it][2025-11-07 09:29:21,182] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 506/2088 [7:19:48<23:16:49, 52.98s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9927313773790962, 'learning_rate': 7.57662835249042e-07, 'completion_length': 234.43359375, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.4453125, 'rewards/think_step_av_with_neutral_reward': 0.40947210788726807, 'reward': 1.655565857887268, 'reward_std': 0.47561347484588623, 'kl': 0.0294189453125, 'epoch': 0.48}
 24%|██▍       | 506/2088 [7:19:48<23:16:49, 52.98s/it][2025-11-07 09:30:20,440] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 507/2088 [7:20:48<24:05:35, 54.86s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.045423538147776, 'learning_rate': 7.571839080459769e-07, 'completion_length': 233.15234375, 'rewards/av_format_reward': 0.6015625, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.5146212428808212, 'reward': 1.6552462577819824, 'reward_std': 0.5070913136005402, 'kl': 0.03179931640625, 'epoch': 0.49}
 24%|██▍       | 507/2088 [7:20:48<24:05:35, 54.86s/it][2025-11-07 09:31:13,756] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 508/2088 [7:21:41<23:52:28, 54.40s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9922100535898977, 'learning_rate': 7.567049808429118e-07, 'completion_length': 229.78125, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.40779657661914825, 'reward': 1.567952811717987, 'reward_std': 0.4605114310979843, 'kl': 0.0311279296875, 'epoch': 0.49}
 24%|██▍       | 508/2088 [7:21:41<23:52:28, 54.40s/it][2025-11-07 09:32:07,447] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 509/2088 [7:22:35<23:45:58, 54.19s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9697462795566631, 'learning_rate': 7.562260536398467e-07, 'completion_length': 245.53125, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.45703125, 'rewards/think_step_av_with_neutral_reward': 0.41147322952747345, 'reward': 1.540379524230957, 'reward_std': 0.5721917003393173, 'kl': 0.02923583984375, 'epoch': 0.49}
 24%|██▍       | 509/2088 [7:22:35<23:45:58, 54.19s/it][2025-11-07 09:32:56,598] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 510/2088 [7:23:24<23:05:21, 52.68s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9140405824097624, 'learning_rate': 7.557471264367816e-07, 'completion_length': 228.72265625, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.4380957931280136, 'reward': 1.7193458080291748, 'reward_std': 0.4177763909101486, 'kl': 0.029296875, 'epoch': 0.49}
 24%|██▍       | 510/2088 [7:23:24<23:05:21, 52.68s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2691.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11526.mp4
[DEBUG] Step 510
[DEBUG] prompt_completion_ids shape: torch.Size([32, 344])
[DEBUG] Shortest output (len=155): <aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads: "I have some ten-dollar bills." This sentence may be the man's statement about having money.</aud_desc>
<vis_desc>In the video, he slightly furrows his brow, with a smile on his face, appearing to be engaged in a relatively positive conversation.</vis_desc>
<think>Based on the video clues of the man slightly furrowing his brow and the smile on his face, as well as the audio clue of the neutral tone, we can infer that this sentence may be a neutral statement about the man's financial situation. The man may be informing others about their financial condition without expressing any particular emotions.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=344): <aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, we see a young female character standing outdoors at night. Her eyes are initially wide open and she seems to be looking straight ahead, possibly seeing or hearing something interesting. As time passes, she slowly turns her head to the left side, her eyes following her gaze. During the turn, her focus shifts, and she looks downwards, seeming indifferent or less engaged with the environment.</vis_desc>
<think>The young female character's visual cues—initially focused forward with a neutral expression and a downward gaze—suggest engagement without strong emotional inclination. Her slow turn and shifting focus imply a transition away from initial interest to something more distant or uninterested, reflecting a subtle but not dramatic shift. The absence of overt facial expressions or vocal cues in the absence of a subtitle content means any discernible emotional signals rely heavily on contextual shifts. However, the auditory information combined with the visual signals provides a crucial insight into her emotional state. Since the audio does not communicate emotional content, weturn to the subtitle text for this analysis. The subtitle content reveals that she is “Reading a book,” which is a relatively calm and contemplative action rather than a shift from one topic to another. The even and steady tone indicates focused attention and concentration, consistent with a neutral emotional state rather than one charged with feelings such as sadness or frustration. Together, these multimodal signals support the conclusion that her emotional state is neutral, reflecting thoughtful engagement without apparent emotional intensities.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9729.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08690.mp4
[DEBUG] Step 510
[DEBUG] prompt_completion_ids shape: torch.Size([32, 321])
[DEBUG] Shortest output (len=177): <aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle reads, "I'm the Director of Education." This sentence is likely spoken by the male character during the emotional outburst.</aud_desc>
<vis_desc>In the video, the scene shows a male with a furrowed brow and an aggressive facial expression, seemingly speaking or singing loudly, possibly engaged in a heated argument with someone.</vis_desc>
<think>Based on the aggressive facial expression and furrowed brow of the male character in the video, as well as the description of a stern and aggressive tone in the audio, we can infer that this sentence carries a sense of anger or hatred. The male character may be arguing with someone and expressing his dominance over them through words, hence having a stern and aggressive tone.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=321): <vis_desc>In the video, the screen shows a male character wearing traditional Chinese clothing, with his back to the camera. He appears to be speaking, as indicated by the movement of his mouth and the deviation in the position of his head. In the video, his facial expression is stern and aggressive, with a furrowed brow and an open mouth, as if expressing his opinion or emotions. His eyes are fixed on the other person, giving the impression that he is talking to someone of high stature or importance. Overall, he is engaged in a discussion or debate that may be very serious or heated.</vis_desc>
<aud_desc>In the audio, the tone is bold and aggressive, with a stern and aggressive tone. Combined with the text content, the character expresses his dissatisfaction and anger. The subtitle in the text says, "I'm the place, the boss, and I'm the ruler of the world." This sentence indicates the attitude and emotions of the male character.</aud_desc>
<think>Based on the video clues of the male character's stern and aggressive facial expression, the open position of his mouth, and the fixed gaze on the other person, it can be inferred that he is engaged in a serious and aggressive discussion or debate. Additionally, the audio clue describing the bold and aggressive tone confirms the serious and intense emotional state of the male character. Therefore, this sentence expresses the mindset and attitude of the male character blaming others and asserting his own power and authority, which aligns with the overall display of seriousness and aggression.</think>
<answer>angry</answer>
[2025-11-07 09:33:54,526] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 24%|██▍       | 511/2088 [7:24:22<23:45:54, 54.25s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0342158933625507, 'learning_rate': 7.552681992337164e-07, 'completion_length': 230.4765625, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.671875, 'rewards/think_step_av_with_neutral_reward': 0.5875431299209595, 'reward': 1.9625431895256042, 'reward_std': 0.5342012047767639, 'kl': 0.0296630859375, 'epoch': 0.49}
 24%|██▍       | 511/2088 [7:24:22<23:45:54, 54.25s/it][2025-11-07 09:34:47,225] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 512/2088 [7:25:14<23:32:46, 53.79s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0158336844247575, 'learning_rate': 7.547892720306514e-07, 'completion_length': 238.84375, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.5078125, 'rewards/think_step_av_with_neutral_reward': 0.4712248146533966, 'reward': 1.6821622848510742, 'reward_std': 0.593865692615509, 'kl': 0.0306396484375, 'epoch': 0.49}
 25%|██▍       | 512/2088 [7:25:14<23:32:46, 53.79s/it][2025-11-07 09:35:39,201] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 513/2088 [7:26:06<23:17:36, 53.24s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9870110155095644, 'learning_rate': 7.543103448275862e-07, 'completion_length': 230.234375, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.512168824672699, 'reward': 1.9262312650680542, 'reward_std': 0.4047900438308716, 'kl': 0.0302734375, 'epoch': 0.49}
 25%|██▍       | 513/2088 [7:26:06<23:17:36, 53.24s/it][2025-11-07 09:36:27,109] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 514/2088 [7:26:54<22:34:44, 51.64s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.077970770681915, 'learning_rate': 7.538314176245211e-07, 'completion_length': 226.98046875, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5746233463287354, 'reward': 1.9535294771194458, 'reward_std': 0.497637003660202, 'kl': 0.03045654296875, 'epoch': 0.49}
 25%|██▍       | 514/2088 [7:26:54<22:34:44, 51.64s/it][2025-11-07 09:37:19,583] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 515/2088 [7:27:47<22:40:26, 51.89s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0889007903657495, 'learning_rate': 7.533524904214558e-07, 'completion_length': 226.36328125, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.39453125, 'rewards/think_step_av_with_neutral_reward': 0.4637020528316498, 'reward': 1.5965145826339722, 'reward_std': 0.49597932398319244, 'kl': 0.0294189453125, 'epoch': 0.49}
 25%|██▍       | 515/2088 [7:27:47<22:40:26, 51.89s/it][2025-11-07 09:38:12,841] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 516/2088 [7:28:40<22:50:18, 52.30s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9403531243068812, 'learning_rate': 7.528735632183908e-07, 'completion_length': 238.34375, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.515473410487175, 'reward': 1.8435983657836914, 'reward_std': 0.5055685341358185, 'kl': 0.02978515625, 'epoch': 0.49}
 25%|██▍       | 516/2088 [7:28:40<22:50:18, 52.30s/it][2025-11-07 09:39:01,942] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 517/2088 [7:29:29<22:24:17, 51.34s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0093671377525686, 'learning_rate': 7.523946360153256e-07, 'completion_length': 236.72265625, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.72265625, 'rewards/think_step_av_with_neutral_reward': 0.6344711184501648, 'reward': 2.1618149280548096, 'reward_std': 0.4065871089696884, 'kl': 0.0291748046875, 'epoch': 0.5}
 25%|██▍       | 517/2088 [7:29:29<22:24:17, 51.34s/it][2025-11-07 09:39:48,975] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 518/2088 [7:30:16<21:49:36, 50.05s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0884794922678966, 'learning_rate': 7.519157088122605e-07, 'completion_length': 222.40625, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.7734375, 'rewards/think_step_av_with_neutral_reward': 0.6221535801887512, 'reward': 2.176841139793396, 'reward_std': 0.4535662829875946, 'kl': 0.02972412109375, 'epoch': 0.5}
 25%|██▍       | 518/2088 [7:30:16<21:49:36, 50.05s/it][2025-11-07 09:40:41,322] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 519/2088 [7:31:08<22:06:48, 50.74s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.097628557046896, 'learning_rate': 7.514367816091953e-07, 'completion_length': 232.33203125, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5207238346338272, 'reward': 1.958223819732666, 'reward_std': 0.535789966583252, 'kl': 0.03057861328125, 'epoch': 0.5}
 25%|██▍       | 519/2088 [7:31:08<22:06:48, 50.74s/it][2025-11-07 09:41:33,992] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 520/2088 [7:32:01<22:21:06, 51.32s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9836833476215185, 'learning_rate': 7.509578544061303e-07, 'completion_length': 229.15625, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.4776704013347626, 'reward': 1.872201681137085, 'reward_std': 0.4203154444694519, 'kl': 0.02874755859375, 'epoch': 0.5}
 25%|██▍       | 520/2088 [7:32:01<22:21:06, 51.32s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02776.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4043.mp4
[DEBUG] Step 520
[DEBUG] prompt_completion_ids shape: torch.Size([32, 330])
[DEBUG] Shortest output (len=172): <aud_desc>In the audio, the tone is neutral. In the text, the subtitle reads: "Will you accept the honor of being my wife?" This sentence may be the male character's inquiry or request to the other person.</aud_desc>
<vis_desc>In the video, we see a male character with a neutral facial expression, focused and looking at the other person.</vis_desc>
<think>Based on the video clue of the male character's neutral facial expression and focused gaze, as well as the audio clue of the neutral tone, we can infer that this sentence may be an inquiry or request from the male character to the other person, expressing his desire for the other person to accept his honor of being his wife. Therefore, this sentence displays the male character's curiosity and desire for recognition.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=330): <aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "Second friend, and he's dead now, because he also stepped on something." This sentence may be a criticism or accusation from the character on the left to the person on the right.</aud_desc>
<vis_desc>In the video, the scene shows a character wearing military uniform and military gear walking on a sandy battlefield. In the video, his facial expression is relatively calm, with a straight nose and direct eye contact, giving a sense of seriousness. His body posture is relatively relaxed, with his arms bent at the elbows and his gaze steady, conveying a sense of professionalism and seriousness.</vis_desc>
<think>Based on the character's expression and body posture in the video clues, as well as the description in the subtitle, we can infer that the character on the left may be criticizing or accusing the person on the right for his deadness and stepping on something. Since the audio clues cannot determine the character's emotional state, we primarily rely on the video clues of the character's calm facial expression, direct eye contact, and steady body posture to speculate on his emotional state. The description in the subtitle also gives a sense of professionalism and seriousness, implying that the character on the left may hold a rank higher than the person on the right and may have taken on a more responsible role in the event. Therefore, this sentence may carry a sense of seriousness and professionalism, reflecting the character on the left's stern evaluation of the person on the right's actions.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09889.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10682.mp4
[DEBUG] Step 520
[DEBUG] prompt_completion_ids shape: torch.Size([32, 303])
[DEBUG] Shortest output (len=184): <vis_desc>In the video, we see a bald man wearing a military uniform talking on his cellphone in a dimly lit outdoor environment. His facial expression is intense and his mouth is open, seemingly speaking loudly or listening attentively.</vis_desc>
<aud_desc>In the audio, the character's tone is stern and aggressive. The speed of speech is fast, suggesting anger and excitement. In the text, the subtitle reads: " Ronald, you are stupid!" This sentence is likely spoken by the man to another person during the phone call.</aud_desc>
<think>Based on the video clue of the man's intense facial expression and open mouth, as well as the audio clue of the character's stern and aggressive tone, we can infer that this sentence carries a sense of anger and excitement. The man is likely angrily accusing Ronald and criticizing his stupid behavior.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=303): <aud_desc>In the audio, the character's voice sounds very scared, with a fast pace and small tremors. In the text, the subtitle says, "Let the man go fast." This sentence may be a request or warning from the woman to someone nearby.</aud_desc>
<vis_desc>In the video, the screen shows a woman wearing blue medical uniforms, holding a white headband, and looking down at her hands. The environment appears to be a clinical setting, possibly a doctor's office or patient ward. In the video, her eyes are wide open and her facial expression appears serious and focused, as if she is attentively observing or listening to something. Her mouth is slightly open, indicating that she may be speaking or discussing a certain topic actively. Overall, she seems to be in a calm and composed state, possibly engaged in a task or waiting for medical assistance.</vis_desc>
<think>Based on the serious and focused facial expression of the woman in the video, as well as her posture and gaze at the bottom of the frame, it can be inferred that she is attentively observing or waiting for someone's response or medical assistance. Additionally, the description of the character's voice sound as very scared, with a fast pace and small tremors further supports this inference. Therefore, this sentence expresses the woman's fear and anxiety, indicating that she may be trying to prevent a potential risk or danger to someone nearby and avoid potential harm.</think>
<answer>fear</answer>
[2025-11-07 09:42:26,663] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▍       | 521/2088 [7:32:54<22:30:51, 51.72s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9575461790134145, 'learning_rate': 7.504789272030651e-07, 'completion_length': 233.0546875, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.52455073595047, 'reward': 1.8800196051597595, 'reward_std': 0.4289711266756058, 'kl': 0.02679443359375, 'epoch': 0.5}
 25%|██▍       | 521/2088 [7:32:54<22:30:51, 51.72s/it][2025-11-07 09:43:18,694] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 522/2088 [7:33:46<22:32:24, 51.82s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.977547502472858, 'learning_rate': 7.5e-07, 'completion_length': 236.3671875, 'rewards/av_format_reward': 0.875, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.543263852596283, 'reward': 1.9885762929916382, 'reward_std': 0.4967683255672455, 'kl': 0.02789306640625, 'epoch': 0.5}
 25%|██▌       | 522/2088 [7:33:46<22:32:24, 51.82s/it][2025-11-07 09:44:07,025] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 523/2088 [7:34:34<22:04:15, 50.77s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9470988941513665, 'learning_rate': 7.495210727969348e-07, 'completion_length': 225.1328125, 'rewards/av_format_reward': 0.63671875, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.49707961082458496, 'reward': 1.676767110824585, 'reward_std': 0.5283157825469971, 'kl': 0.029296875, 'epoch': 0.5}
 25%|██▌       | 523/2088 [7:34:34<22:04:15, 50.77s/it][2025-11-07 09:44:56,895] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 524/2088 [7:35:24<21:56:22, 50.50s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.761493202497483, 'learning_rate': 7.490421455938697e-07, 'completion_length': 233.59375, 'rewards/av_format_reward': 0.57421875, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.4429001063108444, 'reward': 1.4975876212120056, 'reward_std': 0.3447980135679245, 'kl': 0.02691650390625, 'epoch': 0.5}
 25%|██▌       | 524/2088 [7:35:24<21:56:22, 50.50s/it][2025-11-07 09:45:46,249] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 525/2088 [7:36:13<21:46:34, 50.16s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.038738302690826, 'learning_rate': 7.485632183908046e-07, 'completion_length': 239.0625, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.4429299831390381, 'reward': 1.735898733139038, 'reward_std': 0.5843081176280975, 'kl': 0.030029296875, 'epoch': 0.5}
 25%|██▌       | 525/2088 [7:36:13<21:46:34, 50.16s/it][2025-11-07 09:46:34,282] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 526/2088 [7:37:01<21:29:09, 49.52s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8555196117113943, 'learning_rate': 7.480842911877394e-07, 'completion_length': 228.0390625, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.3927749842405319, 'reward': 1.6232436895370483, 'reward_std': 0.4743736535310745, 'kl': 0.03070068359375, 'epoch': 0.5}
 25%|██▌       | 526/2088 [7:37:01<21:29:09, 49.52s/it][2025-11-07 09:47:21,846] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 527/2088 [7:37:49<21:13:04, 48.93s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.939122392198839, 'learning_rate': 7.476053639846743e-07, 'completion_length': 226.3046875, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.5141140669584274, 'reward': 1.8109890818595886, 'reward_std': 0.4899216443300247, 'kl': 0.031494140625, 'epoch': 0.5}
 25%|██▌       | 527/2088 [7:37:49<21:13:04, 48.93s/it][2025-11-07 09:48:13,715] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 528/2088 [7:38:41<21:35:09, 49.81s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.056362405999848, 'learning_rate': 7.471264367816092e-07, 'completion_length': 226.26953125, 'rewards/av_format_reward': 0.5859375, 'rewards/accuracy_reward': 0.55859375, 'rewards/think_step_av_with_neutral_reward': 0.5744598805904388, 'reward': 1.7189910411834717, 'reward_std': 0.38146622478961945, 'kl': 0.02880859375, 'epoch': 0.51}
 25%|██▌       | 528/2088 [7:38:41<21:35:09, 49.81s/it][2025-11-07 09:49:04,209] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 529/2088 [7:39:31<21:39:37, 50.02s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9628849352526385, 'learning_rate': 7.466475095785441e-07, 'completion_length': 226.26171875, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.46335120499134064, 'reward': 1.8578824996948242, 'reward_std': 0.48458559811115265, 'kl': 0.031494140625, 'epoch': 0.51}
 25%|██▌       | 529/2088 [7:39:31<21:39:37, 50.02s/it][2025-11-07 09:49:54,629] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 530/2088 [7:40:22<21:41:55, 50.14s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.8726371737671241, 'learning_rate': 7.461685823754788e-07, 'completion_length': 230.51953125, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.51171875, 'rewards/think_step_av_with_neutral_reward': 0.4867668002843857, 'reward': 1.7133293151855469, 'reward_std': 0.5666202306747437, 'kl': 0.0286865234375, 'epoch': 0.51}
 25%|██▌       | 530/2088 [7:40:22<21:41:55, 50.14s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09791.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8049.mp4
[DEBUG] Step 530
[DEBUG] prompt_completion_ids shape: torch.Size([32, 296])
[DEBUG] Shortest output (len=147): <vis_desc>In the video, we see a female character with wide eyes and slightly open mouth, seemingly surprised by something.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "Is it a red envelope?" This sentence may be an inquiry or confirmation from the female character to someone outside the frame.</aud_desc>
<think>Based on the facial expression of the female character in the video, with wide eyes and slightly open mouth, we can sense her surprise and confusion. The answer to this question is "surprise," which aligns with the emotions exhibited by the female character.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=296): <vis_desc>In the video, we see a female character with slightly widened eyes and slightly open mouth, showing a scared expression. Her body posture appears somewhat stiff, as if she is trying to maintain balance or stabilize herself. The environment is dark, with some blurry lights around her, suggesting that she may be in a dimly lit corridor or a similar place.</vis_desc>
<aud_desc>In the audio, the character's voice trembles and hissts, indicating a state of fear and anxiety. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The female character’s visual cues clearly convey fear—her slightly widened eyes and slightly open mouth suggest surprise and alarm, while her stiff body posture adds to the impression that she is trying to maintain some level of balance or stabilization, even in an unforeseen situation. The dark surroundings coupled with her fearful expression and body language strongly hint at an emotional state deeply charged—possibly indicating a threat or danger in her midst. Although the audio and subtitle content do not provide clear emotional information, the tremors and hardasts in her voice carry a distinct and compelling tone that aligns with fear, especially considering the context of her words describing a ‘fear’ or ‘anxiety.” Together, these multimodal signals, particularly the visually intensified expressions and body language, convincingly support the prediction that the character is experiencing fear.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1165.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8961.mp4
[DEBUG] Step 530
[DEBUG] prompt_completion_ids shape: torch.Size([32, 279])
[DEBUG] Shortest output (len=180): <vis_desc>In the video, we see a character wearing a colorful top, with a furrowed brow and a serious facial expression, seemingly engaged in a tense conversation or confrontation with someone off-screen.</vis_desc>
<aud_desc>In the audio, the tone is aggressive. In the text, the subtitle reads: "You're welcome, I couldn't stop talking to you." This sentence may be the character emphasizing his previous words or expressing his own opinions.</aud_desc>
<think>Based on the serious expression and furrowed brow of the character in the video, as well as the aggressive tone in the audio, we can infer that this sentence carries a sense of anger or dissatisfaction. The character may be expressing his previous words or emphasizing his own opinions, which aligns with the overall tense and aggressive emotions displayed by the character.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=279): <vis_desc>In the video, we see a woman in a dimly lit indoor environment. Her facial expression appears quite sad and pained. Her eyebrows are furrowed, and the corners of her mouth are slightly downturned, as if she is crying or in a state of sadness.</vis_desc>
<aud_desc>In the audio, the character speaks with a low and tired tone, indicating that the character is probablyOlder than 60 years old. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the woman is experiencing sadness. Visually, her facial expression—marked by furrowed eyebrows, downturned corners of the mouth—creates a compelling image of sadness and pain, while the dimly lit setting contributes to a somber mood. Audibly, the low and tired tone suggests she is older, adding context that could imply time pressure or a general sense of weariness. Although the subtitles do not clarify her emotions, the interplay of her sad facial features and the heavy, subdued tone around her reinforces the interpretation that she is feeling sadness. Together, these multimodal signals create a coherent picture of someone who is deeply emotional and possibly burdened, though the auditory input does not explicitly confirm her feelings.</think>
<answer>sad</answer>
[2025-11-07 09:50:45,894] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 531/2088 [7:41:13<21:49:52, 50.48s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0069614432646077, 'learning_rate': 7.456896551724137e-07, 'completion_length': 228.23046875, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4312436580657959, 'reward': 1.6421812176704407, 'reward_std': 0.35037992894649506, 'kl': 0.03143310546875, 'epoch': 0.51}
 25%|██▌       | 531/2088 [7:41:13<21:49:52, 50.48s/it][2025-11-07 09:51:35,842] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 25%|██▌       | 532/2088 [7:42:03<21:44:54, 50.32s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9463110319779782, 'learning_rate': 7.452107279693486e-07, 'completion_length': 226.73046875, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.4539620280265808, 'reward': 1.5633370280265808, 'reward_std': 0.4911639839410782, 'kl': 0.0301513671875, 'epoch': 0.51}
 25%|██▌       | 532/2088 [7:42:03<21:44:54, 50.32s/it][2025-11-07 09:52:26,217] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 533/2088 [7:42:53<21:44:30, 50.33s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9918257239940886, 'learning_rate': 7.447318007662835e-07, 'completion_length': 238.859375, 'rewards/av_format_reward': 0.8671875, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5813111662864685, 'reward': 2.1242798566818237, 'reward_std': 0.5340453088283539, 'kl': 0.03082275390625, 'epoch': 0.51}
 26%|██▌       | 533/2088 [7:42:53<21:44:30, 50.33s/it][2025-11-07 09:53:16,671] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 534/2088 [7:43:44<21:44:36, 50.37s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9691960036208767, 'learning_rate': 7.442528735632183e-07, 'completion_length': 232.33984375, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5175056904554367, 'reward': 1.8573495149612427, 'reward_std': 0.4958850145339966, 'kl': 0.029052734375, 'epoch': 0.51}
 26%|██▌       | 534/2088 [7:43:44<21:44:36, 50.37s/it][2025-11-07 09:54:07,352] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 535/2088 [7:44:34<21:46:10, 50.46s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8771337949762945, 'learning_rate': 7.437739463601532e-07, 'completion_length': 228.140625, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.3750222474336624, 'reward': 1.5195534825325012, 'reward_std': 0.46152228116989136, 'kl': 0.029541015625, 'epoch': 0.51}
 26%|██▌       | 535/2088 [7:44:34<21:46:10, 50.46s/it][2025-11-07 09:54:56,123] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 536/2088 [7:45:23<21:32:11, 49.96s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.9117926925487903, 'learning_rate': 7.432950191570882e-07, 'completion_length': 228.91796875, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.5546875, 'rewards/think_step_av_with_neutral_reward': 0.48862187564373016, 'reward': 1.8948718309402466, 'reward_std': 0.4168601632118225, 'kl': 0.0286865234375, 'epoch': 0.51}
 26%|██▌       | 536/2088 [7:45:23<21:32:11, 49.96s/it][2025-11-07 09:55:45,990] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 537/2088 [7:46:13<21:30:40, 49.93s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0344147613711563, 'learning_rate': 7.42816091954023e-07, 'completion_length': 235.57421875, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.5125402957201004, 'reward': 1.8601964712142944, 'reward_std': 0.6020025014877319, 'kl': 0.03082275390625, 'epoch': 0.51}
 26%|██▌       | 537/2088 [7:46:13<21:30:40, 49.93s/it][2025-11-07 09:56:34,953] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 538/2088 [7:47:02<21:22:21, 49.64s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0801780382309714, 'learning_rate': 7.423371647509579e-07, 'completion_length': 230.04296875, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.4776192605495453, 'reward': 1.8018380403518677, 'reward_std': 0.521936684846878, 'kl': 0.0322265625, 'epoch': 0.52}
 26%|██▌       | 538/2088 [7:47:02<21:22:21, 49.64s/it][2025-11-07 09:57:27,021] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 539/2088 [7:47:54<21:40:20, 50.37s/it]                                                       {'loss': 0.0013, 'grad_norm': 8.430036991303805, 'learning_rate': 7.418582375478926e-07, 'completion_length': 230.9453125, 'rewards/av_format_reward': 0.84765625, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.4217301458120346, 'reward': 1.8475112915039062, 'reward_std': 0.43197377026081085, 'kl': 0.0316162109375, 'epoch': 0.52}
 26%|██▌       | 539/2088 [7:47:54<21:40:20, 50.37s/it][2025-11-07 09:58:17,074] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 540/2088 [7:48:44<21:37:03, 50.27s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.058312586355338, 'learning_rate': 7.413793103448276e-07, 'completion_length': 231.96875, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.49921277165412903, 'reward': 1.9445253610610962, 'reward_std': 0.5416766107082367, 'kl': 0.0308837890625, 'epoch': 0.52}
 26%|██▌       | 540/2088 [7:48:44<21:37:03, 50.27s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07005.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12629.mp4
[DEBUG] Step 540
[DEBUG] prompt_completion_ids shape: torch.Size([32, 254])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a male character with a furrowed brow and an expressive mouth, seemingly engaged in a heated argument with another person.</vis_desc>
<aud_desc>In the audio, there is a strong tone of command and threat, consistently using the phrase "What are you doing with this shit?" This sentence may be a threat or warning from the male character to the other person.</aud_desc>
<think>Based on the video clues of the male character's furrowed brow and expressive mouth, as well as the strong tone of command and threat in the audio, we can infer that this sentence carries a sense of anger and threatenability. The male character may be expressing his dissatisfaction and threatening the other person, implying that they are commiting a certain act or crimes.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=254): <vis_desc>In the video, we see a character wearing a beanie hat and a red shirt, with a background that appears to be a street scene with buildings and a bare tree. In the video, his facial expression is negative, with a furrowed brow and wide-open eyes, seemingly expressing displeasure or anger. He seems to be staring directly at the camera, possibly engaged in a conversation with someone.</vis_desc>
<aud_desc>In the audio, the voice is strong and the tone is sharp and emotional, indicating that the character is excited and expressing a strong opinion or feeling. In the text, the subtitle says, "Let's start singing! Let's make it like music, just like the movies." This sentence may be the character's suggestion or request.</aud_desc>
<think>Based on the negative facial expression and angry gaze of the character in the video, as well as the excited tone of the character in the audio, we can infer that this sentence carries a sense of anger and dissatisfaction. The character may be dissatisfied with the current situation and hopes to change it through singing and making it like music. Therefore, this sentence expresses the character's anger and expectations for resolution.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7004.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00699.mp4
[DEBUG] Step 540
[DEBUG] prompt_completion_ids shape: torch.Size([32, 304])
[DEBUG] Shortest output (len=163): <vis_desc>In the video, we see a character with a furrowed brow and slightly downturned mouth, seemingly experiencing an unpleasant emotion, possibly reading something negative.</vis_desc>
<aud_desc>In the audio, the voice is soft and the tone is calm. In the text, the subtitle says "Jane." This sentence may be the character's reaction to encountering something negative.</aud_desc>
<think>Based on the visual clue of the character's furrowed brow and slightly downturned mouth, as well as the audio clue of the character's calm voice and downward tone, we can infer that this sentence may carry a sense of sadness or disappointment. The character may feel dissatisfied with what he is reading and may be reflecting on his previous decisions.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=304): <vis_desc>In the video, we see a young woman with a curled upwards curve at the top of her head, wearing a red and black plaid shirt. Her eyes are fixed on a gift bag, and she seems to be reading the presents or trying to find something interesting inside.</vis_desc>
<aud_desc>Sheeps wire is sharp as a knife, and there are no screws or rivets, just simple rivets. It seems like some old bird is trying to break free from the cage, and the lady is determined to capture it. The sound is very quiet, as if she is listening intently, waiting for the right moment to act. In the end, she lifts the camera to look at the gift bag, and there is a sharp moment of realization: it is a bag of cash.</aud_desc>
<think>Based on the video clue of the woman wearing a red and black plaid shirt, with her eyes fixed on a gift bag and her facial expression appearing somewhat surprised, as if she is seeing something unexpected, we can infer that this surprise could be related to the contents of the bag or some unexpected situation. The audio clue describing the sound as very quiet and the woman's determined gaze imply a sense of immediacy and urgency, as if she is anticipating an immediate action. Therefore, this surprise could potentially stem from encountering something threatening or unexpected, aligning with the emotional states portrayed in the video and audio clues.</think>
<answer>surprise</answer>
[2025-11-07 09:59:11,280] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 541/2088 [7:49:38<22:06:38, 51.45s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9789181872644417, 'learning_rate': 7.409003831417624e-07, 'completion_length': 233.78515625, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.62890625, 'rewards/think_step_av_with_neutral_reward': 0.6109364032745361, 'reward': 1.9859364032745361, 'reward_std': 0.4217469245195389, 'kl': 0.02996826171875, 'epoch': 0.52}
 26%|██▌       | 541/2088 [7:49:38<22:06:38, 51.45s/it][2025-11-07 10:00:02,656] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 542/2088 [7:50:30<22:05:11, 51.43s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9733779297207987, 'learning_rate': 7.404214559386973e-07, 'completion_length': 230.4765625, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.7421875, 'rewards/think_step_av_with_neutral_reward': 0.5607694387435913, 'reward': 2.0451444387435913, 'reward_std': 0.5051231384277344, 'kl': 0.03070068359375, 'epoch': 0.52}
 26%|██▌       | 542/2088 [7:50:30<22:05:11, 51.43s/it][2025-11-07 10:00:53,085] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 543/2088 [7:51:20<21:56:35, 51.13s/it]                                                       {'loss': 0.0014, 'grad_norm': 1.934023930085063, 'learning_rate': 7.399425287356321e-07, 'completion_length': 234.62890625, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.49609375, 'rewards/think_step_av_with_neutral_reward': 0.44421107321977615, 'reward': 1.7449923753738403, 'reward_std': 0.4485391676425934, 'kl': 0.03387451171875, 'epoch': 0.52}
 26%|██▌       | 543/2088 [7:51:20<21:56:35, 51.13s/it][2025-11-07 10:01:47,681] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 544/2088 [7:52:15<22:22:30, 52.17s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0338036027779376, 'learning_rate': 7.394636015325671e-07, 'completion_length': 227.7578125, 'rewards/av_format_reward': 0.8125, 'rewards/accuracy_reward': 0.62109375, 'rewards/think_step_av_with_neutral_reward': 0.5314125120639801, 'reward': 1.9650062918663025, 'reward_std': 0.46372057497501373, 'kl': 0.0318603515625, 'epoch': 0.52}
 26%|██▌       | 544/2088 [7:52:15<22:22:30, 52.17s/it][2025-11-07 10:02:39,631] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 545/2088 [7:53:07<22:19:56, 52.10s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8971107242804592, 'learning_rate': 7.389846743295019e-07, 'completion_length': 237.875, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5687543749809265, 'reward': 1.9906293749809265, 'reward_std': 0.5246894657611847, 'kl': 0.02886962890625, 'epoch': 0.52}
 26%|██▌       | 545/2088 [7:53:07<22:19:56, 52.10s/it][2025-11-07 10:03:24,320] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 546/2088 [7:53:51<21:21:53, 49.88s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.1409910059789388, 'learning_rate': 7.385057471264368e-07, 'completion_length': 219.8359375, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.43266524374485016, 'reward': 1.616258978843689, 'reward_std': 0.5141375958919525, 'kl': 0.0343017578125, 'epoch': 0.52}
 26%|██▌       | 546/2088 [7:53:51<21:21:53, 49.88s/it][2025-11-07 10:04:15,087] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 547/2088 [7:54:42<21:27:54, 50.15s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0002958754209264, 'learning_rate': 7.380268199233715e-07, 'completion_length': 240.09765625, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4595562219619751, 'reward': 1.822837471961975, 'reward_std': 0.364862859249115, 'kl': 0.0311279296875, 'epoch': 0.52}
 26%|██▌       | 547/2088 [7:54:42<21:27:54, 50.15s/it][2025-11-07 10:05:08,877] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▌       | 548/2088 [7:55:36<21:55:07, 51.24s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8818773747065944, 'learning_rate': 7.375478927203065e-07, 'completion_length': 226.94921875, 'rewards/av_format_reward': 0.80078125, 'rewards/accuracy_reward': 0.671875, 'rewards/think_step_av_with_neutral_reward': 0.5674400627613068, 'reward': 2.040096402168274, 'reward_std': 0.30558307468891144, 'kl': 0.03045654296875, 'epoch': 0.52}
 26%|██▌       | 548/2088 [7:55:36<21:55:07, 51.24s/it][2025-11-07 10:06:00,037] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 549/2088 [7:56:27<21:53:40, 51.22s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.1420974054281667, 'learning_rate': 7.370689655172413e-07, 'completion_length': 219.8203125, 'rewards/av_format_reward': 0.875, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.4821287542581558, 'reward': 1.8844725489616394, 'reward_std': 0.3836929202079773, 'kl': 0.0322265625, 'epoch': 0.53}
 26%|██▋       | 549/2088 [7:56:27<21:53:40, 51.22s/it][2025-11-07 10:06:54,963] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 550/2088 [7:57:22<22:21:21, 52.33s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.1159291121462815, 'learning_rate': 7.365900383141762e-07, 'completion_length': 233.65625, 'rewards/av_format_reward': 0.83984375, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5897793769836426, 'reward': 2.0546231865882874, 'reward_std': 0.566120058298111, 'kl': 0.0321044921875, 'epoch': 0.53}
 26%|██▋       | 550/2088 [7:57:22<22:21:21, 52.33s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3120.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12175.mp4
[DEBUG] Step 550
[DEBUG] prompt_completion_ids shape: torch.Size([32, 311])
[DEBUG] Shortest output (len=179): <vis_desc>In the video, we see a shirtless man with a serious facial expression, wide eyes, and tightly closed lips. His gaze is fixed on someone or something outside the frame, giving off an overall sense of tension.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the emotional state of the person. In the text, the character emphasizes "Ah" in repeated sentences, suggesting that the character is feeling puzzled or surprised.</aud_desc>
<think>Based on the serious facial expression and wide eyes of the man in the video, as well as the emphasis on "Ah" in the audio, we can infer that the man is experiencing frustration or uneasiness. This is because he cannot resolve his inner confusion or tension, which aligns with the overall sense of tension displayed by the man.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=311): <vis_desc>In the video, we see a man wearing a white gi, holding a cleaning tool, walking away from the camera, and passing in front of a building with trees and a parked motorcycle. We can infer that the man is likely a security guard or someone working in a supervised environment, such as an apartment or a business area. He may be wearing a gi, which typically means he is wearing a chef's outfit, suggesting that he may be working with food or prepared foods.</vis_desc>
<aud_desc>In the audio, footsteps and the sound of paper being rolled can be heard. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The man's behavior—walking away from the camera, passing in front of a building with trees and a parked motorcycle—suggests he is moving through an outdoor supervised environment, possibly maintaining a calm and controlled demeanor despite some distractions in the background. His wearing of a chef's outfit further implies that he may be in a supervisory or professional setting. Additionally, the sounds of footsteps and the roll of paper further suggest his work in an orderly manner. Although the subtitle content offers no clear emotional cues, the subtle but noticeable actions of the man across the scene indicate a smooth, unperturbed flow into his work environment. Together, these multimodal cues create a coherent impression that the man is in a relaxed and orderly state, possibly indicating an uninterrupted day even in the midst of routine.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07450.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08385.mp4
[DEBUG] Step 550
[DEBUG] prompt_completion_ids shape: torch.Size([32, 339])
[DEBUG] Shortest output (len=188): <vis_desc>In the video, the screen shows a man sitting in an indoor setting. In the video, his facial expression appears serious, and his mouth is moving, indicating that he is engaged in a formal conversation or discussion.</vis_desc>
<aud_desc>In the audio, the tone is aggressive and the subtitle carries the tone "Oh no, I can't have that waste of an innocent drink." This sentence may be the man's response to some unsatisfactory situation or event.</aud_desc>
<think>Based on the serious facial expression and movement of the mouth in the video clues, as well as the aggressive tone in the audio clue, we can infer that this sentence is the man expressing his dissatisfaction and negative emotions regarding the destruction of a waste of an innocent drink. The man may be criticizing or complaining about a certain viewpoint or action, expressing his discontent and anger.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=339): <vis_desc>In the video, the scene shows a man sitting on the floor of a bedroom, which can be deduced from the bed he is sitting on and the lamp in the background. At the beginning of the video, he slightly furrows his brow, raising his head with a slight upward curve at the corners, which usually indicates a sense of confusion or distress. His eyes appear somewhat sad, possibly due to feelings of sadness or disappointment. As he turns his head, his gaze looks down, and the slight furrowing of his brow becomes more pronounced, possibly due to internal conflict or unease. Overall, he may be experiencing a conversation that brings him some unpleasant emotions.</vis_desc>
<aud_desc>In the audio, combined with the text content, it can be felt that the character is sad because he is having a conversation with an individual who is not his family. Combined with the subtitle content, it can be inferred that the character has invested a lot of money in this individual. In the text, the subtitle says, "It's not the other person's fault, they just want money." This sentence expresses the man's inner feelings and emotions.</aud_desc>
<think>Based on the visual clues of the man furrowing his brow, slightly upward curve of the corner of his mouth, and sad-eyed expression, it can be deduced that the man may have experienced a conversation that made him feel sad and disappointed. Additionally, the audio clue describing the character's sadness due to the other person's decisions also supports this inference. Therefore, this sentence expresses the man's sense of dissatisfaction and sadness towards the other person.</think>
<answer>sad</answer>
[2025-11-07 10:07:42,138] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 551/2088 [7:58:09<21:40:52, 50.78s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9201280643441412, 'learning_rate': 7.361111111111111e-07, 'completion_length': 228.6484375, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.43071286380290985, 'reward': 1.7393066883087158, 'reward_std': 0.3700777441263199, 'kl': 0.0296630859375, 'epoch': 0.53}
 26%|██▋       | 551/2088 [7:58:09<21:40:52, 50.78s/it][2025-11-07 10:08:31,576] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 552/2088 [7:58:59<21:29:42, 50.38s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.137605218408516, 'learning_rate': 7.35632183908046e-07, 'completion_length': 224.4140625, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5185779333114624, 'reward': 1.8427966833114624, 'reward_std': 0.6108603775501251, 'kl': 0.029541015625, 'epoch': 0.53}
 26%|██▋       | 552/2088 [7:58:59<21:29:42, 50.38s/it][2025-11-07 10:09:21,430] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 26%|██▋       | 553/2088 [7:59:49<21:24:49, 50.22s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.120213836803007, 'learning_rate': 7.351532567049809e-07, 'completion_length': 218.234375, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.68359375, 'rewards/think_step_av_with_neutral_reward': 0.5848060548305511, 'reward': 2.0027748346328735, 'reward_std': 0.585861012339592, 'kl': 0.0318603515625, 'epoch': 0.53}
 26%|██▋       | 553/2088 [7:59:49<21:24:49, 50.22s/it][2025-11-07 10:10:11,922] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 554/2088 [8:00:39<21:26:04, 50.30s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9063014763677044, 'learning_rate': 7.346743295019157e-07, 'completion_length': 235.90234375, 'rewards/av_format_reward': 0.64453125, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4582277834415436, 'reward': 1.6496340036392212, 'reward_std': 0.423978716135025, 'kl': 0.03009033203125, 'epoch': 0.53}
 27%|██▋       | 554/2088 [8:00:39<21:26:04, 50.30s/it][2025-11-07 10:11:02,274] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 555/2088 [8:01:29<21:25:36, 50.32s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9083575446955, 'learning_rate': 7.341954022988505e-07, 'completion_length': 237.29296875, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.4266799986362457, 'reward': 1.7118362784385681, 'reward_std': 0.48829351365566254, 'kl': 0.03076171875, 'epoch': 0.53}
 27%|██▋       | 555/2088 [8:01:29<21:25:36, 50.32s/it][2025-11-07 10:11:55,330] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 556/2088 [8:02:22<21:45:45, 51.14s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.108373638799172, 'learning_rate': 7.337164750957854e-07, 'completion_length': 224.11328125, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.5452677011489868, 'reward': 1.935892641544342, 'reward_std': 0.41186898946762085, 'kl': 0.03045654296875, 'epoch': 0.53}
 27%|██▋       | 556/2088 [8:02:22<21:45:45, 51.14s/it][2025-11-07 10:12:51,606] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 557/2088 [8:03:19<22:24:13, 52.68s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0167436793652596, 'learning_rate': 7.332375478927203e-07, 'completion_length': 229.58984375, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.59765625, 'rewards/think_step_av_with_neutral_reward': 0.5525021404027939, 'reward': 1.8415645956993103, 'reward_std': 0.48260498046875, 'kl': 0.0311279296875, 'epoch': 0.53}
 27%|██▋       | 557/2088 [8:03:19<22:24:13, 52.68s/it][2025-11-07 10:13:42,278] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 558/2088 [8:04:09<22:07:58, 52.08s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9422723309080188, 'learning_rate': 7.327586206896551e-07, 'completion_length': 233.25390625, 'rewards/av_format_reward': 0.92578125, 'rewards/accuracy_reward': 0.625, 'rewards/think_step_av_with_neutral_reward': 0.5309225171804428, 'reward': 2.081703782081604, 'reward_std': 0.4160888344049454, 'kl': 0.02978515625, 'epoch': 0.53}
 27%|██▋       | 558/2088 [8:04:09<22:07:58, 52.08s/it][2025-11-07 10:14:30,977] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 559/2088 [8:04:58<21:41:16, 51.06s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.925633913577502, 'learning_rate': 7.3227969348659e-07, 'completion_length': 231.5546875, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.43742963671684265, 'reward': 1.6757108569145203, 'reward_std': 0.5322719365358353, 'kl': 0.03021240234375, 'epoch': 0.54}
 27%|██▋       | 559/2088 [8:04:58<21:41:16, 51.06s/it][2025-11-07 10:15:24,672] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 560/2088 [8:05:52<22:00:31, 51.85s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0554647732368383, 'learning_rate': 7.318007662835249e-07, 'completion_length': 235.59375, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.39968349039554596, 'reward': 1.6535897254943848, 'reward_std': 0.4909975826740265, 'kl': 0.031982421875, 'epoch': 0.54}
 27%|██▋       | 560/2088 [8:05:52<22:00:31, 51.85s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2281.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4182.mp4
[DEBUG] Step 560
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=168): <vis_desc>In the video, we see a man wearing a chef's uniform holding a spade in his hand. His facial expression is fierce and his mouth is wide open, seemingly expressing strong emotions.</vis_desc>
<aud_desc>In the audio, the tone is aggressive, with a sharp and forceful speech, exclaiming "Okay, let's do that." This sentence may be a response from the man to someone off-screen.</aud_desc>
<think>Based on the man's fierce facial expression and open mouth in the video, as well as the aggressive tone in the audio, we can infer that this sentence may carry a sense of anger or a challenge. The man may be responding to a suggestion or demand from someone off-screen, expressing his dissatisfaction or encouragement)</think>
<answer>angry</answer>
[DEBUG] Longest output (len=295): <vis_desc>In the video, we see a woman leaning against a wooden fence, with her head and shoulders hidden behind the metal, and her hands pressing onto the fence. Her facial expression appears very scared, with wide-open eyes and slightly open mouth, seemingly crying out or shouting, possibly experiencing an intense fear.</vis_desc>
<aud_desc>In the audio, footsteps and water sounds can be heard in the background, suggesting that the woman is in a flooded environment, possibly a dungeon or a similar dark place. The volume and tone of the sound remain constant, indicating that it is a short period of recording. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Considering both the visual and auditory cues, the woman’s visual expression clearly conveys a heightened level of fear. Her wide-open eyes and slightly open mouth indicate that she is reacting to an immediate and potentially threatening stimulus, such as a sudden threat from marine life or another hostile presence. The context of the backdrop— flooding or dark, watery surroundings—adds uncertainty and intensity to the scene, suggesting that whatever fear she is experiencing may be unique or unsettling in such environments. Although the auditory background and subtitle content do not provide additional emotional context, the powerful visual signals together strongly suggest that she is experiencing fear. Therefore, integrating all available information, fear becomes the most coherent and supported emotional prediction.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9105.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/12180.mp4
[DEBUG] Step 560
[DEBUG] prompt_completion_ids shape: torch.Size([32, 289])
[DEBUG] Shortest output (len=178): <vis_desc>In the video, we see an elderly man sitting in a courtroom. His facial expression appears relaxed, with slightly squinted eyes, smiling gently, and large eyes that seem to sparkle. Overall, he is experiencing a pleasant conversation.</vis_desc>
<aud_desc>In the audio, the character speaks with a stern and impatient tone. The emphasis is on "Oh shut up, officer." This sentence may be the man's way of accusing the other person.</aud_desc>
<think>Based on the smile and lively expression of the man in the video clues, as well as the stern and impatient tone in the audio clues, we can infer that this sentence carries a sense of dissatisfaction or anger. The man may be unhappy with the other person's behavior or remarks and expresses his viewpoint in a stern and aggressive manner.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=289): <vis_desc>In the video, the scene shows an elderly man sitting in a courtroom, wearing glasses, a brown shirt, and a blue shirt tied with a waistcoat. He is looking down with his eyes closed, seemingly contemplating something. In the middle of the video, he lifts his head and looks at someone above him, with his mouth open, seemingly engaged in a heated argument with that person.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. Combined with the text content, we can sense the character's excitement and anger. The subtitle in the text says, "Oh, shut up, officer." This sentence may be an expression of the elderly man's anger and excitement.</aud_desc>
<think>Based on the facial expression and posture of the elderly man in the video clues, his facial expression is described as such: the corners of his mouth are slightly upturned, his eyes are wide open, and he is looking at someone above him, with his mouth open, indicating an excited and angry emotion. Based on the description of the stern and aggressive tone in the audio clues, as well as the expression of the elderly man in the video clues, we can infer that this sentence is an expression of the elderly man's anger and excitement. Overall, this sentence shows the old man's impatience and anger towards the person above him.</think>
<answer>angry</answer>
[2025-11-07 10:16:14,362] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 561/2088 [8:06:41<21:43:08, 51.20s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9233407985895514, 'learning_rate': 7.313218390804598e-07, 'completion_length': 222.046875, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.51953125, 'rewards/think_step_av_with_neutral_reward': 0.4740560054779053, 'reward': 1.6107747554779053, 'reward_std': 0.515376403927803, 'kl': 0.030029296875, 'epoch': 0.54}
 27%|██▋       | 561/2088 [8:06:41<21:43:08, 51.20s/it][2025-11-07 10:17:02,106] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 562/2088 [8:07:29<21:15:54, 50.17s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0300749152096262, 'learning_rate': 7.308429118773946e-07, 'completion_length': 225.67578125, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.52734375, 'rewards/think_step_av_with_neutral_reward': 0.4725709706544876, 'reward': 1.8163210153579712, 'reward_std': 0.48292484879493713, 'kl': 0.031005859375, 'epoch': 0.54}
 27%|██▋       | 562/2088 [8:07:29<21:15:54, 50.17s/it][2025-11-07 10:17:55,169] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 563/2088 [8:08:22<21:37:08, 51.03s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.1410494496478925, 'learning_rate': 7.303639846743294e-07, 'completion_length': 233.1796875, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5749341249465942, 'reward': 2.035871684551239, 'reward_std': 0.4856899529695511, 'kl': 0.03271484375, 'epoch': 0.54}
 27%|██▋       | 563/2088 [8:08:22<21:37:08, 51.03s/it][2025-11-07 10:18:44,968] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 564/2088 [8:09:12<21:26:52, 50.66s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9673982157937455, 'learning_rate': 7.298850574712644e-07, 'completion_length': 234.88671875, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.34375, 'rewards/think_step_av_with_neutral_reward': 0.277870774269104, 'reward': 1.355995774269104, 'reward_std': 0.5075442492961884, 'kl': 0.031982421875, 'epoch': 0.54}
 27%|██▋       | 564/2088 [8:09:12<21:26:52, 50.66s/it][2025-11-07 10:19:31,333] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 565/2088 [8:09:58<20:53:17, 49.37s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9461952807235923, 'learning_rate': 7.294061302681992e-07, 'completion_length': 222.5, 'rewards/av_format_reward': 0.6328125, 'rewards/accuracy_reward': 0.68359375, 'rewards/think_step_av_with_neutral_reward': 0.5558514595031738, 'reward': 1.8722577691078186, 'reward_std': 0.5248948931694031, 'kl': 0.031005859375, 'epoch': 0.54}
 27%|██▋       | 565/2088 [8:09:58<20:53:17, 49.37s/it][2025-11-07 10:20:18,132] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 566/2088 [8:10:45<20:32:51, 48.60s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.2282645131333596, 'learning_rate': 7.289272030651341e-07, 'completion_length': 224.91796875, 'rewards/av_format_reward': 0.6640625, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.6038835644721985, 'reward': 1.9202898740768433, 'reward_std': 0.47009581327438354, 'kl': 0.031982421875, 'epoch': 0.54}
 27%|██▋       | 566/2088 [8:10:45<20:32:51, 48.60s/it][2025-11-07 10:21:10,817] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 567/2088 [8:11:38<21:03:06, 49.83s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.119051196058144, 'learning_rate': 7.284482758620689e-07, 'completion_length': 235.3828125, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.35546875, 'rewards/think_step_av_with_neutral_reward': 0.3810161352157593, 'reward': 1.443516194820404, 'reward_std': 0.4873518794775009, 'kl': 0.0318603515625, 'epoch': 0.54}
 27%|██▋       | 567/2088 [8:11:38<21:03:06, 49.83s/it][2025-11-07 10:21:57,060] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 568/2088 [8:12:24<20:35:02, 48.75s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.103396103546829, 'learning_rate': 7.279693486590039e-07, 'completion_length': 223.953125, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.58203125, 'rewards/think_step_av_with_neutral_reward': 0.4838470071554184, 'reward': 1.9369719624519348, 'reward_std': 0.4582708775997162, 'kl': 0.0328369140625, 'epoch': 0.54}
 27%|██▋       | 568/2088 [8:12:24<20:35:02, 48.75s/it][2025-11-07 10:22:42,510] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 569/2088 [8:13:10<20:09:09, 47.76s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.066678446856081, 'learning_rate': 7.274904214559387e-07, 'completion_length': 222.56640625, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.65625, 'rewards/think_step_av_with_neutral_reward': 0.5078637897968292, 'reward': 1.9492700099945068, 'reward_std': 0.42658571898937225, 'kl': 0.03082275390625, 'epoch': 0.55}
 27%|██▋       | 569/2088 [8:13:10<20:09:09, 47.76s/it][2025-11-07 10:23:33,311] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 570/2088 [8:14:00<20:31:26, 48.67s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.085416025847548, 'learning_rate': 7.270114942528736e-07, 'completion_length': 230.69140625, 'rewards/av_format_reward': 0.82421875, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.6184336245059967, 'reward': 2.083277404308319, 'reward_std': 0.5372197031974792, 'kl': 0.0350341796875, 'epoch': 0.55}
 27%|██▋       | 570/2088 [8:14:00<20:31:26, 48.67s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00624.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07028.mp4
[DEBUG] Step 570
[DEBUG] prompt_completion_ids shape: torch.Size([32, 310])
[DEBUG] Shortest output (len=157): <aud_desc>In the audio, the character's tone is aggressive and the speech speed is fast, indicating that the character is extremely angry. In the text, the subtitle reads: "You sent me goddamned audios." This sentence can be understood based on the tone and speed of the audio clue.</aud_desc>
<vis_desc>In the video, we see a man sitting at a table, seemingly performing aGodamda performance.</vis_desc>
<think>Based on the aggressive tone and fast speech speed described in the audio clue, as well as the context of the man performing aGodamda, we can infer that this sentence is a negative expression from the man towards someone, expressing his anger and discontent.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=310): <vis_desc>In the video, we see a male character with his counterpart facing away from the camera. We mainly analyze the emotional state of the male character. In the video, his facial expression is serious and his mouth is moving, indicating that he is engaged in a serious conversation. His gaze is directly focused on the other person, suggesting that he is attentive to the other person's remarks or questions. Overall, the male character is engaged in a neutral-to-sad emotion conversation.</vis_desc>
<aud_desc>In the audio, the speech is fast-paced and the tone is aggressive. Combined with the text content, it seems that he is accusing the other person. The subtitle in the text says, "Amid all your complaints, aren't you afraid? It's not your fault." This sentence could be an expression of anger or dissatisfaction from the male character towards the other person.</aud_desc>
<think>Based on the serious facial expression and mouth movement of the male character in the video, as well as his direct gaze towards the other person, it can be inferred that he is engaged in a serious conversation and appears to be engaged in a negative dialogue with the other person. In addition, the forward gaze of the male character's counterpart and the direct and aggressive tone in the audio further support the accusation nature of the sentence. Therefore, this sentence could be an expression of dissatisfaction or anger from the male character towards the other person, as he holds a negative attitude towards their accusations.</think>
<answer>angry</answer>
[mpeg4 @ 0x115594940] Error at MB: 1580
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/06882.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/09797.mp4
[DEBUG] Step 570
[DEBUG] prompt_completion_ids shape: torch.Size([32, 343])
[DEBUG] Shortest output (len=187): <aud_desc>In the audio, the tone is stern and aggressive. In the text, the subtitle reads: "But now, do you realize how dumb you look appearing fake to someone?" This sentence may be the male character's comment on his own behavior or state.</aud_desc>
<vis_desc>In the video, we see a male character with a furrowed brow and an excited facial expression. His gaze is directed towards someone off-screen, indicating that he is engaged in a heated discussion or argument.</vis_desc>
<think>Based on the video clues of the male character's furrowed brow and excited facial expression, as well as the audio clue of the stern and aggressive tone, we can infer that this sentence expresses the male character's dissatisfaction and anger towards his own behavior or state. The overall emotional state of the male character is one of anger and aggression.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=343): <vis_desc>In the video, the environment appears to be an indoor setting, possibly a restaurant or a casual dining atmosphere. In the video, the character's facial expression is fierce and fierce, with an angry and threatening tone. He stares directly at the other person, with his mouth open, seemingly accusing them and begging them to end their existence. The smoke from the food in the background suggests that the character's dining time is heavy, and the other person may be having food that has been around for a while, creating a tense atmosphere.</vis_desc>
<aud_desc>In the audio, the volume is loud and the tone is stern and aggressive. Combined with the text content, it seems that the character is accusing the other person and begging them to end their existence. The meaning behind this accusation and this tone of voice should be analyzed in conjunction with additional context.</aud_desc>
<think>The character's visual expression and auditory tone convey a strong sense of anger and threateniveness from start to finish. His stern and aggressive voice reinforces a intimidating presence, while his accusatory gaze and fierce facial characteristics highlight his resentment and impending severity towards the other person. The constant tense atmosphere created by the heavy smoke from the food further reinforces the idea that this is a life-threatening situation. Complementing these visuals, the auditory tone, characterized by loud volume and sternness, effectively communicates the character's intense emotions and accusatory emotions. Together, these multimodal cues point to the character as one driven by rage and hunger, where anger and threat manifest as an authoritative and intimidating voice, while the threatening and accusatory vocal tone aligns with a volatile and threatening mindset.</think>
<answer>angry</answer>
[2025-11-07 10:24:22,585] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 571/2088 [8:14:50<20:35:10, 48.85s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0337918827267645, 'learning_rate': 7.265325670498083e-07, 'completion_length': 226.16796875, 'rewards/av_format_reward': 0.71875, 'rewards/accuracy_reward': 0.71484375, 'rewards/think_step_av_with_neutral_reward': 0.6029150187969208, 'reward': 2.0365087389945984, 'reward_std': 0.41553573310375214, 'kl': 0.03314208984375, 'epoch': 0.55}
 27%|██▋       | 571/2088 [8:14:50<20:35:10, 48.85s/it][2025-11-07 10:25:11,505] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 572/2088 [8:15:39<20:34:51, 48.87s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.8860107875826586, 'learning_rate': 7.260536398467432e-07, 'completion_length': 219.5703125, 'rewards/av_format_reward': 0.92578125, 'rewards/accuracy_reward': 0.40234375, 'rewards/think_step_av_with_neutral_reward': 0.3735641837120056, 'reward': 1.7016891241073608, 'reward_std': 0.48694345355033875, 'kl': 0.032470703125, 'epoch': 0.55}
 27%|██▋       | 572/2088 [8:15:39<20:34:51, 48.87s/it][2025-11-07 10:26:02,340] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 573/2088 [8:16:29<20:48:54, 49.46s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9746642020071419, 'learning_rate': 7.255747126436781e-07, 'completion_length': 224.125, 'rewards/av_format_reward': 0.67578125, 'rewards/accuracy_reward': 0.32421875, 'rewards/think_step_av_with_neutral_reward': 0.31761641800403595, 'reward': 1.3176164031028748, 'reward_std': 0.567731499671936, 'kl': 0.03057861328125, 'epoch': 0.55}
 27%|██▋       | 573/2088 [8:16:29<20:48:54, 49.46s/it][2025-11-07 10:26:56,007] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 27%|██▋       | 574/2088 [8:17:23<21:19:54, 50.72s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.070689984442876, 'learning_rate': 7.25095785440613e-07, 'completion_length': 223.046875, 'rewards/av_format_reward': 0.515625, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.48442262411117554, 'reward': 1.6016101837158203, 'reward_std': 0.6142386198043823, 'kl': 0.0338134765625, 'epoch': 0.55}
 27%|██▋       | 574/2088 [8:17:23<21:19:54, 50.72s/it][2025-11-07 10:27:45,947] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 575/2088 [8:18:13<21:13:09, 50.49s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0767436472201672, 'learning_rate': 7.246168582375478e-07, 'completion_length': 227.625, 'rewards/av_format_reward': 0.6640625, 'rewards/accuracy_reward': 0.6953125, 'rewards/think_step_av_with_neutral_reward': 0.5846576690673828, 'reward': 1.9440327882766724, 'reward_std': 0.4758298695087433, 'kl': 0.03173828125, 'epoch': 0.55}
 28%|██▊       | 575/2088 [8:18:13<21:13:09, 50.49s/it][2025-11-07 10:28:39,016] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 576/2088 [8:19:06<21:31:48, 51.26s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.979502195020209, 'learning_rate': 7.241379310344827e-07, 'completion_length': 222.08203125, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.5022016018629074, 'reward': 1.9123579263687134, 'reward_std': 0.4184166193008423, 'kl': 0.0323486328125, 'epoch': 0.55}
 28%|██▊       | 576/2088 [8:19:06<21:31:48, 51.26s/it][2025-11-07 10:29:30,595] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 577/2088 [8:19:58<21:33:21, 51.36s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0648722969977835, 'learning_rate': 7.236590038314177e-07, 'completion_length': 217.71875, 'rewards/av_format_reward': 0.59375, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5679838359355927, 'reward': 1.8140775561332703, 'reward_std': 0.4295659065246582, 'kl': 0.02996826171875, 'epoch': 0.55}
 28%|██▊       | 577/2088 [8:19:58<21:33:21, 51.36s/it][2025-11-07 10:30:25,633] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 578/2088 [8:20:53<22:00:17, 52.46s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.0083388406503357, 'learning_rate': 7.231800766283525e-07, 'completion_length': 219.2578125, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5801979005336761, 'reward': 1.9356666803359985, 'reward_std': 0.45124389231204987, 'kl': 0.0340576171875, 'epoch': 0.55}
 28%|██▊       | 578/2088 [8:20:53<22:00:17, 52.46s/it][2025-11-07 10:31:16,383] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 579/2088 [8:21:43<21:46:29, 51.95s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9796630386530554, 'learning_rate': 7.227011494252873e-07, 'completion_length': 215.36328125, 'rewards/av_format_reward': 0.72265625, 'rewards/accuracy_reward': 0.70703125, 'rewards/think_step_av_with_neutral_reward': 0.7053075730800629, 'reward': 2.1349949836730957, 'reward_std': 0.44820670783519745, 'kl': 0.031982421875, 'epoch': 0.55}
 28%|██▊       | 579/2088 [8:21:43<21:46:29, 51.95s/it][2025-11-07 10:32:05,930] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 580/2088 [8:22:33<21:27:30, 51.23s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9886879237314603, 'learning_rate': 7.222222222222221e-07, 'completion_length': 233.8515625, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5848903059959412, 'reward': 1.955984115600586, 'reward_std': 0.5298231840133667, 'kl': 0.0323486328125, 'epoch': 0.56}
 28%|██▊       | 580/2088 [8:22:33<21:27:30, 51.23s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08561.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8002.mp4
[DEBUG] Step 580
[DEBUG] prompt_completion_ids shape: torch.Size([32, 249])
[DEBUG] Shortest output (len=178): <aud_desc>In the audio, the character's voice soundshappy and relaxed. In the text, the subtitle says, "Sure, it's fourteen times." This sentence may be the man's response to a question or statement.</aud_desc>
<vis_desc>In the video, we see a man with a happy expression, smile, and widened eyes, which may indicate that he is feeling happy or satisfied. This may be due to the pleasant conversation or event that brought him pleasure.</vis_desc>
<think>Based on the audio clue describing the character's voice as happy and relaxed, as well as the man's happy expression and smile in the video clue, we can infer that this sentence expresses the man's satisfaction and enjoyment of the positive event or conversation. The man may find this number interesting or funny, hence expressing this sentence.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=249): <vis_desc>In the video, we see a woman in a dim environment, smiling with a slight upward curve at the corners of her mouth, looking directly at the other person, and her arms being crossed. It can be inferred that she is likely the male's wife. In the middle of the video, her mouth opens, seemingly teasing the other person, and there is laughter in her voice. Overall, the female character is in a happy emotional state, possibly engaged in pleasant conversation.</vis_desc>
<aud_desc>In the audio, the laughter can be felt, indicating that the character's mood is also quiteHappy. In the text, the subtitle says "Fourteen times." This sentence may be a playful response from the male character to the lady in the dim environment.</aud_desc>
<think>Based on the positive emotions displayed by the female character in the video, smiling, direct eye contact, and the action of her mouth opening, it can be inferred that she is likely in a happy emotional state. Additionally, the laughter in the audio further supports her joyful mood. Therefore, this sentence may be an attempt to humorously engage with the other person, expressing a positive attitude.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00569.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04757.mp4
[DEBUG] Step 580
[DEBUG] prompt_completion_ids shape: torch.Size([32, 308])
[DEBUG] Shortest output (len=162): <vis_desc>In the video, we see a male character sitting indoors. His gaze initially focuses towards the left, then shifts towards the right before returning to the left, as if examining or contemplating something on the table in front of him. His facial expression appears somewhat neutral.</vis_desc>
<aud_desc>In the audio, the character's tone is neutral and the background music adds a relaxed atmosphere. In the text, the subtitle reads: "I sent you a Titan bat, and it should be here this week, so this is for you." This sentence may be a response or reply from the male character to someone.</aud_desc>
<think>This sentence may be the male character confirming the delivery of the Titan bat to the person he is communicating with.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=308): <vis_desc>In the video, in the opening scene, we see a person sitting in a relatively calm indoor environment. His gaze is directly facing the other person, and his facial expression is relatively calm. As time passes, the person lifts their head to explain something to the other person. The corners of their mouth turned up a few degrees, showing a smile. In the following scene, the person puts down the object they were explaining and raises their head again, continuing to smile at the other person. In the middle of the video, the other person gestures with their hands, seemingly emphasizing their words or explaining something carefully. Overall, the person's facial expression maintains a positive emotional state throughout the video.</vis_desc>
<aud_desc>In the audio, the character's voice is relaxed and sounds comfortable. In the text, the subtitle reads: "I said I'm not aTitanium Park, it should be here this week, so this is my gift." This sentence may be the person's response or explanation to the other person.</aud_desc>
<think>Based on the positive emotion exhibited by the person in the video and the action of pointing and smiling, as well as the relaxed tone and calm expression described in the audio, we can infer that this sentence may be a confident and positive statement. The person's smile and calm posture also suggest that they are satisfied with their answer. Therefore, this sentence may be the person's confirmation and satisfaction with a certain idea or gift.</think>
<answer>happy</answer>
[2025-11-07 10:32:56,302] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 581/2088 [8:23:23<21:20:13, 50.97s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.941572535220976, 'learning_rate': 7.217432950191571e-07, 'completion_length': 227.0546875, 'rewards/av_format_reward': 0.59375, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.486855149269104, 'reward': 1.643105149269104, 'reward_std': 0.5817876160144806, 'kl': 0.03216552734375, 'epoch': 0.56}
 28%|██▊       | 581/2088 [8:23:23<21:20:13, 50.97s/it][2025-11-07 10:33:47,228] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 582/2088 [8:24:14<21:19:01, 50.96s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.075339810898677, 'learning_rate': 7.212643678160919e-07, 'completion_length': 226.625, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.43359375, 'rewards/think_step_av_with_neutral_reward': 0.40409860014915466, 'reward': 1.6267548203468323, 'reward_std': 0.4469505101442337, 'kl': 0.0302734375, 'epoch': 0.56}
 28%|██▊       | 582/2088 [8:24:14<21:19:01, 50.96s/it][2025-11-07 10:34:42,860] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 583/2088 [8:25:10<21:53:21, 52.36s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.14565122303135, 'learning_rate': 7.207854406130268e-07, 'completion_length': 232.29296875, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.671875, 'rewards/think_step_av_with_neutral_reward': 0.5901732444763184, 'reward': 1.9690794348716736, 'reward_std': 0.5120952725410461, 'kl': 0.0321044921875, 'epoch': 0.56}
 28%|██▊       | 583/2088 [8:25:10<21:53:21, 52.36s/it][2025-11-07 10:35:32,490] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 584/2088 [8:26:00<21:31:57, 51.54s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0143689626120405, 'learning_rate': 7.203065134099616e-07, 'completion_length': 224.89453125, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.66015625, 'rewards/think_step_av_with_neutral_reward': 0.5649491548538208, 'reward': 2.0532304644584656, 'reward_std': 0.4762169122695923, 'kl': 0.032958984375, 'epoch': 0.56}
 28%|██▊       | 584/2088 [8:26:00<21:31:57, 51.54s/it][2025-11-07 10:36:22,582] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 585/2088 [8:26:50<21:20:12, 51.11s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9294310836620285, 'learning_rate': 7.198275862068966e-07, 'completion_length': 225.00390625, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.50390625, 'rewards/think_step_av_with_neutral_reward': 0.5110119134187698, 'reward': 1.7180431485176086, 'reward_std': 0.39845985174179077, 'kl': 0.03216552734375, 'epoch': 0.56}
 28%|██▊       | 585/2088 [8:26:50<21:20:12, 51.11s/it][2025-11-07 10:37:14,376] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 586/2088 [8:27:41<21:24:31, 51.31s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0948886485496336, 'learning_rate': 7.193486590038314e-07, 'completion_length': 226.92578125, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.48828125, 'rewards/think_step_av_with_neutral_reward': 0.45020006597042084, 'reward': 1.676762580871582, 'reward_std': 0.5267057418823242, 'kl': 0.03118896484375, 'epoch': 0.56}
 28%|██▊       | 586/2088 [8:27:41<21:24:31, 51.31s/it][2025-11-07 10:38:02,164] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 587/2088 [8:28:29<20:57:12, 50.26s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.022938356409015, 'learning_rate': 7.188697318007662e-07, 'completion_length': 221.484375, 'rewards/av_format_reward': 0.66015625, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.4170825183391571, 'reward': 1.4678637385368347, 'reward_std': 0.5496109127998352, 'kl': 0.03076171875, 'epoch': 0.56}
 28%|██▊       | 587/2088 [8:28:29<20:57:12, 50.26s/it][2025-11-07 10:38:51,608] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 588/2088 [8:29:19<20:50:17, 50.01s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.053883562931255, 'learning_rate': 7.18390804597701e-07, 'completion_length': 227.390625, 'rewards/av_format_reward': 0.76171875, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.5369126498699188, 'reward': 1.845506489276886, 'reward_std': 0.5015740394592285, 'kl': 0.03179931640625, 'epoch': 0.56}
 28%|██▊       | 588/2088 [8:29:19<20:50:17, 50.01s/it][2025-11-07 10:39:44,514] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 589/2088 [8:30:12<21:11:09, 50.88s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9117537923215902, 'learning_rate': 7.17911877394636e-07, 'completion_length': 230.078125, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5037184059619904, 'reward': 1.8748121857643127, 'reward_std': 0.3591061979532242, 'kl': 0.03253173828125, 'epoch': 0.56}
 28%|██▊       | 589/2088 [8:30:12<21:11:09, 50.88s/it][2025-11-07 10:40:39,244] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 590/2088 [8:31:06<21:39:08, 52.03s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.946931946115618, 'learning_rate': 7.174329501915708e-07, 'completion_length': 227.46875, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.640625, 'rewards/think_step_av_with_neutral_reward': 0.5772270262241364, 'reward': 1.9600394368171692, 'reward_std': 0.41089417040348053, 'kl': 0.02984619140625, 'epoch': 0.57}
 28%|██▊       | 590/2088 [8:31:06<21:39:08, 52.03s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03027.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/7918.mp4
[DEBUG] Step 590
[DEBUG] prompt_completion_ids shape: torch.Size([32, 273])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a man sitting in an office environment. His facial expression is smiling, his eyes slightly squinted, and the corners of his mouth turned up. This indicates that he may be feeling satisfied or relaxed.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle says "And now..." This sentence may be the man's response to a previous statement or opinion.</aud_desc>
<think>Based on the man's smiling facial expression and relaxed demeanor in the video clues, as well as the description in the subtitle, we can infer that this sentence may carry a sense of satisfaction or joy. The man may be satisfied with what he has done or is about to do, hence expressing such a positive sentence.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=273): <vis_desc>In the video, we see a man in a formal setting, possibly an important meeting or conference. His facial expression appears joyful and satisfied, with his mouth upturned in a smile, indicating a friendly and positive emotional state. This suggests that he may have heard some good news or is working on a interesting project, which aligns with the relaxed and happy atmosphere indicated by the man's smile in the video.</vis_desc>
<aud_desc>In the audio, the tone is calm. Combined with the text content, it seems that the character is seeking help. In the text, the subtitle reads: "I hope nothing happens, he just got promoted, man." This sentence indicates the character's expectation and surprise about receiving a promotion.</aud_desc>
<think>Based on the man's joyful and satisfied facial expression and smile in the video clues, as well as the description of a calm tone in the audio clues, we can infer that this sentence is a positive statement about the man's situation. The man's expression and tone both indicate a sense of optimism and satisfaction, suggesting that he trusts the positive outcome and feels happy and friendly about it. Therefore, this sentence aligns with the overall positive emotions indicated by the man's smile in the video clues, giving a positive and friendly emotional tone.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1705.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/6660.mp4
[DEBUG] Step 590
[DEBUG] prompt_completion_ids shape: torch.Size([32, 302])
[DEBUG] Shortest output (len=173): <vis_desc>In the video, we see a woman wearing a red traditional Chinese dress, facing another person on a dirt path. She is holding a phone and appears to be talking to the other person.</vis_desc>
<aud_desc>In the audio, the character's tone is enthusiastic and cheerful, with a positive and optimistic attitude. In the text, the subtitle says "Happy birthday, lady.", which is likely the Chinese greeting the woman shoulds receive.</aud_desc>
<think>Based on the facial expression of the woman in the video, which shows a smile and a slight upward curve at the corners of her mouth, as well as the cheerful tone and positive attitude of the character in the audio, we can infer that this greeting is very positive and the woman is very happy and pleased to receive it.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=302): <aud_desc>In the audio, there is an alarm sound followed by the character's scream " terror", indicating that the person is experiencing a panic attack. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, we see a woman in a dim environment, holding a knife in her hand. At the beginning of the video, her mouth is wide open, eyes wide open, and she is running forward. This may indicate that she is feeling nervous, scared, or anxious. During the process of running, her mouth opens wide, eyes widen, and she seems to be shouting or cheering, possibly hoping to feel calm or the person next to her will help them.</vis_desc>
<think>The combined visual and auditory cues present a clear and intense picture of a person experiencing a panic attack. Visually, the woman's wide-open mouth, eyes, and running posture strongly suggest an emotional shock and heightened state of alertness common to someone facing a perceived threat or danger. Auditory signals reinforce this impression—the urgent, distressing scream "trauma" heard confirms a physical arousal that aligns with panic attacks. Even though the subtitle text does not clarify her emotions, the combined visuals and auditory evidence together paint a vivid picture of terror and psychological turmoil underpinned by a panic attack scenario. This integration of modalities logically supports the prediction that the target emotion is panic).</think>
<answer>anxiety</answer>
[2025-11-07 10:41:33,452] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 591/2088 [8:32:01<21:54:32, 52.69s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0448234676128214, 'learning_rate': 7.169540229885057e-07, 'completion_length': 228.6640625, 'rewards/av_format_reward': 0.87109375, 'rewards/accuracy_reward': 0.73046875, 'rewards/think_step_av_with_neutral_reward': 0.5711415410041809, 'reward': 2.172703981399536, 'reward_std': 0.319466307759285, 'kl': 0.031982421875, 'epoch': 0.57}
 28%|██▊       | 591/2088 [8:32:01<21:54:32, 52.69s/it][2025-11-07 10:42:21,721] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 592/2088 [8:32:49<21:20:37, 51.36s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0352118076603514, 'learning_rate': 7.164750957854406e-07, 'completion_length': 222.76171875, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4824429303407669, 'reward': 1.8457242250442505, 'reward_std': 0.44690366089344025, 'kl': 0.029541015625, 'epoch': 0.57}
 28%|██▊       | 592/2088 [8:32:49<21:20:37, 51.36s/it][2025-11-07 10:43:15,171] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 593/2088 [8:33:42<21:35:22, 51.99s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9569817350401137, 'learning_rate': 7.159961685823755e-07, 'completion_length': 230.984375, 'rewards/av_format_reward': 0.83203125, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.4557810127735138, 'reward': 1.819062352180481, 'reward_std': 0.44206927716732025, 'kl': 0.0322265625, 'epoch': 0.57}
 28%|██▊       | 593/2088 [8:33:42<21:35:22, 51.99s/it][2025-11-07 10:44:01,662] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 594/2088 [8:34:29<20:53:26, 50.34s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.1032658501756187, 'learning_rate': 7.155172413793104e-07, 'completion_length': 223.16015625, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.6015625, 'rewards/think_step_av_with_neutral_reward': 0.549450695514679, 'reward': 1.803356945514679, 'reward_std': 0.5486659407615662, 'kl': 0.03118896484375, 'epoch': 0.57}
 28%|██▊       | 594/2088 [8:34:29<20:53:26, 50.34s/it][2025-11-07 10:44:54,023] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 28%|██▊       | 595/2088 [8:35:21<21:07:41, 50.95s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.069320791543612, 'learning_rate': 7.150383141762451e-07, 'completion_length': 224.046875, 'rewards/av_format_reward': 0.6171875, 'rewards/accuracy_reward': 0.71484375, 'rewards/think_step_av_with_neutral_reward': 0.6237532794475555, 'reward': 1.955784559249878, 'reward_std': 0.5053670704364777, 'kl': 0.032470703125, 'epoch': 0.57}
 28%|██▊       | 595/2088 [8:35:21<21:07:41, 50.95s/it][2025-11-07 10:45:46,236] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 596/2088 [8:36:13<21:16:18, 51.33s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.08239560957533, 'learning_rate': 7.1455938697318e-07, 'completion_length': 231.58203125, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.533982664346695, 'reward': 1.9402326941490173, 'reward_std': 0.46777477860450745, 'kl': 0.03057861328125, 'epoch': 0.57}
 29%|██▊       | 596/2088 [8:36:13<21:16:18, 51.33s/it][2025-11-07 10:46:36,006] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 597/2088 [8:37:03<21:03:51, 50.86s/it]                                                       {'loss': 0.0011, 'grad_norm': 1.956351834259903, 'learning_rate': 7.140804597701149e-07, 'completion_length': 233.328125, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.65234375, 'rewards/think_step_av_with_neutral_reward': 0.5836630463600159, 'reward': 1.9899131059646606, 'reward_std': 0.4165342003107071, 'kl': 0.02862548828125, 'epoch': 0.57}
 29%|██▊       | 597/2088 [8:37:03<21:03:51, 50.86s/it][h264 @ 0xb8e60b40] error while decoding MB 72 39, bytestream -7
[2025-11-07 10:47:22,336] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 598/2088 [8:37:49<20:29:15, 49.50s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.1265829161875014, 'learning_rate': 7.136015325670498e-07, 'completion_length': 224.375, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.75390625, 'rewards/think_step_av_with_neutral_reward': 0.6119716167449951, 'reward': 2.2447842359542847, 'reward_std': 0.4322374761104584, 'kl': 0.0345458984375, 'epoch': 0.57}
 29%|██▊       | 598/2088 [8:37:49<20:29:15, 49.50s/it][2025-11-07 10:48:12,872] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 599/2088 [8:38:40<20:36:08, 49.81s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0277788286736724, 'learning_rate': 7.131226053639846e-07, 'completion_length': 235.30078125, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.68359375, 'rewards/think_step_av_with_neutral_reward': 0.604805052280426, 'reward': 1.999336302280426, 'reward_std': 0.3295467793941498, 'kl': 0.032470703125, 'epoch': 0.57}
 29%|██▊       | 599/2088 [8:38:40<20:36:08, 49.81s/it][2025-11-07 10:49:04,294] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▊       | 600/2088 [8:39:31<20:47:18, 50.29s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9059481875447764, 'learning_rate': 7.126436781609195e-07, 'completion_length': 230.16015625, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.64453125, 'rewards/think_step_av_with_neutral_reward': 0.5637738704681396, 'reward': 1.9231489300727844, 'reward_std': 0.43583303689956665, 'kl': 0.032470703125, 'epoch': 0.57}
 29%|██▊       | 600/2088 [8:39:31<20:47:18, 50.29s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/07879.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/11081.mp4
[DEBUG] Step 600
[DEBUG] prompt_completion_ids shape: torch.Size([32, 286])
[DEBUG] Shortest output (len=172): <vis_desc>In the video, we see a man with a furrowed brow and slightly downturned mouth, his facial expression appearing serious and serious.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. The character's emotional state appears angry. In the text, the subtitle reads: "She's putting up silence, waiting for a chance to beat him." This sentence may be the man commenting on the other person's silence and threatening them.</aud_desc>
<think>Based on the serious and serious facial expressions of the man in the video clues, as well as the stern and aggressive tone in the audio clues, we can infer that this sentence carries a sense of anger and dissatisfaction. The man may be expressing his discontent and threat towards the other person's silence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=286): <aud_desc>In the audio, the tone is serious. Combined with the text content, it seems that the character is warning another person not to engage in meaningless actions. In the text, the subtitle reads: "Let the guilty remain guilty, and the innocent be judged by others, okay?" This sentence may be a warning or suggestion from the character on the right to the character on the left.</aud_desc>
<vis_desc>In the video, we see a man on the left and a man on the right. The man on the left has his back to the camera, while the man on the right is facing the camera. We mainly analyze the emotional state of the man on the right. He has a furrowed brow and a slightly downturned mouth, indicating that his expression is serious. He is seemingly having a conversation with the man on the left. Overall, he seems to be going through some serious discussions or negotiations, and his emotions appear neutral.</vis_desc>
<think>Based on the serious expression and downward mouth of the man on the right in the video clues, as well as the warning tone described in the audio clues, we can infer that this sentence carries a serious and warning tone. The man on the right may be emphasizing the importance of avoiding unnecessary actions or making decisions based on a guilty or innocent distinction, expressing a serious and concerned emotion.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8033.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5554.mp4
[DEBUG] Step 600
[DEBUG] prompt_completion_ids shape: torch.Size([32, 327])
[DEBUG] Shortest output (len=174): <vis_desc>In the video, we see a female character with a furrowed brow and slightly downturned mouth, expressing extreme fear and distress.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle reads: "Attention, people! The tiger is attacking!" This sentence may be a warning or call to action from the female character to others.</aud_desc>
<think>Based on the visual clue of the female character's furrowed brow and downturned mouth, as well as the audio clue of the inability to determine the character's emotional state, we can infer that this sentence may carry a sense of anxiety or danger. The female character may be communicating with others or preparing for a risky situation, thus expressing her extreme fear.</think>
<answer>fear</answer>
[DEBUG] Longest output (len=327): <vis_desc>In the video, the screen shows a lady wearing traditional Chinese attire with a blue scarf and a floral accessory in her hair. In the video, she appears to be smiling with a slight upward curve at the corners of her mouth, giving a relaxed and pleasant feeling. Her eyes are looking directly at the camera, which may indicate that she is interacting with someone or waiting for a response. The overall atmosphere is positive and friendly. Due to the low resolution and blurry images, it is difficult to definitively determine her emotional state.</vis_desc>
<aud_desc>In the audio, there is an emphasis on the character's "bow," suggesting that she is performing a form of dance or music. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly suggest that the lady is experiencing happiness. Visually, her smiling face with a slight upward curve at the corners of her mouth, direct eye contact, and direct gaze conveys a genuine sense of interaction and relaxation, all typical of a pleasant social interaction. The calm and positive atmosphere created by this attire further supports the impression of happiness. Audibly, the emphasis on the "bow" further suggests that she is engaged in a form of performance—possibly a gesture of self-mockfulness or formality—that aligns with feelings of happiness. Together, the visual indicators of her joyful smile and attentive expression, coupled with the vocal cue of the "bow," create a compelling multimodal picture of someone who is genuinely content and happy.</think>
<answer>happy</answer>
[2025-11-07 10:50:09,639] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 601/2088 [8:40:37<22:38:22, 54.81s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.011906671599158, 'learning_rate': 7.121647509578544e-07, 'completion_length': 230.87109375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.5859375, 'rewards/think_step_av_with_neutral_reward': 0.5158050656318665, 'reward': 1.8165863752365112, 'reward_std': 0.5097078382968903, 'kl': 0.0303955078125, 'epoch': 0.58}
 29%|██▉       | 601/2088 [8:40:37<22:38:22, 54.81s/it][2025-11-07 10:51:03,518] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 602/2088 [8:41:31<22:30:32, 54.53s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.004655086690875, 'learning_rate': 7.116858237547893e-07, 'completion_length': 230.53515625, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4443894773721695, 'reward': 1.706108272075653, 'reward_std': 0.4280441403388977, 'kl': 0.03369140625, 'epoch': 0.58}
 29%|██▉       | 602/2088 [8:41:31<22:30:32, 54.53s/it][2025-11-07 10:51:53,794] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 603/2088 [8:42:21<21:58:02, 53.25s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9718108473546858, 'learning_rate': 7.11206896551724e-07, 'completion_length': 229.98046875, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.48565244674682617, 'reward': 1.9036211967468262, 'reward_std': 0.5296502560377121, 'kl': 0.0325927734375, 'epoch': 0.58}
 29%|██▉       | 603/2088 [8:42:21<21:58:02, 53.25s/it][2025-11-07 10:52:48,253] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 604/2088 [8:43:15<22:06:05, 53.62s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9664387929387954, 'learning_rate': 7.107279693486589e-07, 'completion_length': 232.765625, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.505588710308075, 'reward': 1.739963710308075, 'reward_std': 0.5144892930984497, 'kl': 0.033203125, 'epoch': 0.58}
 29%|██▉       | 604/2088 [8:43:15<22:06:05, 53.62s/it][2025-11-07 10:53:38,876] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 605/2088 [8:44:06<21:43:00, 52.72s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0888050749012894, 'learning_rate': 7.102490421455939e-07, 'completion_length': 232.75390625, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5016580522060394, 'reward': 1.7790018320083618, 'reward_std': 0.5322982668876648, 'kl': 0.0333251953125, 'epoch': 0.58}
 29%|██▉       | 605/2088 [8:44:06<21:43:00, 52.72s/it][2025-11-07 10:54:29,934] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 606/2088 [8:44:57<21:29:49, 52.22s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0314732285488906, 'learning_rate': 7.097701149425287e-07, 'completion_length': 227.984375, 'rewards/av_format_reward': 0.55859375, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.6104076504707336, 'reward': 1.8057201504707336, 'reward_std': 0.5328488945960999, 'kl': 0.0301513671875, 'epoch': 0.58}
 29%|██▉       | 606/2088 [8:44:57<21:29:49, 52.22s/it][2025-11-07 10:55:23,454] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 607/2088 [8:45:51<21:38:35, 52.61s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.082395193989224, 'learning_rate': 7.092911877394636e-07, 'completion_length': 235.3046875, 'rewards/av_format_reward': 0.8046875, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5204032957553864, 'reward': 1.961809515953064, 'reward_std': 0.4651525020599365, 'kl': 0.03216552734375, 'epoch': 0.58}
 29%|██▉       | 607/2088 [8:45:51<21:38:35, 52.61s/it][2025-11-07 10:56:16,163] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 608/2088 [8:46:43<21:38:26, 52.64s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0613827906861495, 'learning_rate': 7.088122605363984e-07, 'completion_length': 224.72265625, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5045665800571442, 'reward': 1.7740978598594666, 'reward_std': 0.5085395872592926, 'kl': 0.031982421875, 'epoch': 0.58}
 29%|██▉       | 608/2088 [8:46:43<21:38:26, 52.64s/it][2025-11-07 10:57:08,835] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 609/2088 [8:47:36<21:37:48, 52.65s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.7752498426906689, 'learning_rate': 7.083333333333334e-07, 'completion_length': 232.42578125, 'rewards/av_format_reward': 0.8515625, 'rewards/accuracy_reward': 0.42578125, 'rewards/think_step_av_with_neutral_reward': 0.397987961769104, 'reward': 1.675331711769104, 'reward_std': 0.4299967288970947, 'kl': 0.03118896484375, 'epoch': 0.58}
 29%|██▉       | 609/2088 [8:47:36<21:37:48, 52.65s/it][2025-11-07 10:57:56,912] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 610/2088 [8:48:24<21:03:07, 51.28s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9655107871233621, 'learning_rate': 7.078544061302682e-07, 'completion_length': 225.6796875, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.41142579913139343, 'reward': 1.5637694597244263, 'reward_std': 0.5482501983642578, 'kl': 0.03082275390625, 'epoch': 0.58}
 29%|██▉       | 610/2088 [8:48:24<21:03:07, 51.28s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2727.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5453.mp4
[DEBUG] Step 610
[DEBUG] prompt_completion_ids shape: torch.Size([32, 317])
[DEBUG] Shortest output (len=174): <aud_desc>In the audio, the tone and intonation of the character's voice are relatively steady. In the text, the subtitle reads: "Deputy commander, the time has changed." This sentence may be the military captain's statement or response to someone.</aud_desc>
<vis_desc>In the video, we see a military captain standing in a corridor. His facial expression appears slightly fierce, suggesting a serious conversation or debate is about to occur.</vis_desc>
<think>Based on the video clue of the military captain's slightly fierce facial expression and the audio clue of the character's steady tone and intonation, we can infer that this sentence may carry a serious or solemn tone. Therefore, this sentence may be the military captain emphasizing the importance of a certain situation or request, rather than a mere greeting.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=317): <vis_desc>In the video, we see a male character walking in a corridor with an arched door. The walls are simple, the floor is light-colored, and there is a picture frame on the wall. At the beginning of the video, the character's eyes are wide open and their focus is sharp, indicating that they are attentively observing something. In the following scenes, the character slightly turns his head to the right side, giving a glimpse of a younger male's face. Although the specific emotions of the younger male need to be analyzed with additional context, it can be inferred that he is likely in a relatively calm state. Overall, the character is in a relatively stable emotional state.</vis_desc>
<aud_desc>In the audio, the character's tone and intonation are relatively calm. In the text, the subtitle reads: "Director, the time has changed." This sentence may be the male character's response to the information on the phone.</aud_desc>
<think>Based on the video clues of the male character's wide-open eyes and focused gaze, as well as the analysis of the character's calm tone and intonation in the audio clues, we can infer that this sentence may be the male character's inquiry about the current time or the response to some information. Since the subtitle content does not carry emotional clues, we can only speculate based on the clues in the audio and video. The character's calm tone and intonation may indicate that he is collecting information or asking about the other person's stance.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04607.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02627.mp4
[DEBUG] Step 610
[DEBUG] prompt_completion_ids shape: torch.Size([32, 349])
[DEBUG] Shortest output (len=187): <vis_desc>In the video, we see a man wearing a black hat and formal attire, seemingly engaged in a serious conversation with another person.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, it is impossible to judge the emotional state based on the subtitle content.</aud_desc>
<think>Although the auditory cues and subtitle content do not provide clear emotional information, the visual signals offer valuable insight into the man's emotional state. His serious facial expression and buttoned-up formal attire suggest a formal or academic setting. Considering these visual elements, where possible, reinforces the idea of a serious or solemn encounter, it is reasonable to conclude that the man's emotional state is indeed serious. The neutral auditory cues do not contradict this interpretation and instead imply that the target emotion may be more neutral or non-intrusive.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=349): <vis_desc>In the video, the scene shows a woman indoors holding a pair of glasses up for a closer look. At the beginning of the video, she raises her hand, making a gesture that is difficult to determine the emotional state of the character. At the same time, there is a raised eyebrow, indicating that she may have an evaluation or reaction related to the quality of the glasses. At the end of the video, she smiles, opens her mouth, and widens her eyes, showing a more excited and confident facial expression. Taking these clues together, it can be inferred that the woman in this video may feel happy, excited, or confident when displaying the pair of glasses.</vis_desc>
<aud_desc>In the audio, the tone is very excited. Combined with the text content, it feels like a threat to the other person's character. In the text, the subtitle says, "And the male主人公 will get an upraised salary." This sentence may be a threat or insult to the other person.</aud_desc>
<think>Based on the woman's facial expressions and gestures in the video clues, she initially shows a hesitant and serious facial expression, then a smiling and excited expression, indicating a positive emotional state when expressing the pair of glasses. At the same time, based on the description of the woman's excited and threatening tone in the audio clues, it can be inferred that this sentence carries a sense of anger or dissatisfaction when said. Taking these clues together, it can be deduced that the woman in this video wants to threaten or insult the other person when displaying the pair of glasses, which aligns with her overall display of an excited and confident emotional state.</think>
<answer>happy</answer>
[2025-11-07 10:58:48,376] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 611/2088 [8:49:15<21:03:39, 51.33s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9371320247999837, 'learning_rate': 7.073754789272031e-07, 'completion_length': 241.8125, 'rewards/av_format_reward': 0.859375, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.5006214380264282, 'reward': 1.938121497631073, 'reward_std': 0.44308041036129, 'kl': 0.030517578125, 'epoch': 0.59}
 29%|██▉       | 611/2088 [8:49:15<21:03:39, 51.33s/it][mpeg4 @ 0xc26f0cc0] P cbpy damaged at 36 22
[mpeg4 @ 0xc26f0cc0] Error at MB: 1818
[2025-11-07 10:59:40,086] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 612/2088 [8:50:07<21:05:34, 51.45s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.04068686346255, 'learning_rate': 7.068965517241378e-07, 'completion_length': 223.41796875, 'rewards/av_format_reward': 0.921875, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.46713945269584656, 'reward': 1.9046393632888794, 'reward_std': 0.4654020071029663, 'kl': 0.0313720703125, 'epoch': 0.59}
 29%|██▉       | 612/2088 [8:50:07<21:05:34, 51.45s/it][2025-11-07 11:00:28,068] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 613/2088 [8:50:55<20:39:10, 50.41s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.962868346486221, 'learning_rate': 7.064176245210728e-07, 'completion_length': 228.5859375, 'rewards/av_format_reward': 0.61328125, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.5274806916713715, 'reward': 1.679824411869049, 'reward_std': 0.43217866122722626, 'kl': 0.0311279296875, 'epoch': 0.59}
 29%|██▉       | 613/2088 [8:50:55<20:39:10, 50.41s/it][2025-11-07 11:01:19,183] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 614/2088 [8:51:46<20:43:33, 50.62s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.025909695713151, 'learning_rate': 7.059386973180076e-07, 'completion_length': 229.66796875, 'rewards/av_format_reward': 0.8359375, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.48032262921333313, 'reward': 1.9295414090156555, 'reward_std': 0.477390319108963, 'kl': 0.02984619140625, 'epoch': 0.59}
 29%|██▉       | 614/2088 [8:51:46<20:43:33, 50.62s/it][2025-11-07 11:02:11,565] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 29%|██▉       | 615/2088 [8:52:39<20:55:41, 51.15s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.993132987473261, 'learning_rate': 7.054597701149425e-07, 'completion_length': 233.5859375, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.4722023755311966, 'reward': 1.7339211106300354, 'reward_std': 0.5554073750972748, 'kl': 0.032958984375, 'epoch': 0.59}
 29%|██▉       | 615/2088 [8:52:39<20:55:41, 51.15s/it][mpeg4 @ 0x11621ef00] Error at MB: 3190
[2025-11-07 11:03:01,936] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 616/2088 [8:53:29<20:49:07, 50.92s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.037789007921985, 'learning_rate': 7.049808429118773e-07, 'completion_length': 228.6484375, 'rewards/av_format_reward': 0.74609375, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5535931289196014, 'reward': 1.932499349117279, 'reward_std': 0.5947896838188171, 'kl': 0.031982421875, 'epoch': 0.59}
 30%|██▉       | 616/2088 [8:53:29<20:49:07, 50.92s/it][2025-11-07 11:03:52,229] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 617/2088 [8:54:19<20:43:41, 50.73s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0082502051377484, 'learning_rate': 7.045019157088123e-07, 'completion_length': 220.859375, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.453125, 'rewards/think_step_av_with_neutral_reward': 0.41987496614456177, 'reward': 1.5839374661445618, 'reward_std': 0.5262865424156189, 'kl': 0.0325927734375, 'epoch': 0.59}
 30%|██▉       | 617/2088 [8:54:19<20:43:41, 50.73s/it][2025-11-07 11:04:42,507] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 618/2088 [8:55:10<20:39:32, 50.59s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.09990748919445, 'learning_rate': 7.040229885057471e-07, 'completion_length': 229.9140625, 'rewards/av_format_reward': 0.890625, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.5004289895296097, 'reward': 1.9066790342330933, 'reward_std': 0.43536892533302307, 'kl': 0.0301513671875, 'epoch': 0.59}
 30%|██▉       | 618/2088 [8:55:10<20:39:32, 50.59s/it][2025-11-07 11:05:37,413] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 619/2088 [8:56:04<21:10:22, 51.89s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.072922002217995, 'learning_rate': 7.03544061302682e-07, 'completion_length': 232.55078125, 'rewards/av_format_reward': 0.7421875, 'rewards/accuracy_reward': 0.69921875, 'rewards/think_step_av_with_neutral_reward': 0.6059370040893555, 'reward': 2.0473432540893555, 'reward_std': 0.4474746584892273, 'kl': 0.0335693359375, 'epoch': 0.59}
 30%|██▉       | 619/2088 [8:56:05<21:10:22, 51.89s/it][2025-11-07 11:06:27,264] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 620/2088 [8:56:54<20:54:33, 51.28s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.855074085605221, 'learning_rate': 7.030651340996168e-07, 'completion_length': 227.26953125, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.703125, 'rewards/think_step_av_with_neutral_reward': 0.5530119836330414, 'reward': 2.0842618346214294, 'reward_std': 0.4834052324295044, 'kl': 0.0301513671875, 'epoch': 0.59}
 30%|██▉       | 620/2088 [8:56:54<20:54:33, 51.28s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/3152.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00942.mp4
[DEBUG] Step 620
[DEBUG] prompt_completion_ids shape: torch.Size([32, 282])
[DEBUG] Shortest output (len=153): <aud_desc>In the audio, the character's voice sounds surprised. In the text, the subtitle says, "Ah, Ah, Ah, okay, okay, okay." This sentence may be the man's response to a stimulus or question.</aud_desc>
<vis_desc>In the video, we see a man with wide-open eyes and open mouth, seemingly expressing surprise or excitement.</vis_desc>
<think>Based on the visual clue of the man's wide-open eyes and open mouth, as well as the audio clue of the character's surprised tone, we can infer that this sentence may be the man's reaction to unexpected news or a sudden situation. The man may be expressing his surprise or enjoyment.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=282): <vis_desc>In the video, the screen shows a male character sitting on a sofa. He is holding a phone in his hand and seems to be talking on the phone. In the video, his head is slightly tilted and the corners of his mouth are slightly downturned. His facial expression appears to be neutral.</vis_desc>
<aud_desc>At the same time, there is music playing in the background. The character's voice is calm and conveys a sense of professionalism. In the audio, there is a voiceover with a stern and angry tone, expressing the character's anger. Overall, the character's emotions are quite intense.</aud_desc>
<think>The combined visual and auditory cues suggest that the male character is experiencing anger rather than happiness. Visually, his slight tilt of the head and downturned mouth corners indicates a neutral or mildly dissatisfied facial expression, while his casually tilted neck and relaxed posture indicate a calm, non-emotive state. Audibly, the voiceover with a stern and angry tone further supports this interpretation, as such vocal qualities typically correlate with anger. Although the ambient music and background audio levels do not clarify his feelings, the combined visual expression and vocal tone strongly imply that he is feeling angry. Thus, these multimodal signals cohesively point to the character expressing angry emotions through his behavior and vocal delivery.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/01545.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03367.mp4
[DEBUG] Step 620
[DEBUG] prompt_completion_ids shape: torch.Size([32, 278])
[DEBUG] Shortest output (len=166): <vis_desc>In the video, we see a male character with a neutral facial expression, looking directly at the other person, possibly engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle says, "Which one did you choose?" This sentence may be a questioning or complaint statement from the male character towards another person.</aud_desc>
<think>Based on the video clue of the male character's neutral facial expression and direct eye contact, as well as the audio clue without valuable emotional clues, we can infer that this sentence may carry a serious or inquiry-seeking tone. Therefore, this sentence may be the male character asking or questioning the other person, possibly about a decision or something important.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=278): <aud_desc>In the audio, the voice is soft, with a happy tone and a sense of wishing the other person luck. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, the screen shows a woman in what appears to be an indoor setting. In the video, her facial expression appears happy, with a slight upward curve at the corners of her mouth and a relaxed smile. Her eyes appear to be shining, indicating good health and a positive emotional state. Overall, she may be experiencing a pleasant moment in her day.</vis_desc>
<think>The visual cues strongly suggest that the woman is experiencing happiness: her joyful smile and slightly upturned corners of her mouth indicate a happy but content facial expression, and her relaxed demeanor reflects a positive emotional state overall. The presence of soft vocal tone with a happy tone further supports the interpretation of a pleasant emotional experience. Additionally, the soft voice and wishing-for-ennie sentiment in the subtitle reinforces the notion that her mood is one of joy and encouragement. Even though the auditory cues and subtitle content do not explicitly reveal her emotional level, the strong visual signals together create a consistent and convincing picture of her happy state. Thus, the multimodal evidence collectively supports the inference that the woman is happy in this moment.</think>
<answer>happy</answer>
[2025-11-07 11:07:15,333] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 621/2088 [8:57:42<20:30:10, 50.31s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.1428868206765532, 'learning_rate': 7.025862068965517e-07, 'completion_length': 224.27734375, 'rewards/av_format_reward': 0.9296875, 'rewards/accuracy_reward': 0.56640625, 'rewards/think_step_av_with_neutral_reward': 0.46201255917549133, 'reward': 1.9581064581871033, 'reward_std': 0.46513745188713074, 'kl': 0.03466796875, 'epoch': 0.59}
 30%|██▉       | 621/2088 [8:57:42<20:30:10, 50.31s/it][2025-11-07 11:08:05,131] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 622/2088 [8:58:32<20:25:33, 50.16s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9341262859109254, 'learning_rate': 7.021072796934866e-07, 'completion_length': 222.2734375, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.31640625, 'rewards/think_step_av_with_neutral_reward': 0.3010697290301323, 'reward': 1.3245072960853577, 'reward_std': 0.5151781737804413, 'kl': 0.0325927734375, 'epoch': 0.6}
 30%|██▉       | 622/2088 [8:58:32<20:25:33, 50.16s/it][2025-11-07 11:08:59,493] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 623/2088 [8:59:27<20:55:30, 51.42s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0181078458414263, 'learning_rate': 7.016283524904214e-07, 'completion_length': 236.14453125, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.3682270050048828, 'reward': 1.5049458146095276, 'reward_std': 0.511501669883728, 'kl': 0.03173828125, 'epoch': 0.6}
 30%|██▉       | 623/2088 [8:59:27<20:55:30, 51.42s/it][2025-11-07 11:09:49,689] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 624/2088 [9:00:17<20:45:41, 51.05s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.938392351805854, 'learning_rate': 7.011494252873563e-07, 'completion_length': 229.40625, 'rewards/av_format_reward': 0.85546875, 'rewards/accuracy_reward': 0.78515625, 'rewards/think_step_av_with_neutral_reward': 0.6556856334209442, 'reward': 2.2963106632232666, 'reward_std': 0.5143458992242813, 'kl': 0.0318603515625, 'epoch': 0.6}
 30%|██▉       | 624/2088 [9:00:17<20:45:41, 51.05s/it][2025-11-07 11:10:38,136] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 625/2088 [9:01:05<20:25:46, 50.27s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.7840519135690442, 'learning_rate': 7.006704980842912e-07, 'completion_length': 232.078125, 'rewards/av_format_reward': 0.62109375, 'rewards/accuracy_reward': 0.4375, 'rewards/think_step_av_with_neutral_reward': 0.37611761689186096, 'reward': 1.4347113966941833, 'reward_std': 0.4594559669494629, 'kl': 0.03125, 'epoch': 0.6}
 30%|██▉       | 625/2088 [9:01:05<20:25:46, 50.27s/it][mpeg4 @ 0x1031a5500] I cbpc damaged at 33 42
[mpeg4 @ 0x1031a5500] Error at MB: 3435
[2025-11-07 11:11:27,818] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|██▉       | 626/2088 [9:01:55<20:20:37, 50.09s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.99601190706828, 'learning_rate': 7.001915708812261e-07, 'completion_length': 225.63671875, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.6875, 'rewards/think_step_av_with_neutral_reward': 0.5767113268375397, 'reward': 1.8892112970352173, 'reward_std': 0.515614777803421, 'kl': 0.0328369140625, 'epoch': 0.6}
 30%|██▉       | 626/2088 [9:01:55<20:20:37, 50.09s/it][2025-11-07 11:12:15,524] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 627/2088 [9:02:43<20:02:20, 49.38s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9959402342162365, 'learning_rate': 6.997126436781609e-07, 'completion_length': 223.16796875, 'rewards/av_format_reward': 0.6875, 'rewards/accuracy_reward': 0.4609375, 'rewards/think_step_av_with_neutral_reward': 0.4584697484970093, 'reward': 1.606907308101654, 'reward_std': 0.5769930779933929, 'kl': 0.03155517578125, 'epoch': 0.6}
 30%|███       | 627/2088 [9:02:43<20:02:20, 49.38s/it][2025-11-07 11:13:05,508] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 628/2088 [9:03:33<20:05:56, 49.56s/it]                                                       {'loss': 0.0014, 'grad_norm': 1.9463287961941818, 'learning_rate': 6.992337164750957e-07, 'completion_length': 221.4453125, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.76171875, 'rewards/think_step_av_with_neutral_reward': 0.6241163015365601, 'reward': 2.2022414207458496, 'reward_std': 0.4548153728246689, 'kl': 0.03387451171875, 'epoch': 0.6}
 30%|███       | 628/2088 [9:03:33<20:05:56, 49.56s/it][2025-11-07 11:13:54,953] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 629/2088 [9:04:22<20:04:17, 49.53s/it]                                                       {'loss': 0.0014, 'grad_norm': 1.987468130279936, 'learning_rate': 6.987547892720305e-07, 'completion_length': 217.37109375, 'rewards/av_format_reward': 0.69140625, 'rewards/accuracy_reward': 0.578125, 'rewards/think_step_av_with_neutral_reward': 0.5146522223949432, 'reward': 1.7841835021972656, 'reward_std': 0.4789162874221802, 'kl': 0.034912109375, 'epoch': 0.6}
 30%|███       | 629/2088 [9:04:22<20:04:17, 49.53s/it][2025-11-07 11:14:44,290] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 630/2088 [9:05:11<20:02:05, 49.47s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0088874832544112, 'learning_rate': 6.982758620689655e-07, 'completion_length': 219.4296875, 'rewards/av_format_reward': 0.609375, 'rewards/accuracy_reward': 0.5, 'rewards/think_step_av_with_neutral_reward': 0.4743822067975998, 'reward': 1.583757221698761, 'reward_std': 0.5363209843635559, 'kl': 0.03125, 'epoch': 0.6}
 30%|███       | 630/2088 [9:05:11<20:02:05, 49.47s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9926.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5241.mp4
[DEBUG] Step 630
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=160): <vis_desc>In the video, we see a male character with a slightly widened eyes and slightly open mouth, looking directly at someone or something outside of the frame, showing a surprised expression.</vis_desc>
<aud_desc>In the audio, it is impossible to determine the character's emotional state. In the text, the subtitle says, "What's up?" This sentence may be spoken by the male character when seeing or hearing something unexpected.</aud_desc>
<think>Based on the surprised expression shown by the male character in the video clues, as well as the description in the subtitle, we can infer that this sentence may carry a sense of surprise or uncertainty. The male character may feel surprised by what is happening, hence showing this sentence.</think>
<answer>surprise</answer>
[DEBUG] Longest output (len=295): <aud_desc>In the audio, there are obvious sighs during breathing, indicating feelings of sadness and despair. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<vis_desc>In the video, we see a male character standing in a dimly lit room. His facial expression is twisted and his eyes are wide open, giving a sense of shock and realization. He seems to be looking at something or someone in the room that is not in the frame, which could be a sudden noise, a sudden light on the wall, or some other unexpected situation that caught him off guard.</vis_desc>
<think>The male character’s visual cues present a clear and vivid picture of surprise and sadness. His wide-open eyes and twisted facial expression strongly suggest that something unexpected has happened, likely something unexpected and unsettling in the dimly lit setting. The fact that he is focused on something or someone outside the frame further indicates that this unexpected event has caught him off guard. Although the auditory information offers no direct emotional information through speech or tone, the noticeable change in his facial expression and body language alone conveys a strong emotional state of shock and realization. Together, these multimodal signals—primarily the visual appearance combined with the subtle, silent tone of the voice—jointly support the conclusion that the character is experiencing surprise and sadness as a result of an unexpected and unsettling event</think>
<answer>surprise</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/02470.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2036.mp4
[DEBUG] Step 630
[DEBUG] prompt_completion_ids shape: torch.Size([32, 295])
[DEBUG] Shortest output (len=184): <vis_desc>In the video, we see a woman using a cellphone in a dimly lit indoor environment.</vis_desc>
<aud_desc>In the audio, there seems to be some noise and the voice sounds busy. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the woman is experiencing anger. Visually, her frowning and serious expression suggest a mood of frustration or dissatisfaction towards the phone call. Audibly, the background noise of talking on the phone combined with a busy tone conveys an atmosphere of immediacy and heightened tension, typical of someone feeling angry or frustrated. Although the voice is unclear in determining emotional state, the interplay of her angry demeanor and the stressful environment supported by her visual appearance supports the inference that she is indeed angry.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=295): <vis_desc>In the video, the screen shows a woman talking on her phone in a dimly lit indoor environment. At the beginning of the video, her facial expression is relatively neutral. In the following scenes, she opens her eyes and turns her head to the left, seemingly paying attention to something outside the frame or hearing some unusual sound that caught her attention.</vis_desc>
<aud_desc>In the audio, the tone is calm and the character's emotions are relatively stable. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The woman's visual cues, characterized by a neutral facial expression initially followed by a turn to the left, suggest a transition from focused listening to a more engaged and attentive state, which typically indicates a heightened emotional reaction. Although her eyes remain open and she doesn't convey vocal excitement or sadness, the subtle shift in posture and focus implies an internal processing or reaction consistent with surprise or discovery. The dimly lit setting further emphasizes a moment of introspection or discovery rather than an emotionally charged occasion. Complementing this, the calm audio maintains a subdued and composed tone that doesn't signaling heightened emotion, neither through vocal sounds nor through intonation. Together, these multimodal signals coherently point to a moment of calm reflection or neutral environmental engagement, casting meaningful scrutiny on her emotional state without clear emotional indicators from verbal or textual cues.</think>
<answer>neutral</answer>
[2025-11-07 11:15:35,730] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 631/2088 [9:06:03<20:15:38, 50.06s/it]                                                       {'loss': 0.0015, 'grad_norm': 2.0660135751448307, 'learning_rate': 6.977969348659003e-07, 'completion_length': 228.8671875, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.5390625, 'rewards/think_step_av_with_neutral_reward': 0.5486903786659241, 'reward': 1.837752878665924, 'reward_std': 0.6217761933803558, 'kl': 0.036865234375, 'epoch': 0.6}
 30%|███       | 631/2088 [9:06:03<20:15:38, 50.06s/it][2025-11-07 11:16:24,001] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 632/2088 [9:06:51<20:01:46, 49.52s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9660671933148044, 'learning_rate': 6.973180076628352e-07, 'completion_length': 218.78515625, 'rewards/av_format_reward': 0.625, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.38306108117103577, 'reward': 1.4846235513687134, 'reward_std': 0.47768647968769073, 'kl': 0.031005859375, 'epoch': 0.61}
 30%|███       | 632/2088 [9:06:51<20:01:46, 49.52s/it][2025-11-07 11:17:15,536] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 633/2088 [9:07:43<20:15:34, 50.13s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.94195480262622, 'learning_rate': 6.968390804597702e-07, 'completion_length': 227.34765625, 'rewards/av_format_reward': 0.828125, 'rewards/accuracy_reward': 0.61328125, 'rewards/think_step_av_with_neutral_reward': 0.4958251267671585, 'reward': 1.937231421470642, 'reward_std': 0.56181800365448, 'kl': 0.0328369140625, 'epoch': 0.61}
 30%|███       | 633/2088 [9:07:43<20:15:34, 50.13s/it][2025-11-07 11:18:08,568] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 634/2088 [9:08:36<20:35:51, 51.00s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.914557823205539, 'learning_rate': 6.96360153256705e-07, 'completion_length': 222.61328125, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.4407491683959961, 'reward': 1.846999168395996, 'reward_std': 0.46407997608184814, 'kl': 0.0330810546875, 'epoch': 0.61}
 30%|███       | 634/2088 [9:08:36<20:35:51, 51.00s/it][2025-11-07 11:18:56,252] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 635/2088 [9:09:23<20:10:55, 50.00s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9197732129266458, 'learning_rate': 6.958812260536399e-07, 'completion_length': 221.57421875, 'rewards/av_format_reward': 0.65234375, 'rewards/accuracy_reward': 0.76171875, 'rewards/think_step_av_with_neutral_reward': 0.6387363970279694, 'reward': 2.0527989864349365, 'reward_std': 0.4934774935245514, 'kl': 0.0311279296875, 'epoch': 0.61}
 30%|███       | 635/2088 [9:09:23<20:10:55, 50.00s/it][2025-11-07 11:19:44,813] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 30%|███       | 636/2088 [9:10:12<19:59:37, 49.57s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9295771028552042, 'learning_rate': 6.954022988505746e-07, 'completion_length': 217.06640625, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4337201416492462, 'reward': 1.6798138618469238, 'reward_std': 0.47510650753974915, 'kl': 0.0328369140625, 'epoch': 0.61}
 30%|███       | 636/2088 [9:10:12<19:59:37, 49.57s/it][2025-11-07 11:20:38,345] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 637/2088 [9:11:05<20:27:31, 50.76s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9509500473107857, 'learning_rate': 6.949233716475095e-07, 'completion_length': 224.59375, 'rewards/av_format_reward': 0.71484375, 'rewards/accuracy_reward': 0.703125, 'rewards/think_step_av_with_neutral_reward': 0.6334916949272156, 'reward': 2.051460385322571, 'reward_std': 0.5814100652933121, 'kl': 0.033203125, 'epoch': 0.61}
 31%|███       | 637/2088 [9:11:05<20:27:31, 50.76s/it][2025-11-07 11:21:28,770] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 638/2088 [9:11:56<20:24:15, 50.66s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0562333802647617, 'learning_rate': 6.944444444444444e-07, 'completion_length': 227.8125, 'rewards/av_format_reward': 0.75, 'rewards/accuracy_reward': 0.6484375, 'rewards/think_step_av_with_neutral_reward': 0.5454643666744232, 'reward': 1.9439019560813904, 'reward_std': 0.49787791073322296, 'kl': 0.0325927734375, 'epoch': 0.61}
 31%|███       | 638/2088 [9:11:56<20:24:15, 50.66s/it][mpeg4 @ 0x10604a980] 1. marker bit missing in 3. esc
[mpeg4 @ 0x10604a980] Error at MB: 2449
[mpeg4 @ 0x10604a980] Error at MB: 257
[mpeg4 @ 0x10604a980] I cbpy damaged at 68 34
[mpeg4 @ 0x10604a980] Error at MB: 2822
[2025-11-07 11:22:15,160] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 639/2088 [9:12:42<19:52:29, 49.38s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9534553041114084, 'learning_rate': 6.939655172413793e-07, 'completion_length': 228.87890625, 'rewards/av_format_reward': 0.82421875, 'rewards/accuracy_reward': 0.390625, 'rewards/think_step_av_with_neutral_reward': 0.3852465897798538, 'reward': 1.6000903248786926, 'reward_std': 0.5344153344631195, 'kl': 0.0323486328125, 'epoch': 0.61}
 31%|███       | 639/2088 [9:12:42<19:52:29, 49.38s/it][2025-11-07 11:23:06,191] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 640/2088 [9:13:33<20:03:37, 49.87s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.8056565354020444, 'learning_rate': 6.934865900383141e-07, 'completion_length': 226.578125, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.46840164065361023, 'reward': 1.6988703608512878, 'reward_std': 0.4791545867919922, 'kl': 0.0328369140625, 'epoch': 0.61}
 31%|███       | 640/2088 [9:13:33<20:03:37, 49.87s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/00389.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/10173.mp4
[DEBUG] Step 640
[DEBUG] prompt_completion_ids shape: torch.Size([32, 281])
[DEBUG] Shortest output (len=157): <vis_desc>In the video, we see a man standing indoors. His eyes are looking directly at the other person, and his facial expression appears serious and focused. It seems that he is attentively observing or listening to the other person.</vis_desc>
<aud_desc>In the audio, there is a sigh at the beginning of the speech, "Ah." This sigh is the man's way of expressing sadness and regret.</aud_desc>
<think>Based on the man's serious and focused facial expression in the video, as well as the audio clue of the man's sigh and sadness, we can infer that this emotion is related to the previous conversation or thinking. The man may be reminiscing about something and feeling very sad.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=281): <vis_desc>In the video, we see a female character with furrowed brows and slightly downturned mouth, which may indicate that she is feeling upset or angry. Her gaze is fixed on the other person, and her facial expression appears serious and negative. During the conversation, she accompanies her speech with movements of her head, which may emphasize her viewpoint or request.</vis_desc>
<aud_desc>In the audio, the tone is stern and aggressive. The character's emotion is intense, and the tone is firm. Overall, it reflects the character's emotional state of anger and hatred. In the text, the subtitle reads: "的儿子，你，你，你，你是，你，你，你，你是，你，你，你，你是。 "This sentence expresses the anger and hatred of the female character towards the other person.</aud_desc>
<think>Based on the video clues of the female character's furrowed brows, downturned mouth, and serious and negative facial expression, as well as the audio clue of the character's stern and aggressive demeanor with a tone of accusation and emphasis, we can infer that this sentence carries a sense of anger and hatred. The female character may be angry and dissatisfied with the behavior or actions of the other person, and she wants to express her anger through blame and stern warning.</think>
<answer>angry</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/06856.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/04383.mp4
[DEBUG] Step 640
[DEBUG] prompt_completion_ids shape: torch.Size([32, 318])
[DEBUG] Shortest output (len=167): <vis_desc>In the video, we see a woman with slightly furrowed brows and slightly downturned corners of her mouth, tears streaming down her face. Her eyes are not looking at the camera, but rather fixed on the wall behind her, indicating that something is wrong inside her. She seems to be experiencing an emotional shock or fear.</vis_desc>
<aud_desc>In the audio, there are obvious sobbing sounds and the character's tone is very low and tearful, suggesting that the character is crying heavily and feeling very sad. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>Overall, the cues from both the visual and auditory modalities provide a clear and cohesive picture of a person experiencing sadness.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=318): <vis_desc>In the video, the first scene shows a female character wearing red glasses. Her facial expression appears calm and friendly, with a slight smile. She looks directly at the other person, with her mouth slightly open, seemingly communicating with the character on the right. In the following scene, the female character's expression remains calm and friendly, with a slight smile. She looks directly at the other person, with her mouth slightly open, indicating that she is still engaged in a calm conversation. Overall, the facial expression of the female character in the video suggests a positive and friendly emotional state.</vis_desc>
<aud_desc>In the audio, the character's voice is calm and soothing, with a positive and optimistic tone. In the text, the subtitle says, "You don't have to always put your success down on this marriage. I know it can always end badly." This sentence may have a positive and encouraging tone.</aud_desc>
<think>Based on the facial expression of the female character in the video clues, which shows a calm and friendly emotional state, with a slight smile, she seems to be engaged in a calm conversation. Additionally, the character's voice is calm and soothing, with a positive and optimistic tone, this sentence may have a positive and encouraging tone, suggesting that the female character believes in the positive aspect of success and says that it can always end badly. Therefore, this sentence may have a positive and pleasant emotional expression, aligning with the overall positive and friendly emotional state displayed by the female character.</think>
<answer>happy</answer>
[2025-11-07 11:24:00,034] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 641/2088 [9:14:27<20:31:31, 51.06s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.911230235714741, 'learning_rate': 6.93007662835249e-07, 'completion_length': 228.671875, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.73046875, 'rewards/think_step_av_with_neutral_reward': 0.5778744220733643, 'reward': 2.0388119220733643, 'reward_std': 0.4688204675912857, 'kl': 0.031494140625, 'epoch': 0.61}
 31%|███       | 641/2088 [9:14:27<20:31:31, 51.06s/it][2025-11-07 11:24:49,113] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 642/2088 [9:15:16<20:16:18, 50.47s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9997790283699102, 'learning_rate': 6.925287356321839e-07, 'completion_length': 226.6796875, 'rewards/av_format_reward': 0.81640625, 'rewards/accuracy_reward': 0.70703125, 'rewards/think_step_av_with_neutral_reward': 0.5740833282470703, 'reward': 2.0975208282470703, 'reward_std': 0.41325950622558594, 'kl': 0.0333251953125, 'epoch': 0.61}
 31%|███       | 642/2088 [9:15:16<20:16:18, 50.47s/it][2025-11-07 11:25:42,557] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 643/2088 [9:16:10<20:36:57, 51.36s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0030803107329507, 'learning_rate': 6.920498084291188e-07, 'completion_length': 226.43359375, 'rewards/av_format_reward': 0.78125, 'rewards/accuracy_reward': 0.83203125, 'rewards/think_step_av_with_neutral_reward': 0.6718380749225616, 'reward': 2.2851192951202393, 'reward_std': 0.42333976924419403, 'kl': 0.033203125, 'epoch': 0.62}
 31%|███       | 643/2088 [9:16:10<20:36:57, 51.36s/it][2025-11-07 11:26:27,887] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 644/2088 [9:16:55<19:52:33, 49.55s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.008041954777506, 'learning_rate': 6.915708812260535e-07, 'completion_length': 221.06640625, 'rewards/av_format_reward': 0.7734375, 'rewards/accuracy_reward': 0.4921875, 'rewards/think_step_av_with_neutral_reward': 0.4284495413303375, 'reward': 1.69407457113266, 'reward_std': 0.5564658343791962, 'kl': 0.0323486328125, 'epoch': 0.62}
 31%|███       | 644/2088 [9:16:55<19:52:33, 49.55s/it][2025-11-07 11:27:21,269] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 645/2088 [9:17:48<20:19:21, 50.70s/it]                                                       {'loss': 0.0019, 'grad_norm': 2.6929703748193283, 'learning_rate': 6.910919540229884e-07, 'completion_length': 226.109375, 'rewards/av_format_reward': 0.7578125, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.4026429057121277, 'reward': 1.6096742153167725, 'reward_std': 0.4821653217077255, 'kl': 0.04852294921875, 'epoch': 0.62}
 31%|███       | 645/2088 [9:17:48<20:19:21, 50.70s/it][2025-11-07 11:28:10,016] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 646/2088 [9:18:37<20:04:25, 50.11s/it]                                                       {'loss': 0.0023, 'grad_norm': 2.201926170463358, 'learning_rate': 6.906130268199234e-07, 'completion_length': 222.90625, 'rewards/av_format_reward': 0.66796875, 'rewards/accuracy_reward': 0.80078125, 'rewards/think_step_av_with_neutral_reward': 0.6845639944076538, 'reward': 2.153313994407654, 'reward_std': 0.4944048225879669, 'kl': 0.058837890625, 'epoch': 0.62}
 31%|███       | 646/2088 [9:18:37<20:04:25, 50.11s/it][2025-11-07 11:29:01,496] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 647/2088 [9:19:29<20:13:25, 50.52s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8607516287770112, 'learning_rate': 6.901340996168582e-07, 'completion_length': 225.484375, 'rewards/av_format_reward': 0.75390625, 'rewards/accuracy_reward': 0.33984375, 'rewards/think_step_av_with_neutral_reward': 0.33385515958070755, 'reward': 1.4276050925254822, 'reward_std': 0.403520330786705, 'kl': 0.03125, 'epoch': 0.62}
 31%|███       | 647/2088 [9:19:29<20:13:25, 50.52s/it][2025-11-07 11:29:49,220] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 648/2088 [9:20:16<19:52:24, 49.68s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.044153434227981, 'learning_rate': 6.896551724137931e-07, 'completion_length': 224.6875, 'rewards/av_format_reward': 0.54296875, 'rewards/accuracy_reward': 0.58984375, 'rewards/think_step_av_with_neutral_reward': 0.5453419387340546, 'reward': 1.678154468536377, 'reward_std': 0.46059614419937134, 'kl': 0.0328369140625, 'epoch': 0.62}
 31%|███       | 648/2088 [9:20:16<19:52:24, 49.68s/it][2025-11-07 11:30:36,691] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 649/2088 [9:21:04<19:35:40, 49.02s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.944147028906961, 'learning_rate': 6.891762452107279e-07, 'completion_length': 225.2109375, 'rewards/av_format_reward': 0.796875, 'rewards/accuracy_reward': 0.63671875, 'rewards/think_step_av_with_neutral_reward': 0.5729335844516754, 'reward': 2.0065274238586426, 'reward_std': 0.5027855634689331, 'kl': 0.0313720703125, 'epoch': 0.62}
 31%|███       | 649/2088 [9:21:04<19:35:40, 49.02s/it][2025-11-07 11:31:27,393] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 650/2088 [9:21:54<19:46:56, 49.52s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9500090381358646, 'learning_rate': 6.886973180076629e-07, 'completion_length': 225.9921875, 'rewards/av_format_reward': 0.79296875, 'rewards/accuracy_reward': 0.60546875, 'rewards/think_step_av_with_neutral_reward': 0.5238084048032761, 'reward': 1.9222458600997925, 'reward_std': 0.5531207919120789, 'kl': 0.03125, 'epoch': 0.62}
 31%|███       | 650/2088 [9:21:54<19:46:56, 49.52s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/2293.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/6508.mp4
[DEBUG] Step 650
[DEBUG] prompt_completion_ids shape: torch.Size([32, 338])
[DEBUG] Shortest output (len=168): <vis_desc>In the video, we see a woman with furrowed brows and slightly tilted shoulders, her mouth slightly open, seemingly speaking or waiting for a response. Her facial expression appears serious and negative.</vis_desc>
<aud_desc>In the audio, the tone is sharp and aggressive. Combined with the text content, it seems that the character is accusing the other person. The woman's anger and anger manifest in her speech.</aud_desc>
<think>Based on the serious and negative facial expression of the woman in the video clues, as well as the sharp and aggressive tone in the audio clues, we can infer that this sentence carries a sense of anger and dissatisfaction. The woman may be unhappy with the other person's behavior or remarks and expresses her anger by blaming them.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=338): <vis_desc>In the video, we see a character wearing a white robe and a white crown, standing in front of a mountainous background. In the video, his eyes are wide open, and his facial expression appears somewhat surprised and scared. His mouth is slightly open, as if speaking or reacting to someone or something. Overall, he seems to be experiencing a stressful or dangerous situation, and this situation seems to require him to manage something unexpected or uncertain.</vis_desc>
<aud_desc>In the audio, there are obvious panting sounds, indicating that the character is breathing heavily, which usually signifies being in a state of panting or excitement. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The combined visual and auditory cues strongly indicate that the character is experiencing fear. Visually, his wide-open eyes and slightly open mouth convey a classic expression of being suddenly confronted by an unforeseen threat, which are classic facial expressions associated with fear. The dramatic scene in front of a mountainous backdrop adds a sense of tension and vulnerability, where even in seemingly calm moments, an immediate sense of danger canonenacexist. Audibly, the panting sounds confirm an intense state of nervousness and physical arousal, which are key physical markers of fear. Although the subtitle text does not provide emotional context, the harmonious integration of his fearful facial expression and the fearful vocal tone creates a convincing portrayal of someone who is genuinely feeling fear in this moment. Together, these multimodal signals create a cohesive and compelling picture of a character grappling with the ambiguous and potentially threatening situation before him.</think>
<answer>fear</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/8310.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4139.mp4
[DEBUG] Step 650
[DEBUG] prompt_completion_ids shape: torch.Size([32, 329])
[DEBUG] Shortest output (len=159): <vis_desc>In the video, we see a female character with a raised head and slightly open mouth, seemingly speaking or shouting loudly, possibly experiencing an intense conversation or event.</vis_desc>
<aud_desc>In the audio, the tone is confident. In the text, the subtitle says, "A team of robots has arrived to introduce you to a robots kingdom." This sentence is likely spoken by the female character during her speech.</aud_desc>
<think>Based on the video clues of the female character's raised head and slightly open mouth, as well as the confident tone in the audio, we can infer that this sentence is likely described something unexpected or interesting about the robots. Therefore, this sentence expresses the female character's excitement and confidence.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=329): <vis_desc>In the video, the screen shows a man driving a car. Through the video, we can see that the driver's head is leaning forward, indicating that the driver is driving attentively. At the same time, his eyebrows are slightly furrowed, suggesting that he is experiencing an unpleasant emotion. Additionally, we see some fine雾 clouds in the air, indicating that it is likely to be a rainy day. Overall, we can infer that the man driving the car is possibly in a state of excitement or anxiety.</vis_desc>
<aud_desc>In the audio, the character's voice sounds very scared and urgent, with a loud sound at the end "Oh Brother". It can be inferred that the character is experiencing an emergency situation, with a high temperature and food poisoning. In the text, it is impossible to determine the emotional state based on the subtitle content.</aud_desc>
<think>The man's visual cues strongly convey a state of fear and anxiety: his slightly furrowed eyebrows and tense facial expression indicate heightened alertness and nervousness. The driver's leaning posture and the presence of drips on the windshield further suggest an un controlled situation, such as heatstroke or food poisoning, which would further intensify his fearful state. Although the audio character's voice echoes calm and controlled, the tense and concerned vocal tone aligns with the visual indicators of the sudden onset of an emergency, particularly in the case of fear and anxiety. Together, these multimodal signals paint a coherent picture of someone faced with a life-threatening situation, driven by a highly worried emotional state.</think>
<answer>fear</answer>
[2025-11-07 11:32:19,512] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 651/2088 [9:22:47<20:04:45, 50.30s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0458084262672056, 'learning_rate': 6.882183908045977e-07, 'completion_length': 232.4375, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.4765625, 'rewards/think_step_av_with_neutral_reward': 0.40462855994701385, 'reward': 1.592128574848175, 'reward_std': 0.6641512215137482, 'kl': 0.0323486328125, 'epoch': 0.62}
 31%|███       | 651/2088 [9:22:47<20:04:45, 50.30s/it][2025-11-07 11:33:10,324] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███       | 652/2088 [9:23:37<20:07:34, 50.46s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.01331058954085, 'learning_rate': 6.877394636015325e-07, 'completion_length': 226.59375, 'rewards/av_format_reward': 0.6953125, 'rewards/accuracy_reward': 0.703125, 'rewards/think_step_av_with_neutral_reward': 0.6063233017921448, 'reward': 2.0047607421875, 'reward_std': 0.5790627002716064, 'kl': 0.031982421875, 'epoch': 0.62}
 31%|███       | 652/2088 [9:23:37<20:07:34, 50.46s/it][2025-11-07 11:34:01,128] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 653/2088 [9:24:28<20:09:13, 50.56s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.8024118146181798, 'learning_rate': 6.872605363984673e-07, 'completion_length': 232.95703125, 'rewards/av_format_reward': 0.77734375, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.47959718108177185, 'reward': 1.7999097108840942, 'reward_std': 0.4785116910934448, 'kl': 0.02978515625, 'epoch': 0.63}
 31%|███▏      | 653/2088 [9:24:28<20:09:13, 50.56s/it][2025-11-07 11:34:47,799] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 654/2088 [9:25:15<19:40:30, 49.39s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.8304245053966433, 'learning_rate': 6.867816091954023e-07, 'completion_length': 221.5234375, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.4734809100627899, 'reward': 1.8562934398651123, 'reward_std': 0.41307538747787476, 'kl': 0.03271484375, 'epoch': 0.63}
 31%|███▏      | 654/2088 [9:25:15<19:40:30, 49.39s/it][2025-11-07 11:35:37,943] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 655/2088 [9:26:05<19:45:03, 49.62s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0421240142916486, 'learning_rate': 6.863026819923371e-07, 'completion_length': 226.1171875, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5403090417385101, 'reward': 1.8528091311454773, 'reward_std': 0.5864812433719635, 'kl': 0.03173828125, 'epoch': 0.63}
 31%|███▏      | 655/2088 [9:26:05<19:45:03, 49.62s/it][2025-11-07 11:36:30,911] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 656/2088 [9:26:58<20:08:12, 50.62s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9036936814610197, 'learning_rate': 6.85823754789272e-07, 'completion_length': 222.90234375, 'rewards/av_format_reward': 0.703125, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5145135968923569, 'reward': 1.827013611793518, 'reward_std': 0.482684463262558, 'kl': 0.03228759765625, 'epoch': 0.63}
 31%|███▏      | 656/2088 [9:26:58<20:08:12, 50.62s/it][2025-11-07 11:37:27,124] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 31%|███▏      | 657/2088 [9:27:54<20:47:21, 52.30s/it]                                                       {'loss': 0.0014, 'grad_norm': 1.9803492234232898, 'learning_rate': 6.853448275862068e-07, 'completion_length': 233.91015625, 'rewards/av_format_reward': 0.80859375, 'rewards/accuracy_reward': 0.48046875, 'rewards/think_step_av_with_neutral_reward': 0.45674265921115875, 'reward': 1.7458051443099976, 'reward_std': 0.48737001419067383, 'kl': 0.0341796875, 'epoch': 0.63}
 31%|███▏      | 657/2088 [9:27:54<20:47:21, 52.30s/it][2025-11-07 11:38:16,588] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 658/2088 [9:28:44<20:26:12, 51.45s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0429344105589986, 'learning_rate': 6.848659003831418e-07, 'completion_length': 227.1796875, 'rewards/av_format_reward': 0.84375, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.46354082226753235, 'reward': 1.83854079246521, 'reward_std': 0.45421577990055084, 'kl': 0.0325927734375, 'epoch': 0.63}
 32%|███▏      | 658/2088 [9:28:44<20:26:12, 51.45s/it][2025-11-07 11:39:06,722] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 659/2088 [9:29:34<20:15:57, 51.05s/it]                                                       {'loss': 0.0014, 'grad_norm': 2.000935828739959, 'learning_rate': 6.843869731800766e-07, 'completion_length': 225.5546875, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.48597611486911774, 'reward': 1.7906636595726013, 'reward_std': 0.5556714534759521, 'kl': 0.0340576171875, 'epoch': 0.63}
 32%|███▏      | 659/2088 [9:29:34<20:15:57, 51.05s/it][2025-11-07 11:40:00,473] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 660/2088 [9:30:28<20:34:21, 51.86s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.009869517854789, 'learning_rate': 6.839080459770114e-07, 'completion_length': 233.38671875, 'rewards/av_format_reward': 0.765625, 'rewards/accuracy_reward': 0.67578125, 'rewards/think_step_av_with_neutral_reward': 0.5398003309965134, 'reward': 1.9812066555023193, 'reward_std': 0.5031825304031372, 'kl': 0.031494140625, 'epoch': 0.63}
 32%|███▏      | 660/2088 [9:30:28<20:34:21, 51.86s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/4672.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/5142.mp4
[DEBUG] Step 660
[DEBUG] prompt_completion_ids shape: torch.Size([32, 305])
[DEBUG] Shortest output (len=194): <vis_desc>In the video, we see a woman sitting at a table with wine glasses and bottles in front of her. Her facial expression is neutral, her body posture is upright, and her eyes are looking directly at the other person, seemingly engaged in a serious conversation.</vis_desc>
<aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle reads: "What's that smell?" This sentence may be spoken by the woman when she is smelling something.</aud_desc>
<think>Based on the neutral facial expression, upright body posture, and direct eye contact of the woman in the video, as well as the audio clue without valuable emotional clues, we can infer that this sentence may be a neutral inquiry or query. Therefore, the emotional state of the woman in this scenario is not expressing strong emotions or mood, but rather as a neutral response to a certain question.</think>
<answer>neutral</answer>
[DEBUG] Longest output (len=305): <aud_desc>In the audio, there are no valuable emotional clues provided. In the text, the subtitle reads: "You're the one who invited me to visit your house." This sentence may be the woman's inquiry or response to the person sitting in front of her.</aud_desc>
<vis_desc>In the video, the screen shows a woman sitting in a restaurant. She is wearing a white dress with a white blouse and a white hat, and her hair is styled in an updo. The environment appears to be a restaurant with luxurious furnishing, including an elegant table, candles, and a TV. In the video, her facial expression is neutral, looking downward as if she is thinking or reminiscing about something. As time passes, her gaze lifts back up and she looks towards the person sitting in front of her, without making any additional facial expressions or movements. It can be inferred that she may be waiting for the other person's response or inquiry.</vis_desc>
<think>Based on the woman's neutral facial expression and downward gaze in the video clues, as well as the question or response she makes in the subtitle, we can deduce that she may be in a state of neutrality or confusion about the other person's visit. She may be waiting for the other person's response or inquiry, which aligns with her neutral emotion. Therefore, this sentence may be a sincere and straightforward inquiry from the woman to the person sitting in front of her.</think>
<answer>neutral</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/6612.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/1791.mp4
[DEBUG] Step 660
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=179): <aud_desc>In the audio, the tone is serious. Combined with the text content, it seems that he is emphasizing his point or expression. In the text, the subtitle says, "No, push me to respond." This sentence may be a reply or response from the male character to someone else.</aud_desc>
<vis_desc>In the video, we see a male character with a furrowed brow and slightly downturned mouth, seemingly engaged in a tense conversation.</vis_desc>
<think>Based on the video clues of the male character's furrowed brow and downturned mouth, as well as the serious tone described in the audio clue, we can infer that this sentence may carry a sense of anger, dissatisfaction, or resentment. The male character may be expressing his dissatisfaction or anger towards someone or something in this sentence.</think>
<answer>angry</answer>
[DEBUG] Longest output (len=350): <vis_desc>In the video, the screen shows a male character having a conversation with another person indoors. At the beginning of the video, he frowns, his facial muscles tense, and his mouth is open, indicating that he is speaking with force and displaying a provoking attitude. In the middle of the video, he extends his arm, palm up, and his index finger points forward, pointing at the other person, also showing a gesture of provocation. At the same time, his head moves up and down, emphasizing his words and displaying a very threatening and aggressive facial expression.</vis_desc>
<aud_desc>In the audio, the tone is positive and pleasant, with an accent on "definitely." In the text, the subtitle reads: "I definitely say no to the traffickers, the most absolute answer I'll give, uncle, how is that for a great explanation?" This sentence may involve a controversial topic, such as the rights of trafficked women.</aud_desc>
<think>Based on the facial expressions and gestures of the male character in the video, his facial muscles tense, showing a provoking and aggressive attitude. At the same time, he extends his arm, palms up, pointing at the other person, emphasizing his words and displaying a very threatening and aggressive facial expression. Combined with the positive and pleasant tone in the audio, it can be inferred that this sentence may have a provocative and aggressive tone, expressing the male character's firmness and anger towards the traffickers. Therefore, this sentence may imply that the male character will certainly refuse the traffickers and will not agree or let them have their way, which aligns with the overall display of threatening and aggressive emotions by the male character.</think>
<
[2025-11-07 11:40:54,340] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 661/2088 [9:31:21<20:47:47, 52.46s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.875557752855141, 'learning_rate': 6.834291187739463e-07, 'completion_length': 229.625, 'rewards/av_format_reward': 0.73828125, 'rewards/accuracy_reward': 0.59375, 'rewards/think_step_av_with_neutral_reward': 0.4963798075914383, 'reward': 1.8284111022949219, 'reward_std': 0.47242966294288635, 'kl': 0.03125, 'epoch': 0.63}
 32%|███▏      | 661/2088 [9:31:21<20:47:47, 52.46s/it][2025-11-07 11:41:45,336] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 662/2088 [9:32:12<20:36:27, 52.02s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9406109387344241, 'learning_rate': 6.829501915708812e-07, 'completion_length': 226.640625, 'rewards/av_format_reward': 0.7109375, 'rewards/accuracy_reward': 0.5625, 'rewards/think_step_av_with_neutral_reward': 0.480366587638855, 'reward': 1.7538040280342102, 'reward_std': 0.5096674263477325, 'kl': 0.0330810546875, 'epoch': 0.63}
 32%|███▏      | 662/2088 [9:32:12<20:36:27, 52.02s/it][2025-11-07 11:42:35,557] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 663/2088 [9:33:03<20:22:43, 51.48s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.088772530475712, 'learning_rate': 6.824712643678161e-07, 'completion_length': 224.12109375, 'rewards/av_format_reward': 0.70703125, 'rewards/accuracy_reward': 0.44921875, 'rewards/think_step_av_with_neutral_reward': 0.421526774764061, 'reward': 1.5777767300605774, 'reward_std': 0.4779464900493622, 'kl': 0.03033447265625, 'epoch': 0.64}
 32%|███▏      | 663/2088 [9:33:03<20:22:43, 51.48s/it][2025-11-07 11:43:27,493] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 664/2088 [9:33:55<20:25:05, 51.62s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9785384683207339, 'learning_rate': 6.819923371647509e-07, 'completion_length': 222.80078125, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.515625, 'rewards/think_step_av_with_neutral_reward': 0.48650483787059784, 'reward': 1.7325985431671143, 'reward_std': 0.4875013381242752, 'kl': 0.0325927734375, 'epoch': 0.64}
 32%|███▏      | 664/2088 [9:33:55<20:25:05, 51.62s/it][2025-11-07 11:44:16,558] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 665/2088 [9:34:44<20:06:03, 50.85s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.0645451425823156, 'learning_rate': 6.815134099616858e-07, 'completion_length': 221.90234375, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.57421875, 'rewards/think_step_av_with_neutral_reward': 0.5025614202022552, 'reward': 1.8970927000045776, 'reward_std': 0.5513405650854111, 'kl': 0.031982421875, 'epoch': 0.64}
 32%|███▏      | 665/2088 [9:34:44<20:06:03, 50.85s/it][2025-11-07 11:45:07,727] [WARNING] [stage3.py:2105:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 666/2088 [9:35:35<20:07:27, 50.95s/it]                                                       {'loss': 0.0097, 'grad_norm': 23.76387414692734, 'learning_rate': 6.810344827586207e-07, 'completion_length': 218.84375, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5189413726329803, 'reward': 1.823628842830658, 'reward_std': 0.5392412543296814, 'kl': 0.242919921875, 'epoch': 0.64}
 32%|███▏      | 666/2088 [9:35:35<20:07:27, 50.95s/it][2025-11-07 11:45:57,386] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 667/2088 [9:36:24<19:57:27, 50.56s/it]                                                       {'loss': 0.0014, 'grad_norm': 1.8894980299830253, 'learning_rate': 6.805555555555556e-07, 'completion_length': 220.78125, 'rewards/av_format_reward': 0.65625, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.36408576369285583, 'reward': 1.4070545434951782, 'reward_std': 0.4897022098302841, 'kl': 0.0340576171875, 'epoch': 0.64}
 32%|███▏      | 667/2088 [9:36:24<19:57:27, 50.56s/it][2025-11-07 11:46:45,903] [WARNING] [stage3.py:2105:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 668/2088 [9:37:13<19:42:05, 49.95s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.997492695848689, 'learning_rate': 6.800766283524903e-07, 'completion_length': 223.8984375, 'rewards/av_format_reward': 0.76953125, 'rewards/accuracy_reward': 0.546875, 'rewards/think_step_av_with_neutral_reward': 0.4551597237586975, 'reward': 1.7715659737586975, 'reward_std': 0.4198666512966156, 'kl': 0.0323486328125, 'epoch': 0.64}
 32%|███▏      | 668/2088 [9:37:13<19:42:05, 49.95s/it][2025-11-07 11:47:36,982] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 669/2088 [9:38:04<19:49:17, 50.29s/it]                                                       {'loss': 0.0013, 'grad_norm': 2.064978108095801, 'learning_rate': 6.795977011494252e-07, 'completion_length': 223.91796875, 'rewards/av_format_reward': 0.67578125, 'rewards/accuracy_reward': 0.53125, 'rewards/think_step_av_with_neutral_reward': 0.49038055539131165, 'reward': 1.697411835193634, 'reward_std': 0.6140630543231964, 'kl': 0.0328369140625, 'epoch': 0.64}
 32%|███▏      | 669/2088 [9:38:04<19:49:17, 50.29s/it][2025-11-07 11:48:28,777] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 670/2088 [9:38:56<19:59:08, 50.74s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9859316847298356, 'learning_rate': 6.791187739463601e-07, 'completion_length': 223.65625, 'rewards/av_format_reward': 0.73046875, 'rewards/accuracy_reward': 0.5234375, 'rewards/think_step_av_with_neutral_reward': 0.47282689809799194, 'reward': 1.7267332077026367, 'reward_std': 0.5331893563270569, 'kl': 0.03131103515625, 'epoch': 0.64}
 32%|███▏      | 670/2088 [9:38:56<19:59:08, 50.74s/it][DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/03696.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/MAFW/clips/08884.mp4
[DEBUG] Step 670
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=180): <vis_desc>In the video, we see a woman with a smiling face and slightly parted lips, indicating that she is saying something positive to someone.</vis_desc>
<aud_desc>In the audio, the character uses positive language and enthusiastic tone, with a relaxed and cheerful voice. In the text, the subtitle reads: "So, let's start seeing a lot more of BIONA and me." This sentence may be an encouragement or suggestion from the woman to someone.</aud_desc>
<think>Based on the video clues of the woman's smiling face and slightly parted lips, as well as the positive language and enthusiastic tone in the audio, we can infer that this sentence may be the woman expressing her enthusiasm and eagerness to meet with someone, BIONA and herself. The woman may be anticipating seeing them and feeling happy about it.</think>
<answer>happy</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, the tone of the character's voice is relaxed and cheerful, indicating positive emotions. In the text, the subtitle reads: "So, be ready to start seeing a lot more of 'B Yionia' and me." This sentence may be a friendly promotion or reminder from the woman to someone.</aud_desc>
<vis_desc>In the video, at the beginning of the video, we see a woman whose facial expression appears joyful and excited. Her mouth moves slightly, and her gaze is directed towards the left of the screen, giving a sense of surprise to the other person. The positive emotional state of the woman becomes more evident as the video progresses towards the middle part. In the following scenes, her smile becomes more obvious and friendly, her mouth slightly curves upwards, and she waves her hand forward, as if playing a pleasant game with the other person. Overall, it can be inferred that the woman in the video is in a very positive and happy emotional state.</vis_desc>
<think>Taking these clues into consideration, it can be predicted that the woman in the video may be in a happy and positive emotional state. Based on the woman's joyful and excited facial expression in the video clues, as well as the positive emotional state displayed by the woman in the audio clues, it can be inferred that the woman in the video may feel happy and positive. Furthermore, the woman's smile and friendly gestures in the video further reinforce her joyful emotion. Therefore, this sentence may be an invitation or reminder from the woman to the other person to start seeing a lot more of a happy group and the woman, in a friendly and positive mood, wishes them good news.</think>
<answer>happy</answer>
[DEBUG] Batch size: 2
[DEBUG] Example prompt text: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|video_pad|><|vision_end|><vi_start><video><vi_end>
<au_start><audio><au_end>
As an emotion recognition expert, analyze the video considering visual behavior, and audio tone. Describe your observations using the tags <vis_desc> and <aud_desc> for visual, and audio cues respectively. Focus first on the modality that provides the most important information for emotion prediction. Then, integrate all these cues into a reasoning process in <think> </think>, and provide the final predicted emotion in <answer> </answer> tags.<|im_end|>
<|im_start|>assistant

[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/975.mp4
[DEBUG] Example video path: /mnt/ssd_hs/Dataset/DFEW/clip/9271.mp4
[DEBUG] Step 670
[DEBUG] prompt_completion_ids shape: torch.Size([32, 350])
[DEBUG] Shortest output (len=165): <vis_desc>In the video, we see a woman with a furrowed brow and slightly downturned mouth, seemingly expressing sadness or disappointment.</vis_desc>
<aud_desc>In the audio, the tone is low and conveys a sense of weightiness. In the text, the subtitle reads, "It's all poison." This sentence may be spoken by the woman during her sad expression.</aud_desc>
<think>Based on the description of the woman's furrowed brow and downturned mouth in the video clues, as well as the low tone in the audio clues, we can infer that this sentence may carry a sense of sadness or disappointment. The woman may be expressing her disappointment and sorrow about something, which aligns with the description of her sad expression.</think>
<answer>sad</answer>
[DEBUG] Longest output (len=350): <aud_desc>In the audio, the tone is low and conveys a sense of sadness and vulnerability. In the text, the subtitle reads, "It's all a matter of life." This sentence may be spoken by the female character in response to something or someone.</aud_desc>
<vis_desc>In the video, we see a female character with a frown, drooping eyes, furrowed brows, and slightly downturned corners of the mouth, all of which indicate a relatively negative emotion. Her gaze is fixed on the left, seemingly focused on observing or contemplating something.</vis_desc>
<think>Based on the visual cues of the female character's frown, drooping eyes, furrowed brows, and slight downturned corners of the mouth, as well as her intense gaze on the left, it can be inferred that she may have encountered a difficult or painful situation in her life. The mention of "all is a matter of life" in the subtitle may reflect the female character's view or attitude towards difficulties in her life or the meaning of life. Due to the low audio tone and emotion expression, it is difficult to determine the specific emotional state of the female character. However, based on the visual indicators of the female character's frown, drooping eyes, furrowed brows, and slight downturned corners of the mouth, as well as her intense gaze on the left, it can be inferred that she may have encountered a difficult or painful situation in her life and the mention of "all is a matter of life" may be a reflection or expression of acceptance and acceptance of one's own condition or purpose in life. This sentence may indicate the female character's negative attitude and outlook on life, suggesting that life
[2025-11-07 11:49:24,130] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 671/2088 [9:39:51<20:30:59, 52.12s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9652875555420888, 'learning_rate': 6.78639846743295e-07, 'completion_length': 221.19921875, 'rewards/av_format_reward': 0.87890625, 'rewards/accuracy_reward': 0.6328125, 'rewards/think_step_av_with_neutral_reward': 0.5265890061855316, 'reward': 2.0383076667785645, 'reward_std': 0.5484843105077744, 'kl': 0.0325927734375, 'epoch': 0.64}
 32%|███▏      | 671/2088 [9:39:51<20:30:59, 52.12s/it][2025-11-07 11:50:11,749] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 672/2088 [9:40:39<19:58:13, 50.77s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9623057064748028, 'learning_rate': 6.781609195402298e-07, 'completion_length': 213.46875, 'rewards/av_format_reward': 0.671875, 'rewards/accuracy_reward': 0.53515625, 'rewards/think_step_av_with_neutral_reward': 0.5235537737607956, 'reward': 1.7305850386619568, 'reward_std': 0.5532942414283752, 'kl': 0.0308837890625, 'epoch': 0.64}
 32%|███▏      | 672/2088 [9:40:39<19:58:13, 50.77s/it][2025-11-07 11:51:09,762] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 673/2088 [9:41:37<20:48:36, 52.94s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0352491862931563, 'learning_rate': 6.776819923371647e-07, 'completion_length': 232.59765625, 'rewards/av_format_reward': 0.68359375, 'rewards/accuracy_reward': 0.47265625, 'rewards/think_step_av_with_neutral_reward': 0.4576026499271393, 'reward': 1.6138526797294617, 'reward_std': 0.645881712436676, 'kl': 0.03125, 'epoch': 0.64}
 32%|███▏      | 673/2088 [9:41:37<20:48:36, 52.94s/it][2025-11-07 11:52:02,870] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 674/2088 [9:42:30<20:48:52, 52.99s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.974484617890325, 'learning_rate': 6.772030651340997e-07, 'completion_length': 225.90625, 'rewards/av_format_reward': 0.8203125, 'rewards/accuracy_reward': 0.5703125, 'rewards/think_step_av_with_neutral_reward': 0.4793694466352463, 'reward': 1.8699944615364075, 'reward_std': 0.5779760181903839, 'kl': 0.03369140625, 'epoch': 0.65}
 32%|███▏      | 674/2088 [9:42:30<20:48:52, 52.99s/it][2025-11-07 11:52:56,411] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 675/2088 [9:43:23<20:51:51, 53.16s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.9445367647952192, 'learning_rate': 6.767241379310345e-07, 'completion_length': 230.23828125, 'rewards/av_format_reward': 0.734375, 'rewards/accuracy_reward': 0.54296875, 'rewards/think_step_av_with_neutral_reward': 0.564841628074646, 'reward': 1.842185378074646, 'reward_std': 0.5585981607437134, 'kl': 0.030029296875, 'epoch': 0.65}
 32%|███▏      | 675/2088 [9:43:23<20:51:51, 53.16s/it][2025-11-07 11:53:47,249] [WARNING] [stage3.py:2105:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 676/2088 [9:44:14<20:34:35, 52.46s/it]                                                       {'loss': 0.0012, 'grad_norm': 2.0074687702846896, 'learning_rate': 6.762452107279693e-07, 'completion_length': 227.58203125, 'rewards/av_format_reward': 0.65625, 'rewards/accuracy_reward': 0.609375, 'rewards/think_step_av_with_neutral_reward': 0.5078443586826324, 'reward': 1.7734693884849548, 'reward_std': 0.713140994310379, 'kl': 0.03082275390625, 'epoch': 0.65}
 32%|███▏      | 676/2088 [9:44:14<20:34:35, 52.46s/it][2025-11-07 11:54:40,245] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 677/2088 [9:45:07<20:37:30, 52.62s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9124343119383107, 'learning_rate': 6.757662835249041e-07, 'completion_length': 226.76953125, 'rewards/av_format_reward': 0.7890625, 'rewards/accuracy_reward': 0.41015625, 'rewards/think_step_av_with_neutral_reward': 0.4005035310983658, 'reward': 1.5997222661972046, 'reward_std': 0.42538444697856903, 'kl': 0.0316162109375, 'epoch': 0.65}
 32%|███▏      | 677/2088 [9:45:07<20:37:30, 52.62s/it][2025-11-07 11:55:33,162] [WARNING] [stage3.py:2105:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 32%|███▏      | 678/2088 [9:46:00<20:38:41, 52.71s/it]                                                       {'loss': 0.0012, 'grad_norm': 1.934694834611196, 'learning_rate': 6.752873563218391e-07, 'completion_length': 232.83203125, 'rewards/av_format_reward': 0.69921875, 'rewards/accuracy_reward': 0.484375, 'rewards/think_step_av_with_neutral_reward': 0.44141459465026855, 'reward': 1.6250082850456238, 'reward_std': 0.54188072681427, 'kl': 0.0302734375, 'epoch': 0.65}
 32%|███▏      | 678/2088 [9:46:00<20:38:41, 52.71s/it][2025-11-07 11:56:19,994] [WARNING] [stage3.py:2105:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 33%|███▎      | 679/2088 [9:46:47<19:56:24, 50.95s/it]                                                       {'loss': 0.0013, 'grad_norm': 1.9832858412876977, 'learning_rate': 6.748084291187739e-07, 'completion_length': 216.421875, 'rewards/av_format_reward': 0.78515625, 'rewards/accuracy_reward': 0.38671875, 'rewards/think_step_av_with_neutral_reward': 0.3223477900028229, 'reward': 1.4942228198051453, 'reward_std': 0.6425265371799469, 'kl': 0.033447265625, 'epoch': 0.65}
 33%|███▎      | 679/2088 [9:46:47<19:56:24, 50.95s/it]